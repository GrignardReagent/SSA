from __future__ import annotations

"""Simple Transformer encoder for CVmCherry time series.

This module provides a **minimal yet well annotated** Transformer model for
classifying single‑feature time series such as the CVmCherry nuclear
localisation traces used throughout this repository.  It is intentionally small
so it can be pre‑trained on synthetic mRNA trajectories generated by
``IY010_simulation.py`` (telegraph model with varying mean, coefficient of
variation and autocorrelation) before being fine‑tuned on experimental data.

The processing steps are:

1. **Linear projection** from the raw scalar signal to a ``d_model`` dimensional
   space (default ``64``).
2. **Prepending a learnt ``[CLS]`` token** that will later act as the sequence
   summary.
3. **Injecting sinusoidal positional encodings** so the model knows the order of
   observations.  These are deterministic and do not require learning.
4. **Passing the sequence through a small Transformer encoder** comprising two
   layers with four attention heads each.
5. **Taking the ``[CLS]`` state** after the encoder and feeding it through a
   lightweight classifier head to obtain logits for ``n_classes`` outputs.

The example below demonstrates typical usage and how the encoder can be frozen
prior to fine‑tuning on experimental traces.

Example
-------
>>> cfg = ModelCfg(n_classes=2)
>>> model = CVmCherryTransformer(cfg)
>>> x = torch.randn(4, 10, 1)
>>> lengths = torch.tensor([10, 7, 5, 3])
>>> logits = model(x, lengths)
>>> model.freeze_encoder()  # freeze before fine-tuning
>>> logits.shape
torch.Size([4, 2])
"""

from dataclasses import dataclass
from typing import Optional
import math

import torch
import torch.nn as nn


@dataclass
class ModelCfg:
    """Typed configuration for :class:`CVmCherryTransformer`.

    The defaults reproduce the architecture described in the module
    documentation.  ``max_len`` governs the size of the positional encoding
    table and therefore the maximum supported sequence length (excluding the
    added ``[CLS]`` token).
    """

    n_classes: int  # number of output classes
    d_model: int = 64  # latent dimensionality after projection
    n_heads: int = 4  # multi-head attention heads per layer
    n_layers: int = 2  # number of stacked encoder layers
    d_ff: int = 128  # width of the feed-forward sublayers
    dropout: float = 0.1  # dropout probability used throughout
    max_len: int = 8192  # maximum sequence length (excluding [CLS])


class PositionalEncoding(nn.Module):
    """Classic sinusoidal positional encoding.

    Pre-computes sine and cosine waves of different frequencies and adds them to
    the input sequence.  A small dropout is applied afterwards as a form of
    regularisation.  The implementation mirrors the one used in
    "Attention is All You Need".
    """

    def __init__(self, d_model: int, dropout: float, max_len: int) -> None:
        super().__init__()
        # ``pe`` holds the sinusoidal table; shape ``[max_len, d_model]``.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # register as buffer so it moves with the model but is not a parameter
        self.register_buffer("pe", pe.unsqueeze(0))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Add positional information to ``x`` and apply dropout."""

        x = x + self.pe[:, : x.size(1)]
        return self.dropout(x)


class CVmCherryTransformer(nn.Module):
    """Encoder-only Transformer for CVmCherry localisation series."""

    def __init__(self, cfg: ModelCfg) -> None:
        super().__init__()
        self.cfg = cfg

        # Project the single input channel to ``d_model`` dimensions.
        self.proj = nn.Linear(1, cfg.d_model)

        # Learnable ``[CLS]`` token used to summarise the sequence.
        self.cls_token = nn.Parameter(torch.empty(1, 1, cfg.d_model))
        nn.init.trunc_normal_(self.cls_token, std=0.02)

        # Core Transformer encoder block from ``torch.nn``.
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=cfg.d_model,
            nhead=cfg.n_heads,
            dim_feedforward=cfg.d_ff,
            dropout=cfg.dropout,
            batch_first=True,
            activation="gelu",
            norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, cfg.n_layers)

        # Positional encoding has one extra slot for the prepended ``[CLS]``.
        self.pos_enc = PositionalEncoding(cfg.d_model, cfg.dropout, cfg.max_len + 1)

        # Final classifier acting on the ``[CLS]`` representation.
        self.classifier = nn.Linear(cfg.d_model, cfg.n_classes)
        nn.init.trunc_normal_(self.classifier.weight, std=0.02)
        nn.init.zeros_(self.classifier.bias)

        self.cls_dropout = nn.Dropout(cfg.dropout)

    def freeze_encoder(self, freeze: bool = True) -> None:
        """Freeze or unfreeze the projection and encoder for transfer learning."""

        for p in self.proj.parameters():
            p.requires_grad = not freeze
        self.cls_token.requires_grad = not freeze
        for p in self.encoder.parameters():
            p.requires_grad = not freeze

    def reset_classifier(self) -> None:
        """Reinitialise the classifier head (useful before fine-tuning)."""

        nn.init.trunc_normal_(self.classifier.weight, std=0.02)
        nn.init.zeros_(self.classifier.bias)

    @staticmethod
    def make_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:
        """Return ``True`` at indices that should be masked out.

        Parameters
        ----------
        lengths:
            Tensor of shape ``[B]`` giving the valid length of each sequence.
        max_len:
            The maximum sequence length present in the batch.
        """

        arange = torch.arange(max_len, device=lengths.device)
        return arange.expand(len(lengths), max_len) >= lengths.unsqueeze(1)

    def forward(
        self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Compute logits for a batch of sequences.

        Parameters
        ----------
        x:
            Input tensor of shape ``[B, T, 1]`` containing the time series.
        lengths:
            Optional ``[B]`` tensor giving the valid sequence lengths.  When
            provided, padding positions are masked so they do not influence the
            attention mechanism.
        """

        b, t, _ = x.shape
        x = self.proj(x)  # [B, T, d_model]

        # Add the ``[CLS]`` token to the front of the sequence.
        cls = self.cls_token.expand(b, -1, -1)
        x = torch.cat([cls, x], dim=1)

        # Inject position information.
        x = self.pos_enc(x)

        mask = None
        if lengths is not None:
            # Build key-padding mask and account for the extra ``[CLS]`` step.
            padding = self.make_padding_mask(lengths, t)
            cls_pad = torch.zeros(b, 1, dtype=torch.bool, device=x.device)
            mask = torch.cat([cls_pad, padding], dim=1)

        x = self.encoder(x, src_key_padding_mask=mask)

        # Classifier operates on the ``[CLS]`` representation.
        cls_state = x[:, 0]
        cls_state = self.cls_dropout(cls_state)
        return self.classifier(cls_state)


if __name__ == "__main__":
    cfg = ModelCfg(n_classes=2)
    model = CVmCherryTransformer(cfg)
    batch = torch.randn(4, 10, 1)
    print("No lengths:", model(batch).shape)
    lens = torch.tensor([10, 7, 5, 3])
    print("With lengths:", model(batch, lens).shape)
