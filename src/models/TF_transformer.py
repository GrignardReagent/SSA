from __future__ import annotations

"""Simple Transformer encoder for TF time series.

This module provides a **minimal yet well annotated** Transformer model for
classifying single‑feature time series such as the Transcription Factor (TF) nuclear
localisation traces used throughout this repository.  It is intentionally small
so it can be pre‑trained on synthetic mRNA trajectories generated by
``IY010_simulation.py`` (telegraph model with varying mean, coefficient of
variation and autocorrelation) before being fine‑tuned on experimental data.

The processing steps are:

1. **Linear projection** from the raw scalar signal to a ``d_model`` dimensional
   space (default ``64``).
2. **Prepending a learnt ``[CLS]`` token** that will later act as the sequence
   summary.
3. **Injecting sinusoidal positional encodings** so the model knows the order of
   observations.  These are deterministic and do not require learning.
4. **Passing the sequence through a small Transformer encoder** comprising two
   layers with four attention heads each.
5. **Taking the ``[CLS]`` state** after the encoder and feeding it through a
   lightweight classifier head to obtain logits for ``n_classes`` outputs.

The example below demonstrates typical usage and how the encoder can be frozen
prior to fine‑tuning on experimental traces.

Example
-------
>>> cfg = ModelCfg(n_classes=2)
>>> model = TFTransformer(cfg)
>>> x = torch.randn(4, 10, 1)
>>> lengths = torch.tensor([10, 7, 5, 3])
>>> logits = model(x, lengths)
>>> model.freeze_encoder()  # freeze before fine-tuning
>>> logits.shape
torch.Size([4, 2])
"""

from dataclasses import dataclass
from typing import Optional
import math
import warnings

import torch
import torch.nn as nn

# Suppress the nested tensor warning from PyTorch transformer
warnings.filterwarnings("ignore", message="enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True")


@dataclass
class ModelCfg:
    """Typed configuration for :class:`TFTransformer`.

    The defaults reproduce the architecture described in the module
    documentation.  ``max_len`` governs the size of the positional encoding
    table and therefore the maximum supported sequence length (excluding the
    added ``[CLS]`` token).
    """

    n_classes: int  # number of output classes
    d_model: int = 64  # latent dimensionality after projection
    n_heads: int = 4  # multi-head attention heads per layer
    n_layers: int = 2  # number of stacked encoder layers
    d_ff: int = 128  # width of the feed-forward sublayers
    dropout: float = 0.1  # dropout probability used throughout
    max_len: int = 8192  # maximum sequence length (excluding [CLS])
    verbose: bool = False  # whether to print verbose logging information


class PositionalEncoding(nn.Module):
    """Classic sinusoidal positional encoding.

    Pre-computes sine and cosine waves of different frequencies and adds them to
    the input sequence.  A small dropout is applied afterwards as a form of
    regularisation.  The implementation mirrors the one used in
    "Attention is All You Need".
    """

    def __init__(self, d_model: int, dropout: float, max_len: int) -> None:
        super().__init__()
        # ``pe`` holds the sinusoidal table; shape ``[max_len, d_model]``.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # register as buffer so it moves with the model but is not a parameter
        self.register_buffer("pe", pe.unsqueeze(0))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Add positional information to ``x`` and apply dropout."""

        x = x + self.pe[:, : x.size(1)]
        return self.dropout(x)


class TFTransformer(nn.Module):
    """Encoder-only Transformer for TF localisation series."""

    def __init__(self, cfg: ModelCfg) -> None:
        super().__init__()
        self.cfg = cfg
        self.verbose = cfg.verbose
        
        # Device detection and setup
        self.device = self._setup_device()
        
        if self.verbose:
            print(f"🔧 Initializing TFTransformer with config: {cfg}")
            print(f"🖥️  Using device: {self.device}")
            if torch.cuda.is_available():
                print(f"🚀 CUDA available with {torch.cuda.device_count()} GPU(s)")
                for i in range(torch.cuda.device_count()):
                    print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")

        # Project the single input channel to ``d_model`` dimensions.
        self.proj = nn.Linear(1, cfg.d_model)

        # Learnable ``[CLS]`` token used to summarise the sequence.
        self.cls_token = nn.Parameter(torch.empty(1, 1, cfg.d_model))
        nn.init.trunc_normal_(self.cls_token, std=0.02)

        # Core Transformer encoder block from ``torch.nn``.
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=cfg.d_model,
            nhead=cfg.n_heads,
            dim_feedforward=cfg.d_ff,
            dropout=cfg.dropout,
            batch_first=True,
            activation="gelu",
            norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, cfg.n_layers)

        # Positional encoding has one extra slot for the prepended ``[CLS]``.
        self.pos_enc = PositionalEncoding(cfg.d_model, cfg.dropout, cfg.max_len + 1)

        # Final classifier acting on the ``[CLS]`` representation.
        self.classifier = nn.Linear(cfg.d_model, cfg.n_classes)
        nn.init.trunc_normal_(self.classifier.weight, std=0.02)
        nn.init.zeros_(self.classifier.bias)

        self.cls_dropout = nn.Dropout(cfg.dropout)
        
        # Move model to device and setup multi-GPU if available
        self.to(self.device)
        self._setup_multi_gpu()
        
        if self.verbose:
            total_params = sum(p.numel() for p in self.parameters())
            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"📊 Model initialized with {total_params:,} total parameters ({trainable_params:,} trainable)")

    def _setup_device(self) -> torch.device:
        """Setup the appropriate device (CPU/GPU)."""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            if self.verbose:
                print(f"✅ CUDA detected, using GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device("cpu")
            if self.verbose:
                print("⚠️  No CUDA available, using CPU")
        return device
    
    def _setup_multi_gpu(self) -> None:
        """Setup multi-GPU training if multiple GPUs are available."""
        if torch.cuda.device_count() > 1:
            if self.verbose:
                print(f"🔄 Setting up DataParallel for {torch.cuda.device_count()} GPUs")
            # Note: We'll apply DataParallel to the core components but keep the main module structure
            # This allows for proper state_dict saving/loading
            self.encoder = nn.DataParallel(self.encoder)
            self.proj = nn.DataParallel(self.proj)
            self.classifier = nn.DataParallel(self.classifier)
            if self.verbose:
                print("✅ Multi-GPU setup complete")
        elif self.verbose and torch.cuda.is_available():
            print("ℹ️  Single GPU detected, using single GPU training")

    def freeze_encoder(self, freeze: bool = True) -> None:
        """Freeze or unfreeze the projection and encoder for transfer learning."""
        
        if self.verbose:
            action = "Freezing" if freeze else "Unfreezing"
            print(f"🔒 {action} encoder components for transfer learning")

        # Handle DataParallel wrapped modules
        proj_module = self.proj.module if isinstance(self.proj, nn.DataParallel) else self.proj
        encoder_module = self.encoder.module if isinstance(self.encoder, nn.DataParallel) else self.encoder
        
        for p in proj_module.parameters():
            p.requires_grad = not freeze
        self.cls_token.requires_grad = not freeze
        for p in encoder_module.parameters():
            p.requires_grad = not freeze
            
        if self.verbose:
            frozen_params = sum(1 for p in self.parameters() if not p.requires_grad)
            total_params = sum(1 for p in self.parameters())
            print(f"📊 {frozen_params}/{total_params} parameters frozen")

    def reset_classifier(self) -> None:
        """Reinitialise the classifier head (useful before fine-tuning)."""
        
        if self.verbose:
            print("🔄 Resetting classifier head weights")
            
        # Handle DataParallel wrapped classifier
        classifier_module = self.classifier.module if isinstance(self.classifier, nn.DataParallel) else self.classifier
        
        nn.init.trunc_normal_(classifier_module.weight, std=0.02)
        nn.init.zeros_(classifier_module.bias)
        
        if self.verbose:
            print("✅ Classifier head reset complete")

    @staticmethod
    def make_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:
        """Return ``True`` at indices that should be masked out.

        Parameters
        ----------
        lengths:
            Tensor of shape ``[B]`` giving the valid length of each sequence.
        max_len:
            The maximum sequence length present in the batch.
        """

        arange = torch.arange(max_len, device=lengths.device)
        return arange.expand(len(lengths), max_len) >= lengths.unsqueeze(1)

    def forward(
        self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Compute logits for a batch of sequences.

        Parameters
        ----------
        x:
            Input tensor of shape ``[B, T, 1]`` containing the time series.
        lengths:
            Optional ``[B]`` tensor giving the valid sequence lengths.  When
            provided, padding positions are masked so they do not influence the
            attention mechanism.
        """
        
        if self.verbose and hasattr(self, '_forward_calls'):
            self._forward_calls += 1
        elif self.verbose:
            self._forward_calls = 1
            print(f"🔄 Starting forward pass (batch size: {x.shape[0]}, seq length: {x.shape[1]})")

        # Ensure input is on the correct device
        x = x.to(self.device)
        if lengths is not None:
            lengths = lengths.to(self.device)

        b, t, _ = x.shape
        x = self.proj(x)  # [B, T, d_model]

        # Add the ``[CLS]`` token to the front of the sequence.
        cls = self.cls_token.expand(b, -1, -1)
        x = torch.cat([cls, x], dim=1)

        # Inject position information.
        x = self.pos_enc(x)

        mask = None
        if lengths is not None:
            # Build key-padding mask and account for the extra ``[CLS]`` step.
            padding = self.make_padding_mask(lengths, t)
            cls_pad = torch.zeros(b, 1, dtype=torch.bool, device=x.device)
            mask = torch.cat([cls_pad, padding], dim=1)
            
            if self.verbose and self._forward_calls <= 3:  # Only log for first few calls
                masked_positions = mask.sum().item() if mask is not None else 0
                print(f"🎭 Applied attention mask with {masked_positions} masked positions")

        x = self.encoder(x, src_key_padding_mask=mask)

        # Classifier operates on the ``[CLS]`` representation.
        cls_state = x[:, 0]
        cls_state = self.cls_dropout(cls_state)
        output = self.classifier(cls_state)
        
        if self.verbose and self._forward_calls <= 3:  # Only log for first few calls
            print(f"📤 Forward pass complete, output shape: {output.shape}")
            
        return output


    def save_model(self, filepath: str) -> None:
        """Save model state dict, handling DataParallel correctly."""
        if self.verbose:
            print(f"💾 Saving model to {filepath}")
            
        # Get the actual state dict, unwrapping DataParallel if necessary
        state_dict = self.state_dict()
        
        # Remove 'module.' prefix from DataParallel wrapped modules
        cleaned_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('proj.module.'):
                cleaned_state_dict[key.replace('proj.module.', 'proj.')] = value
            elif key.startswith('encoder.module.'):
                cleaned_state_dict[key.replace('encoder.module.', 'encoder.')] = value
            elif key.startswith('classifier.module.'):
                cleaned_state_dict[key.replace('classifier.module.', 'classifier.')] = value
            else:
                cleaned_state_dict[key] = value
        
        torch.save(cleaned_state_dict, filepath)
        
        if self.verbose:
            print(f"✅ Model saved successfully")
    
    def load_model(self, filepath: str) -> None:
        """Load model state dict, handling DataParallel correctly."""
        if self.verbose:
            print(f"📥 Loading model from {filepath}")
            
        state_dict = torch.load(filepath, map_location=self.device)
        
        # Handle loading into DataParallel modules if necessary
        if isinstance(self.proj, nn.DataParallel):
            # Add 'module.' prefix for DataParallel wrapped modules
            adjusted_state_dict = {}
            for key, value in state_dict.items():
                if key.startswith('proj.') and not key.startswith('proj.module.'):
                    adjusted_state_dict[key.replace('proj.', 'proj.module.')] = value
                elif key.startswith('encoder.') and not key.startswith('encoder.module.'):
                    adjusted_state_dict[key.replace('encoder.', 'encoder.module.')] = value
                elif key.startswith('classifier.') and not key.startswith('classifier.module.'):
                    adjusted_state_dict[key.replace('classifier.', 'classifier.module.')] = value
                else:
                    adjusted_state_dict[key] = value
            state_dict = adjusted_state_dict
        
        self.load_state_dict(state_dict)
        
        if self.verbose:
            print(f"✅ Model loaded successfully")


if __name__ == "__main__":
    # Test with verbose mode
    cfg = ModelCfg(n_classes=2, verbose=True)
    model = TFTransformer(cfg)
    batch = torch.randn(4, 10, 1)
    print("No lengths:", model(batch).shape)
    lens = torch.tensor([10, 7, 5, 3])
    print("With lengths:", model(batch, lens).shape)
