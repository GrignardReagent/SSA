from __future__ import annotations

"""Simple Transformer encoder for TF time series.

This module provides a **minimal yet well annotated** Transformer model for
classifying singleâ€‘feature time series such as the Transcription Factor (TF) nuclear
localisation traces used throughout this repository.  It is intentionally small
so it can be preâ€‘trained on synthetic mRNA trajectories generated by
``IY010_simulation.py`` (telegraph model with varying mean, coefficient of
variation and autocorrelation) before being fineâ€‘tuned on experimental data.

The processing steps are:

1. **Linear projection** from the raw scalar signal to a ``d_model`` dimensional
   space (default ``64``).
2. **Prepending a learnt ``[CLS]`` token** that will later act as the sequence
   summary.
3. **Injecting sinusoidal positional encodings** so the model knows the order of
   observations.  These are deterministic and do not require learning.
4. **Passing the sequence through a small Transformer encoder** comprising two
   layers with four attention heads each.
5. **Taking the ``[CLS]`` state** after the encoder and feeding it through a
   lightweight classifier head to obtain logits for ``n_classes`` outputs.

The example below demonstrates typical usage and how the encoder can be frozen
prior to fineâ€‘tuning on experimental traces.

Example
-------
>>> cfg = ModelCfg(n_classes=2)
>>> model = TFTransformer(cfg)
>>> x = torch.randn(4, 10, 1)
>>> lengths = torch.tensor([10, 7, 5, 3])
>>> logits = model(x, lengths)
>>> model.freeze_encoder()  # freeze before fine-tuning
>>> logits.shape
torch.Size([4, 2])
"""

from dataclasses import dataclass
from typing import Optional
import math
import warnings

import torch
import torch.nn as nn

# Suppress the nested tensor warning from PyTorch transformer
warnings.filterwarnings("ignore", message="enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True")


@dataclass
class ModelCfg:
    """Typed configuration for :class:`TFTransformer`.

    The defaults reproduce the architecture described in the module
    documentation.  ``max_len`` governs the size of the positional encoding
    table and therefore the maximum supported sequence length (excluding the
    added ``[CLS]`` token).
    """

    n_classes: int  # number of output classes
    d_model: int = 64  # latent dimensionality after projection
    n_heads: int = 4  # multi-head attention heads per layer
    n_layers: int = 2  # number of stacked encoder layers
    d_ff: int = 128  # width of the feed-forward sublayers
    dropout: float = 0.1  # dropout probability used throughout
    max_len: int = 8192  # maximum sequence length (excluding [CLS])
    verbose: bool = False  # whether to print verbose logging information


class PositionalEncoding(nn.Module):
    """Classic sinusoidal positional encoding.

    Pre-computes sine and cosine waves of different frequencies and adds them to
    the input sequence.  A small dropout is applied afterwards as a form of
    regularisation.  The implementation mirrors the one used in
    "Attention is All You Need".
    """

    def __init__(self, d_model: int, dropout: float, max_len: int) -> None:
        super().__init__()
        # ``pe`` holds the sinusoidal table; shape ``[max_len, d_model]``.
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # register as buffer so it moves with the model but is not a parameter
        self.register_buffer("pe", pe.unsqueeze(0))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Add positional information to ``x`` and apply dropout."""

        x = x + self.pe[:, : x.size(1)]
        return self.dropout(x)


class TFTransformer(nn.Module):
    """Encoder-only Transformer for TF localisation series."""

    def __init__(self, cfg: ModelCfg) -> None:
        super().__init__()
        self.cfg = cfg
        self.verbose = cfg.verbose
        
        # Device detection and setup
        self.device = self._setup_device()
        
        if self.verbose:
            print(f"ðŸ”§ Initializing TFTransformer with config: {cfg}")
            print(f"ðŸ–¥ï¸  Using device: {self.device}")
            if torch.cuda.is_available():
                print(f"ðŸš€ CUDA available with {torch.cuda.device_count()} GPU(s)")
                for i in range(torch.cuda.device_count()):
                    print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")

        # Project the single input channel to ``d_model`` dimensions.
        self.proj = nn.Linear(1, cfg.d_model)

        # Learnable ``[CLS]`` token used to summarise the sequence.
        self.cls_token = nn.Parameter(torch.empty(1, 1, cfg.d_model))
        nn.init.trunc_normal_(self.cls_token, std=0.02)

        # Core Transformer encoder block from ``torch.nn``.
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=cfg.d_model,
            nhead=cfg.n_heads,
            dim_feedforward=cfg.d_ff,
            dropout=cfg.dropout,
            batch_first=True,
            activation="gelu",
            norm_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, cfg.n_layers)

        # Positional encoding has one extra slot for the prepended ``[CLS]``.
        self.pos_enc = PositionalEncoding(cfg.d_model, cfg.dropout, cfg.max_len + 1)

        # Final classifier acting on the ``[CLS]`` representation.
        self.classifier = nn.Linear(cfg.d_model, cfg.n_classes)
        nn.init.trunc_normal_(self.classifier.weight, std=0.02)
        nn.init.zeros_(self.classifier.bias)

        self.cls_dropout = nn.Dropout(cfg.dropout)
        
        # Move model to device and setup multi-GPU if available
        self.to(self.device)
        self._setup_multi_gpu()
        
        if self.verbose:
            total_params = sum(p.numel() for p in self.parameters())
            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
            print(f"ðŸ“Š Model initialized with {total_params:,} total parameters ({trainable_params:,} trainable)")

    def _setup_device(self) -> torch.device:
        """Setup the appropriate device (CPU/GPU)."""
        if torch.cuda.is_available():
            device = torch.device("cuda")
            if self.verbose:
                print(f"âœ… CUDA detected, using GPU: {torch.cuda.get_device_name()}")
        else:
            device = torch.device("cpu")
            if self.verbose:
                print("âš ï¸  No CUDA available, using CPU")
        return device
    
    def _setup_multi_gpu(self) -> None:
        """Setup multi-GPU training if multiple GPUs are available."""
        if torch.cuda.device_count() > 1:
            if self.verbose:
                print(f"ðŸ”„ Setting up DataParallel for {torch.cuda.device_count()} GPUs")
            # Note: We'll apply DataParallel to the core components but keep the main module structure
            # This allows for proper state_dict saving/loading
            self.encoder = nn.DataParallel(self.encoder)
            self.proj = nn.DataParallel(self.proj)
            self.classifier = nn.DataParallel(self.classifier)
            if self.verbose:
                print("âœ… Multi-GPU setup complete")
        elif self.verbose and torch.cuda.is_available():
            print("â„¹ï¸  Single GPU detected, using single GPU training")

    def freeze_encoder(self, freeze: bool = True) -> None:
        """Freeze or unfreeze the projection and encoder for transfer learning."""
        
        if self.verbose:
            action = "Freezing" if freeze else "Unfreezing"
            print(f"ðŸ”’ {action} encoder components for transfer learning")

        # Handle DataParallel wrapped modules
        proj_module = self.proj.module if isinstance(self.proj, nn.DataParallel) else self.proj
        encoder_module = self.encoder.module if isinstance(self.encoder, nn.DataParallel) else self.encoder
        
        for p in proj_module.parameters():
            p.requires_grad = not freeze
        self.cls_token.requires_grad = not freeze
        for p in encoder_module.parameters():
            p.requires_grad = not freeze
            
        if self.verbose:
            frozen_params = sum(1 for p in self.parameters() if not p.requires_grad)
            total_params = sum(1 for p in self.parameters())
            print(f"ðŸ“Š {frozen_params}/{total_params} parameters frozen")

    def reset_classifier(self) -> None:
        """Reinitialise the classifier head (useful before fine-tuning)."""
        
        if self.verbose:
            print("ðŸ”„ Resetting classifier head weights")
            
        # Handle DataParallel wrapped classifier
        classifier_module = self.classifier.module if isinstance(self.classifier, nn.DataParallel) else self.classifier
        
        nn.init.trunc_normal_(classifier_module.weight, std=0.02)
        nn.init.zeros_(classifier_module.bias)
        
        if self.verbose:
            print("âœ… Classifier head reset complete")

    @staticmethod
    def make_padding_mask(lengths: torch.Tensor, max_len: int) -> torch.Tensor:
        """Return ``True`` at indices that should be masked out.

        Parameters
        ----------
        lengths:
            Tensor of shape ``[B]`` giving the valid length of each sequence.
        max_len:
            The maximum sequence length present in the batch.
        """

        arange = torch.arange(max_len, device=lengths.device)
        return arange.expand(len(lengths), max_len) >= lengths.unsqueeze(1)

    def forward(
        self, x: torch.Tensor, lengths: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Compute logits for a batch of sequences.

        Parameters
        ----------
        x:
            Input tensor of shape ``[B, T, 1]`` containing the time series.
        lengths:
            Optional ``[B]`` tensor giving the valid sequence lengths.  When
            provided, padding positions are masked so they do not influence the
            attention mechanism.
        """
        
        if self.verbose and hasattr(self, '_forward_calls'):
            self._forward_calls += 1
        elif self.verbose:
            self._forward_calls = 1
            print(f"ðŸ”„ Starting forward pass (batch size: {x.shape[0]}, seq length: {x.shape[1]})")

        # Ensure input is on the correct device
        x = x.to(self.device)
        if lengths is not None:
            lengths = lengths.to(self.device)

        b, t, _ = x.shape
        x = self.proj(x)  # [B, T, d_model]

        # Add the ``[CLS]`` token to the front of the sequence.
        cls = self.cls_token.expand(b, -1, -1)
        x = torch.cat([cls, x], dim=1)

        # Inject position information.
        x = self.pos_enc(x)

        mask = None
        if lengths is not None:
            # Build key-padding mask and account for the extra ``[CLS]`` step.
            padding = self.make_padding_mask(lengths, t)
            cls_pad = torch.zeros(b, 1, dtype=torch.bool, device=x.device)
            mask = torch.cat([cls_pad, padding], dim=1)
            
            if self.verbose and self._forward_calls <= 3:  # Only log for first few calls
                masked_positions = mask.sum().item() if mask is not None else 0
                print(f"ðŸŽ­ Applied attention mask with {masked_positions} masked positions")

        x = self.encoder(x, src_key_padding_mask=mask)

        # Classifier operates on the ``[CLS]`` representation.
        cls_state = x[:, 0]
        cls_state = self.cls_dropout(cls_state)
        output = self.classifier(cls_state)
        
        if self.verbose and self._forward_calls <= 3:  # Only log for first few calls
            print(f"ðŸ“¤ Forward pass complete, output shape: {output.shape}")
            
        return output


    def save_model(self, filepath: str) -> None:
        """Save model state dict, handling DataParallel correctly."""
        if self.verbose:
            print(f"ðŸ’¾ Saving model to {filepath}")
            
        # Get the actual state dict, unwrapping DataParallel if necessary
        state_dict = self.state_dict()
        
        # Remove 'module.' prefix from DataParallel wrapped modules
        cleaned_state_dict = {}
        for key, value in state_dict.items():
            if key.startswith('proj.module.'):
                cleaned_state_dict[key.replace('proj.module.', 'proj.')] = value
            elif key.startswith('encoder.module.'):
                cleaned_state_dict[key.replace('encoder.module.', 'encoder.')] = value
            elif key.startswith('classifier.module.'):
                cleaned_state_dict[key.replace('classifier.module.', 'classifier.')] = value
            else:
                cleaned_state_dict[key] = value
        
        torch.save(cleaned_state_dict, filepath)
        
        if self.verbose:
            print(f"âœ… Model saved successfully")
    
    def load_model(self, filepath: str) -> None:
        """Load model state dict, handling DataParallel correctly."""
        if self.verbose:
            print(f"ðŸ“¥ Loading model from {filepath}")
            
        state_dict = torch.load(filepath, map_location=self.device)
        
        # Handle loading into DataParallel modules if necessary
        if isinstance(self.proj, nn.DataParallel):
            # Add 'module.' prefix for DataParallel wrapped modules
            adjusted_state_dict = {}
            for key, value in state_dict.items():
                if key.startswith('proj.') and not key.startswith('proj.module.'):
                    adjusted_state_dict[key.replace('proj.', 'proj.module.')] = value
                elif key.startswith('encoder.') and not key.startswith('encoder.module.'):
                    adjusted_state_dict[key.replace('encoder.', 'encoder.module.')] = value
                elif key.startswith('classifier.') and not key.startswith('classifier.module.'):
                    adjusted_state_dict[key.replace('classifier.', 'classifier.module.')] = value
                else:
                    adjusted_state_dict[key] = value
            state_dict = adjusted_state_dict
        
        self.load_state_dict(state_dict)
        
        if self.verbose:
            print(f"âœ… Model loaded successfully")


if __name__ == "__main__":
    # Test with verbose mode
    cfg = ModelCfg(n_classes=2, verbose=True)
    model = TFTransformer(cfg)
    batch = torch.randn(4, 10, 1)
    print("No lengths:", model(batch).shape)
    lens = torch.tensor([10, 7, 5, 3])
    print("With lengths:", model(batch, lens).shape)
