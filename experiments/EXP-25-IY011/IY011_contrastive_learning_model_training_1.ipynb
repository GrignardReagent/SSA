{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea9088f",
   "metadata": {},
   "source": [
    "# IY011 Contrastive Learning: Model Training\n",
    "Randomly pick pairs of samples from the dataset, randomly assign labels to each, and train a model to distinguish them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8ed081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from visualisation.plots import plot_mRNA_dist, plot_mRNA_trajectory\n",
    "# ml\n",
    "import torch, itertools\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from models.transformer import TransformerClassifier\n",
    "from training.eval import evaluate_model\n",
    "from training.train import train_model \n",
    "\n",
    "# data handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Build groups\n",
    "from utils.data_processing import build_groups\n",
    "\n",
    "import wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71487214",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/data\")\n",
    "RESULTS_PATH = DATA_ROOT / \"IY011_simulation_parameters_sobol.csv\" #  this csv file stores all the simulation parameters used\n",
    "df_params = pd.read_csv(RESULTS_PATH) \n",
    "# TRAJ_PATH = [DATA_ROOT / f\"mRNA_trajectories_mu{row['mu_target']:.3f}_cv{row['cv_target']:.3f}_tac{row['t_ac_target']:.3f}.csv\" for idx, row in df_params.iterrows()] # the trajectories \n",
    "TRAJ_PATH = [DATA_ROOT / df_params['trajectory_filename'].values[i] for i in range(len(df_params))]\n",
    "TRAJ_NPZ_PATH = [traj_file.with_suffix('.npz') for traj_file in TRAJ_PATH]\n",
    "\n",
    "# extract meta data\n",
    "parameter_sets = [{\n",
    "    'sigma_b': row['sigma_b'],\n",
    "    'sigma_u': row['sigma_u'],\n",
    "    'rho': row['rho'],\n",
    "    'd': row['d'],\n",
    "    'label': 0\n",
    "} for idx, row in df_params.iterrows()]\n",
    "time_points = np.arange(0, 3000, 1.0)\n",
    "size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8409a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building positive groups: 100%|██████████| 4/4 [00:00<00:00, 202.63it/s]\n",
      "Building negative groups: 100%|██████████| 4/4 [00:00<00:00, 173.68it/s]\n"
     ]
    }
   ],
   "source": [
    "num_traj = 500\n",
    "NUM_GROUPS = 8\n",
    "groups = build_groups(TRAJ_NPZ_PATH, num_groups=NUM_GROUPS, num_traj=num_traj) # list of tuples (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3172cb",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38807e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(groups, batch_size):\n",
    "    # Stacked groups -> individual trajectory samples\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "        L, K = Xg.shape\n",
    "        for k in range(K):\n",
    "            X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "            y_samples.append(yg)            # or some other per-trajectory label\n",
    "    X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "    y_samples = np.array(y_samples)\n",
    "    print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')\n",
    "\n",
    "    # with the stacked samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    "    )\n",
    "    X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Data preparation:\")\n",
    "    print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n",
    "    # === Standardise features (across time*batch, per-channel) ===\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Reshape 3D data to 2D for scaling\n",
    "    original_shape_train = X_train.shape\n",
    "    original_shape_val = X_val.shape\n",
    "    original_shape_test = X_test.shape\n",
    "\n",
    "    # Reshape to 2D: (batch * seq_len, features)\n",
    "    X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "    X_val_2d = scaler.transform(X_val_2d)\n",
    "    X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "    # Reshape back to 3D\n",
    "    X_train = X_train_2d.reshape(original_shape_train)\n",
    "    X_val = X_val_2d.reshape(original_shape_val)\n",
    "    X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "    # === Convert to tensors and loaders ===\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "    X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "    y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # check the data loaders\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        print(X_batch.shape, y_batch.shape)\n",
    "        break \n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f277332",
   "metadata": {},
   "source": [
    "## Transformer Model Eval\n",
    "start by defining some model & training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61f1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (4000, 1811, 1), y_samples shape: (4000,)\n",
      "Data preparation:\n",
      "  Train groups: 2560, Val groups: 640, Test groups: 800\n",
      "X_train shape: (2560, 1811, 1)\n",
      "X_val shape: (640, 1811, 1)\n",
      "X_test shape: (800, 1811, 1)\n",
      "torch.Size([64, 1811, 1]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# === Dataloader hyperparams & data prep ===\n",
    "batch_size = 64\n",
    "train_loader, val_loader, test_loader = data_prep(groups, batch_size)\n",
    "# === Dataloader hyperparams & data prep ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9631659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (input_proj): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (pe): PositionalEncoding()\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.001, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.001, inplace=False)\n",
       "  (head): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Model hyperparams ===\n",
    "input_size = 1\n",
    "num_classes = 2\n",
    "d_model=64\n",
    "nhead=4\n",
    "num_layers=2\n",
    "dropout=0.001\n",
    "use_conv1d=False \n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout, \n",
    "    use_conv1d=use_conv1d \n",
    ")\n",
    "# === Model hyperparams ===\n",
    "\n",
    "# === Training hyperparams ===\n",
    "epochs = 50\n",
    "patience = 10\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "### schedulers ### \n",
    "# 1. simple scheduler choice\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5) \n",
    "\n",
    "# 2. cosine scheduler with warmup, most commonly used for transformer\n",
    "total_steps = epochs * len(train_loader)\n",
    "warmup_steps = int(0.1 * total_steps)   # 10% warmup (good default)\n",
    "#  (from huggingface)\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ") \n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "grad_clip = 1.0\n",
    "save_path = None\n",
    "verbose = True\n",
    "\n",
    "model.to(device)\n",
    "# === Training hyperparams ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01569ceb",
   "metadata": {},
   "source": [
    "wandb logging setup: \n",
    "1. set `wandb_logging=True` in `train_model` function call to enable logging\n",
    "2. pass `wandb_config` dictionary to log hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6a416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === wandb config (required for tracking within train_model) ===\n",
    "wandb_config = {\n",
    "    \"entity\": \"grignard-reagent\",\n",
    "    \"project\": \"IY011-contrastive-learning\",\n",
    "    \"name\": f\"groups_{NUM_GROUPS}_traj_{num_traj}_batch_size_{batch_size}\", # change this to what you want\n",
    "    \"dataset\": DATA_ROOT.name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"input_size\": input_size,\n",
    "    \"d_model\": d_model,\n",
    "    \"nhead\": nhead,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"dropout\": dropout,\n",
    "    \"use_conv1d\": use_conv1d,\n",
    "    \"epochs\": epochs,\n",
    "    \"patience\": patience,\n",
    "    \"lr\": lr,\n",
    "    \"optimizer\": type(optimizer).__name__,\n",
    "    \"scheduler\": type(scheduler).__name__,\n",
    "    \"loss_fn\": type(loss_fn).__name__,\n",
    "    \"model\": type(model).__name__,\n",
    "    \"batch_size\": train_loader.batch_size,\n",
    "    \"num_traj_per_group\": num_traj,\n",
    "    \"num_groups\": NUM_GROUPS,\n",
    "}\n",
    "# === wandb config === "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d6246",
   "metadata": {},
   "source": [
    "Using modularised code for training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dd9802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrignardreagent\u001b[0m (\u001b[33mgrignard-reagent\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/wandb/run-20251123_142638-yhl2by0z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/yhl2by0z' target=\"_blank\">groups_8_traj_500_batch_size_64</a></strong> to <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/yhl2by0z' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/yhl2by0z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/50] | train_loss 0.8013 | train_acc 0.5000 | val_loss 0.8026 | val_acc 0.5000\n",
      "Epoch [2/50] | train_loss 0.6843 | train_acc 0.6098 | val_loss 0.6027 | val_acc 0.7453\n",
      "Epoch [3/50] | train_loss 0.5113 | train_acc 0.7762 | val_loss 0.4728 | val_acc 0.7844\n",
      "Epoch [4/50] | train_loss 0.4324 | train_acc 0.8273 | val_loss 0.4231 | val_acc 0.8203\n",
      "Epoch [5/50] | train_loss 0.3950 | train_acc 0.8516 | val_loss 0.3903 | val_acc 0.8516\n",
      "No improvement (1/10).\n",
      "Epoch [6/50] | train_loss 0.3726 | train_acc 0.8594 | val_loss 0.3739 | val_acc 0.8469\n",
      "No improvement (2/10).\n",
      "Epoch [7/50] | train_loss 0.3654 | train_acc 0.8598 | val_loss 0.3628 | val_acc 0.8438\n",
      "Epoch [8/50] | train_loss 0.3524 | train_acc 0.8586 | val_loss 0.3547 | val_acc 0.8547\n",
      "No improvement (1/10).\n",
      "Epoch [9/50] | train_loss 0.3322 | train_acc 0.8598 | val_loss 0.3182 | val_acc 0.8516\n",
      "Epoch [10/50] | train_loss 0.3189 | train_acc 0.8562 | val_loss 0.2837 | val_acc 0.8562\n",
      "Epoch [11/50] | train_loss 0.2750 | train_acc 0.8668 | val_loss 0.2473 | val_acc 0.8797\n",
      "No improvement (1/10).\n",
      "Epoch [12/50] | train_loss 0.2374 | train_acc 0.8934 | val_loss 0.3292 | val_acc 0.8297\n",
      "Epoch [13/50] | train_loss 0.2094 | train_acc 0.9090 | val_loss 0.2172 | val_acc 0.9109\n",
      "Epoch [14/50] | train_loss 0.2040 | train_acc 0.9199 | val_loss 0.1763 | val_acc 0.9297\n",
      "No improvement (1/10).\n",
      "Epoch [15/50] | train_loss 0.1946 | train_acc 0.9227 | val_loss 0.1679 | val_acc 0.9281\n",
      "Epoch [16/50] | train_loss 0.1612 | train_acc 0.9387 | val_loss 0.1350 | val_acc 0.9500\n",
      "No improvement (1/10).\n",
      "Epoch [17/50] | train_loss 0.1301 | train_acc 0.9508 | val_loss 0.1589 | val_acc 0.9500\n",
      "Epoch [18/50] | train_loss 0.1341 | train_acc 0.9496 | val_loss 0.1187 | val_acc 0.9578\n",
      "Epoch [19/50] | train_loss 0.1175 | train_acc 0.9617 | val_loss 0.0917 | val_acc 0.9656\n",
      "No improvement (1/10).\n",
      "Epoch [20/50] | train_loss 0.1089 | train_acc 0.9621 | val_loss 0.1255 | val_acc 0.9484\n",
      "No improvement (2/10).\n",
      "Epoch [21/50] | train_loss 0.0887 | train_acc 0.9695 | val_loss 0.1529 | val_acc 0.9375\n",
      "Epoch [22/50] | train_loss 0.0999 | train_acc 0.9641 | val_loss 0.0941 | val_acc 0.9734\n",
      "No improvement (1/10).\n",
      "Epoch [23/50] | train_loss 0.0703 | train_acc 0.9777 | val_loss 0.1136 | val_acc 0.9578\n",
      "No improvement (2/10).\n",
      "Epoch [24/50] | train_loss 0.0875 | train_acc 0.9695 | val_loss 0.1446 | val_acc 0.9531\n",
      "Epoch [25/50] | train_loss 0.0864 | train_acc 0.9664 | val_loss 0.0933 | val_acc 0.9750\n",
      "No improvement (1/10).\n",
      "Epoch [26/50] | train_loss 0.0658 | train_acc 0.9785 | val_loss 0.1256 | val_acc 0.9609\n",
      "No improvement (2/10).\n",
      "Epoch [27/50] | train_loss 0.0538 | train_acc 0.9828 | val_loss 0.1147 | val_acc 0.9672\n",
      "No improvement (3/10).\n",
      "Epoch [28/50] | train_loss 0.0614 | train_acc 0.9816 | val_loss 0.1145 | val_acc 0.9688\n",
      "No improvement (4/10).\n",
      "Epoch [29/50] | train_loss 0.0770 | train_acc 0.9762 | val_loss 0.0949 | val_acc 0.9688\n",
      "Epoch [30/50] | train_loss 0.0725 | train_acc 0.9773 | val_loss 0.0831 | val_acc 0.9781\n",
      "No improvement (1/10).\n",
      "Epoch [31/50] | train_loss 0.0480 | train_acc 0.9867 | val_loss 0.0587 | val_acc 0.9766\n",
      "No improvement (2/10).\n",
      "Epoch [32/50] | train_loss 0.0431 | train_acc 0.9879 | val_loss 0.0855 | val_acc 0.9781\n",
      "No improvement (3/10).\n",
      "Epoch [33/50] | train_loss 0.0341 | train_acc 0.9898 | val_loss 0.0752 | val_acc 0.9656\n",
      "No improvement (4/10).\n",
      "Epoch [34/50] | train_loss 0.0414 | train_acc 0.9867 | val_loss 0.1106 | val_acc 0.9672\n",
      "No improvement (5/10).\n",
      "Epoch [35/50] | train_loss 0.0334 | train_acc 0.9910 | val_loss 0.0736 | val_acc 0.9766\n",
      "No improvement (6/10).\n",
      "Epoch [36/50] | train_loss 0.0387 | train_acc 0.9879 | val_loss 0.0934 | val_acc 0.9703\n",
      "No improvement (7/10).\n",
      "Epoch [37/50] | train_loss 0.0522 | train_acc 0.9820 | val_loss 0.1154 | val_acc 0.9656\n",
      "Epoch [38/50] | train_loss 0.0286 | train_acc 0.9906 | val_loss 0.0690 | val_acc 0.9828\n",
      "No improvement (1/10).\n",
      "Epoch [39/50] | train_loss 0.0471 | train_acc 0.9859 | val_loss 0.0686 | val_acc 0.9812\n",
      "No improvement (2/10).\n",
      "Epoch [40/50] | train_loss 0.0564 | train_acc 0.9820 | val_loss 0.1395 | val_acc 0.9641\n",
      "No improvement (3/10).\n",
      "Epoch [41/50] | train_loss 0.0707 | train_acc 0.9773 | val_loss 0.1305 | val_acc 0.9703\n",
      "Epoch [42/50] | train_loss 0.0327 | train_acc 0.9922 | val_loss 0.0475 | val_acc 0.9859\n",
      "No improvement (1/10).\n",
      "Epoch [43/50] | train_loss 0.0542 | train_acc 0.9859 | val_loss 0.1028 | val_acc 0.9812\n",
      "No improvement (2/10).\n",
      "Epoch [44/50] | train_loss 0.0611 | train_acc 0.9785 | val_loss 0.3211 | val_acc 0.9344\n",
      "No improvement (3/10).\n",
      "Epoch [45/50] | train_loss 0.0787 | train_acc 0.9707 | val_loss 0.1505 | val_acc 0.9516\n",
      "No improvement (4/10).\n",
      "Epoch [46/50] | train_loss 0.0572 | train_acc 0.9840 | val_loss 0.0906 | val_acc 0.9703\n",
      "No improvement (5/10).\n",
      "Epoch [47/50] | train_loss 0.0478 | train_acc 0.9875 | val_loss 0.0948 | val_acc 0.9656\n",
      "No improvement (6/10).\n",
      "Epoch [48/50] | train_loss 0.0379 | train_acc 0.9883 | val_loss 0.1040 | val_acc 0.9750\n",
      "Epoch [49/50] | train_loss 0.0288 | train_acc 0.9902 | val_loss 0.0644 | val_acc 0.9875\n",
      "No improvement (1/10).\n",
      "Epoch [50/50] | train_loss 0.0308 | train_acc 0.9914 | val_loss 0.1415 | val_acc 0.9625\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>grad/norm</td><td>█▄▃█▆▁▂█▄▇██▄█▇▆████▆▃▆████▃███▇▅▄██▅█▁▇</td></tr><tr><td>lr</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/acc</td><td>▁▃▅▆▆▆▆▆▆▆▇▇▇▇▇█████████████████████████</td></tr><tr><td>train/loss</td><td>█▇▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/acc</td><td>▁▅▅▆▆▆▆▆▆▆▇▇▇▇▇█▇██████████████████▇████</td></tr><tr><td>val/loss</td><td>█▆▅▄▄▄▄▄▃▄▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▁▁▁▁▂▂▁▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.9875</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>grad/norm</td><td>0.85196</td></tr><tr><td>lr</td><td>0.0025</td></tr><tr><td>train/acc</td><td>0.99141</td></tr><tr><td>train/loss</td><td>0.03079</td></tr><tr><td>training_time_sec</td><td>768.58834</td></tr><tr><td>val/acc</td><td>0.9625</td></tr><tr><td>val/loss</td><td>0.14147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">groups_8_traj_500_batch_size_64</strong> at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/yhl2by0z' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/yhl2by0z</a><br> View project at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251123_142638-yhl2by0z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    lr=lr,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    grad_clip=grad_clip,\n",
    "    save_path=save_path,\n",
    "    verbose=verbose,\n",
    "    wandb_logging=True, # this enables wandb logging within train_model\n",
    "    wandb_config=wandb_config, # pass the config dictionary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3c06773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test — loss: 0.13 | acc: 0.96\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "test_loss, test_acc = evaluate_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4dc248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities on test set\n",
    "from training.eval import predict_proba, _compute_probabilities\n",
    "probs, targets = predict_proba(model, test_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9048377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9954e-01, 9.9870e-01, 1.6338e-06, 9.9996e-01, 9.4523e-01, 9.9556e-01,\n",
      "        4.1231e-12, 9.9664e-01, 1.2330e-06, 9.9963e-01])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_probs = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(X)                               # raw logits\n",
    "        probs = _compute_probabilities(logits, loss_fn) # (0,1)\n",
    "\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_targets.append(y.cpu())\n",
    "\n",
    "all_probs = torch.cat(all_probs)\n",
    "all_targets = torch.cat(all_targets)\n",
    "\n",
    "print(all_probs[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac7fd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "probs = all_probs.numpy()\n",
    "targets = all_targets.numpy()\n",
    "\n",
    "fpr, tpr, _ = roc_curve(targets, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d3751",
   "metadata": {},
   "source": [
    "## SVM Model Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8933f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (4000, 1811, 1), y_samples shape: (4000,)\n"
     ]
    }
   ],
   "source": [
    "# Stacked groups -> individual trajectory samples\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "    L, K = Xg.shape\n",
    "    for k in range(K):\n",
    "        X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "        y_samples.append(yg)            # or some other per-trajectory label\n",
    "X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "y_samples = np.array(y_samples)\n",
    "print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "345e059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation:\n",
      "  Train groups: 2560, Val groups: 640, Test groups: 800\n"
     ]
    }
   ],
   "source": [
    "# # Train/val/test with stratify on group label\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    ")\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33fcab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2560, 1811, 1)\n",
      "X_val shape: (640, 1811, 1)\n",
      "X_test shape: (800, 1811, 1)\n"
     ]
    }
   ],
   "source": [
    "# === Standardise features (across time*batch, per-channel) ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape 3D data to 2D for scaling\n",
    "original_shape_train = X_train.shape\n",
    "original_shape_val = X_val.shape\n",
    "original_shape_test = X_test.shape\n",
    "\n",
    "# Reshape to 2D: (batch * seq_len, features)\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "# Scale the data\n",
    "X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "X_val_2d = scaler.transform(X_val_2d)\n",
    "X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train = X_train_2d.reshape(original_shape_train)\n",
    "X_val = X_val_2d.reshape(original_shape_val)\n",
    "X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce7d3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1811, 1]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Torch loaders\n",
    "batch_size = 64\n",
    "\n",
    "# === Convert to tensors and loaders ===\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# check the data loaders\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17d561e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.84 ===\n"
     ]
    }
   ],
   "source": [
    "from classifiers.svm_classifier import svm_classifier\n",
    "\n",
    "# Flatten the time series data for SVM (reshape from (n_samples, seq_len, features) to (n_samples, seq_len * features))\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train_svm,\n",
    "    X_test_svm,\n",
    "    y_train,\n",
    "    y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51adb0a",
   "metadata": {},
   "source": [
    "--\n",
    "# Conclusion\n",
    "1. The \"Old Pipeline\" Flaw: Data Flattening\n",
    "In _1.ipynb (Cell 4), your data_prep function destroyed the contrastive pairs.\n",
    "\n",
    "``` Python\n",
    "\n",
    "# From IY011_contrastive_learning_model_training_1.ipynb\n",
    "def data_prep(groups, batch_size):\n",
    "    # ...\n",
    "    for Xg, yg in groups:          # Xg is a group of trajectories (e.g. 500 from File A)\n",
    "        L, K = Xg.shape\n",
    "        for k in range(K):\n",
    "            # PROBLEM: You strip the trajectory away from its partners\n",
    "            X_samples.append(Xg[:, k:k+1])  \n",
    "            y_samples.append(yg)            \n",
    "    # ...\n",
    "```\n",
    "\n",
    "What the Model Saw:\n",
    "- Input: A single trajectory $x$.\n",
    "- Task: \"Is this trajectory from a 'Positive Group' or a 'Negative Group'?\"\n",
    "\n",
    "Why this is impossible (in theory):\n",
    "- A \"Positive Group\" (Label 1) contains trajectories from File A.\n",
    "- A \"Negative Group\" (Label 0) contains trajectories from File A and File B.\n",
    "- If the model receives a trajectory $x$ from File A, the correct label could be 1 OR 0. It is ambiguous.\n",
    "\n",
    "Why it \"Converged\" (in practice): \n",
    "\n",
    "Your build_groups logic (with NUM_GROUPS=8) likely picked disjoint files for the positive and negative sets due to random sampling.\n",
    "\n",
    "- Group 1 (Pos): File X (Label 1)\n",
    "- Group 2 (Neg): File Y + File Z (Label 0)\n",
    "- The model simply learned: \"If it looks like File X, say 1. If it looks like File Y or Z, say 0.\"\n",
    "- This is just standard classification. It memorized the specific $\\mu$ and $CV$ of File X. It did not learn to compare signals. If you gave it a new File D, it would fail completely.\n",
    "\n",
    "2. Why num_traj mattered in the Old Pipeline\n",
    "You observed: \"the only times the training loop fails to converge was due to when num_traj was set to 2\"\n",
    "\n",
    "- Reason: This was purely a Dataset Size issue.\n",
    "    - With num_groups=8 and num_traj=2: Total data = 16 samples. Too small to train.\n",
    "    - With num_groups=8 and num_traj=500: Total data = 4,000 samples. Enough to train.\n",
    "    \n",
    "It converged with high num_traj because you gave it enough data to memorize File X, not because the pipeline was correct for contrastive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5096d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652c349f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b3cf3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
