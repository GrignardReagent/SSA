{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea9088f",
   "metadata": {},
   "source": [
    "# IY011 Contrastive Learning: Model Training\n",
    "Randomly pick pairs of samples from the dataset, randomly assign labels to each, and train a model to distinguish them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8ed081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from visualisation.plots import plot_mRNA_dist, plot_mRNA_trajectory\n",
    "# simulation\n",
    "from simulation.julia_simulate_telegraph_model import simulate_telegraph_model\n",
    "# ml\n",
    "import torch, itertools\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from models.transformer import TransformerClassifier\n",
    "from training.eval import evaluate_model\n",
    "from training.train import train_model \n",
    "\n",
    "# data handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.data_loader import load_and_split_data\n",
    "from utils.data_processing import add_binary_labels\n",
    "from utils.standardise_time_series import standardise_time_series\n",
    "from utils.steady_state import find_steady_state\n",
    "\n",
    "# Build groups\n",
    "from utils.data_processing import build_groups\n",
    "\n",
    "import wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71487214",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/data\")\n",
    "RESULTS_PATH = DATA_ROOT / \"IY011_simulation_parameters_sobol.csv\" #  this csv file stores all the simulation parameters used\n",
    "df_params = pd.read_csv(RESULTS_PATH) \n",
    "# TRAJ_PATH = [DATA_ROOT / f\"mRNA_trajectories_mu{row['mu_target']:.3f}_cv{row['cv_target']:.3f}_tac{row['t_ac_target']:.3f}.csv\" for idx, row in df_params.iterrows()] # the trajectories \n",
    "TRAJ_PATH = [DATA_ROOT / df_params['trajectory_filename'].values[i] for i in range(len(df_params))]\n",
    "TRAJ_NPZ_PATH = [traj_file.with_suffix('.npz') for traj_file in TRAJ_PATH]\n",
    "\n",
    "# extract meta data\n",
    "parameter_sets = [{\n",
    "    'sigma_b': row['sigma_b'],\n",
    "    'sigma_u': row['sigma_u'],\n",
    "    'rho': row['rho'],\n",
    "    'd': row['d'],\n",
    "    'label': 0\n",
    "} for idx, row in df_params.iterrows()]\n",
    "time_points = np.arange(0, 3000, 1.0)\n",
    "size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8409a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building positive groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 181.53it/s]\n",
      "Building negative groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 158.73it/s]\n"
     ]
    }
   ],
   "source": [
    "num_traj = 500\n",
    "NUM_GROUPS = 2 # a pair: 1 pos, 1 neg\n",
    "groups = build_groups(TRAJ_NPZ_PATH, num_groups=NUM_GROUPS, num_traj=num_traj) # list of tuples (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3172cb",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38807e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (1000, 1811, 1), y_samples shape: (1000,)\n",
      "Data preparation:\n",
      "  Train groups: 640, Val groups: 160, Test groups: 200\n",
      "X_train shape: (640, 1811, 1)\n",
      "X_val shape: (160, 1811, 1)\n",
      "X_test shape: (200, 1811, 1)\n",
      "torch.Size([2, 1811, 1]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "def data_prep(groups, NUM_GROUPS):\n",
    "    # Stacked groups -> individual trajectory samples\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "        L, K = Xg.shape\n",
    "        for k in range(K):\n",
    "            X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "            y_samples.append(yg)            # or some other per-trajectory label\n",
    "    X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "    y_samples = np.array(y_samples)\n",
    "    print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')\n",
    "\n",
    "    # with the stacked samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    "    )\n",
    "    X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Data preparation:\")\n",
    "    print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n",
    "    # === Standardise features (across time*batch, per-channel) ===\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Reshape 3D data to 2D for scaling\n",
    "    original_shape_train = X_train.shape\n",
    "    original_shape_val = X_val.shape\n",
    "    original_shape_test = X_test.shape\n",
    "\n",
    "    # Reshape to 2D: (batch * seq_len, features)\n",
    "    X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "    X_val_2d = scaler.transform(X_val_2d)\n",
    "    X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "    # Reshape back to 3D\n",
    "    X_train = X_train_2d.reshape(original_shape_train)\n",
    "    X_val = X_val_2d.reshape(original_shape_val)\n",
    "    X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    \n",
    "    # Torch loaders\n",
    "    batch_size = NUM_GROUPS\n",
    "\n",
    "    # === Convert to tensors and loaders ===\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "    X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "    y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # check the data loaders\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        print(X_batch.shape, y_batch.shape)\n",
    "        break \n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "train_loader, val_loader, test_loader = data_prep(groups, NUM_GROUPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f277332",
   "metadata": {},
   "source": [
    "## Transformer Model Eval\n",
    "start by defining some model & training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9631659a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (input_proj): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (pe): PositionalEncoding()\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.001, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.001, inplace=False)\n",
       "  (head): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# === Model hyperparams ===\n",
    "input_size = 1\n",
    "num_classes = 2\n",
    "d_model=64\n",
    "nhead=4\n",
    "num_layers=2\n",
    "dropout=0.001\n",
    "use_conv1d=False \n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout, \n",
    "    use_conv1d=use_conv1d \n",
    ")\n",
    "# === Model hyperparams ===\n",
    "\n",
    "# === Training hyperparams ===\n",
    "epochs = 50\n",
    "patience = 10\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "grad_clip = 1.0\n",
    "save_path = None\n",
    "verbose = True\n",
    "\n",
    "model.to(device)\n",
    "# === Training hyperparams ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82897fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrignardreagent\u001b[0m (\u001b[33mgrignard-reagent\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/wandb/run-20251114_113246-s030y2uz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/s030y2uz' target=\"_blank\">groups_4_traj_500_random_neg_split</a></strong> to <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/s030y2uz' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/s030y2uz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Init wandb run ===\n",
    "wandb_config = {\n",
    "    \"dataset\": DATA_ROOT.name,\n",
    "    \"input_size\": input_size,\n",
    "    \"d_model\": d_model,\n",
    "    \"nhead\": nhead,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"dropout\": dropout,\n",
    "    \"use_conv1d\": use_conv1d,\n",
    "    \"epochs\": epochs,\n",
    "    \"patience\": patience,\n",
    "    \"lr\": lr,\n",
    "    \"optimizer\": type(optimizer).__name__,\n",
    "    \"loss_fn\": type(loss_fn).__name__,\n",
    "    \"model\": type(model).__name__,\n",
    "    \"batch_size\": getattr(train_loader, \"batch_size\", None),\n",
    "    \"num_traj_per_group\": num_traj,\n",
    "    \"num_groups\": NUM_GROUPS\n",
    "}\n",
    "run = wandb.init(entity=\"grignard-reagent\",\n",
    "                 project=\"IY011-contrastive-learning\",\n",
    "                 name=f\"groups_{NUM_GROUPS}_traj_{num_traj}_random_neg_split\",\n",
    "                 config=wandb_config)\n",
    "# === Init wandb run ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ef374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | train_loss 0.2568 | train_acc 0.9656 | val_loss 0.1522 | val_acc 0.9875\n",
      "No improvement (1/10).\n",
      "Epoch [2/50] | train_loss 0.2727 | train_acc 0.9688 | val_loss 0.0756 | val_acc 0.9719\n",
      "No improvement (2/10).\n",
      "Epoch [3/50] | train_loss 0.1651 | train_acc 0.9719 | val_loss 0.0646 | val_acc 0.9812\n",
      "No improvement (3/10).\n",
      "Epoch [4/50] | train_loss 0.1989 | train_acc 0.9742 | val_loss 0.0917 | val_acc 0.9719\n",
      "No improvement (4/10).\n",
      "Epoch [5/50] | train_loss 0.1800 | train_acc 0.9805 | val_loss 0.0549 | val_acc 0.9812\n",
      "No improvement (5/10).\n",
      "Epoch [6/50] | train_loss 0.1876 | train_acc 0.9789 | val_loss 0.0931 | val_acc 0.9719\n",
      "No improvement (6/10).\n",
      "Epoch [7/50] | train_loss 0.1529 | train_acc 0.9828 | val_loss 0.1449 | val_acc 0.9719\n",
      "No improvement (7/10).\n",
      "Epoch [8/50] | train_loss 0.1580 | train_acc 0.9766 | val_loss 0.0838 | val_acc 0.9844\n",
      "Epoch [9/50] | train_loss 0.1551 | train_acc 0.9812 | val_loss 0.0907 | val_acc 0.9906\n",
      "No improvement (1/10).\n",
      "Epoch [10/50] | train_loss 0.1499 | train_acc 0.9773 | val_loss 0.2765 | val_acc 0.9781\n",
      "No improvement (2/10).\n",
      "Epoch [11/50] | train_loss 0.2395 | train_acc 0.9781 | val_loss 0.0703 | val_acc 0.9812\n",
      "No improvement (3/10).\n",
      "Epoch [12/50] | train_loss 0.2640 | train_acc 0.9734 | val_loss 0.1932 | val_acc 0.9781\n",
      "No improvement (4/10).\n",
      "Epoch [13/50] | train_loss 0.1496 | train_acc 0.9797 | val_loss 0.1541 | val_acc 0.9844\n",
      "No improvement (5/10).\n",
      "Epoch [14/50] | train_loss 0.1392 | train_acc 0.9797 | val_loss 1.0044 | val_acc 0.9500\n",
      "No improvement (6/10).\n",
      "Epoch [15/50] | train_loss 0.1460 | train_acc 0.9898 | val_loss 0.2393 | val_acc 0.9750\n",
      "No improvement (7/10).\n",
      "Epoch [16/50] | train_loss 0.1758 | train_acc 0.9797 | val_loss 0.2514 | val_acc 0.9750\n",
      "No improvement (8/10).\n",
      "Epoch [17/50] | train_loss 0.1944 | train_acc 0.9805 | val_loss 0.2066 | val_acc 0.9719\n",
      "No improvement (9/10).\n",
      "Epoch [18/50] | train_loss 0.1598 | train_acc 0.9836 | val_loss 0.7513 | val_acc 0.9625\n",
      "No improvement (10/10).\n",
      "üõë Early stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>grad/norm</td><td>‚ñÅ‚ñÜ‚ñÅ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>lr</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñà‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÉ</td></tr><tr><td>val/loss</td><td>‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.99062</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>grad/norm</td><td>0</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train/acc</td><td>0.98359</td></tr><tr><td>train/loss</td><td>0.15984</td></tr><tr><td>training_time_sec</td><td>154.25392</td></tr><tr><td>val/acc</td><td>0.9625</td></tr><tr><td>val/loss</td><td>0.75126</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">groups_4_traj_500_random_neg_split</strong> at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/s030y2uz' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/s030y2uz</a><br> View project at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251114_113246-s030y2uz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "best_val_acc = -1.0\n",
    "epochs_no_improve = 0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        # adjust targets if BCE-type loss\n",
    "        y_batch_mod = y_batch\n",
    "        if isinstance(loss_fn, (nn.BCEWithLogitsLoss, nn.BCELoss)):\n",
    "            y_batch_mod = y_batch_mod.float().unsqueeze(1) if y_batch_mod.dim() == 1 else y_batch_mod.float()\n",
    "            if outputs.dim() == 2 and outputs.size(1) == 2 and y_batch_mod.size(1) == 1:\n",
    "                outputs = outputs[:, 1].unsqueeze(1)\n",
    "\n",
    "        loss = loss_fn(outputs, y_batch_mod)\n",
    "        loss.backward()\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # compute accuracy\n",
    "        if isinstance(loss_fn, (nn.BCEWithLogitsLoss, nn.BCELoss)):\n",
    "            probs = torch.sigmoid(outputs).view(-1)\n",
    "            preds = (probs > 0.5).long()\n",
    "            tgt = y_batch.view(-1).long()\n",
    "        else:\n",
    "            preds = outputs.argmax(1)\n",
    "            tgt = y_batch\n",
    "        correct += (preds == tgt).sum().item()\n",
    "        total += tgt.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc = (None, None)\n",
    "    if val_loader is not None:\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc is not None and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"‚úÖ Model saved at {save_path} (Best Val Acc: {best_val_acc:.4f})\")\n",
    "                # also upload to wandb\n",
    "                try:\n",
    "                    wandb.save(save_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if verbose:\n",
    "                print(f\"No improvement ({epochs_no_improve}/{patience}).\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"üõë Early stopping.\")\n",
    "            break\n",
    "\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    # Log metrics to wandb each epoch\n",
    "    log_dict = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/acc\": train_acc,\n",
    "    }\n",
    "    if val_loss is not None:\n",
    "        log_dict.update({\"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "    # also log current LR\n",
    "    try:\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_dict[\"lr\"] = current_lr\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # optional: log gradient norm (approx)\n",
    "    total_grad_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "    total_grad_norm = total_grad_norm ** 0.5 if total_grad_norm > 0 else 0.0\n",
    "    log_dict[\"grad/norm\"] = total_grad_norm\n",
    "\n",
    "    run.log(log_dict)\n",
    "\n",
    "    if verbose:\n",
    "        msg = f\"Epoch [{epoch+1}/{epochs}] | train_loss {train_loss:.4f} | train_acc {train_acc:.4f}\"\n",
    "        if val_loader is not None:\n",
    "            msg += f\" | val_loss {val_loss:.4f} | val_acc {val_acc:.4f}\"\n",
    "        print(msg)\n",
    "\n",
    "# final save + finish wandb run\n",
    "elapsed = time.time() - start_time\n",
    "run.summary[\"training_time_sec\"] = elapsed\n",
    "run.summary[\"best_val_acc\"] = best_val_acc\n",
    "run.finish()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d6246",
   "metadata": {},
   "source": [
    "Using modularised code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrignardreagent\u001b[0m (\u001b[33mgrignard-reagent\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/wandb/run-20251116_220052-utyxtatm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/utyxtatm' target=\"_blank\">groups_2_traj_500_random_neg_split</a></strong> to <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/utyxtatm' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/utyxtatm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/50] | train_loss 0.1577 | train_acc 0.9891 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (1/10).\n",
      "Epoch [2/50] | train_loss 0.0939 | train_acc 0.9953 | val_loss 0.0793 | val_acc 0.9938\n",
      "No improvement (2/10).\n",
      "Epoch [3/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (3/10).\n",
      "Epoch [4/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (4/10).\n",
      "Epoch [5/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (5/10).\n",
      "Epoch [6/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (6/10).\n",
      "Epoch [7/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (7/10).\n",
      "Epoch [8/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (8/10).\n",
      "Epoch [9/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (9/10).\n",
      "Epoch [10/50] | train_loss 0.0000 | train_acc 1.0000 | val_loss 0.0000 | val_acc 1.0000\n",
      "No improvement (10/10).\n",
      "üõë Early stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>grad/norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>lr</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/acc</td><td>‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/loss</td><td>‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>1</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>grad/norm</td><td>0</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train/acc</td><td>1</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>training_time_sec</td><td>46.04501</td></tr><tr><td>val/acc</td><td>1</td></tr><tr><td>val/loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">groups_2_traj_500_random_neg_split</strong> at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/utyxtatm' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/utyxtatm</a><br> View project at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251116_220052-utyxtatm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# === wandb config (required for tracking within train_model) ===\n",
    "wandb_config = {\n",
    "    \"entity\": \"grignard-reagent\",\n",
    "    \"project\": \"IY011-contrastive-learning\",\n",
    "    \"name\": f\"groups_{NUM_GROUPS}_traj_{num_traj}_random_neg_split\", # change this to what you want\n",
    "    \"dataset\": DATA_ROOT.name,\n",
    "    \"input_size\": input_size,\n",
    "    \"d_model\": d_model,\n",
    "    \"nhead\": nhead,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"dropout\": dropout,\n",
    "    \"use_conv1d\": use_conv1d,\n",
    "    \"epochs\": epochs,\n",
    "    \"patience\": patience,\n",
    "    \"lr\": lr,\n",
    "    \"optimizer\": type(optimizer).__name__,\n",
    "    \"loss_fn\": type(loss_fn).__name__,\n",
    "    \"model\": type(model).__name__,\n",
    "    \"batch_size\": train_loader.batch_size,\n",
    "    \"num_traj_per_group\": num_traj,\n",
    "    \"num_groups\": NUM_GROUPS,\n",
    "}\n",
    "# === wandb config === \n",
    "\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=epochs,\n",
    "    patience=patience,\n",
    "    lr=lr,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    grad_clip=grad_clip,\n",
    "    save_path=save_path,\n",
    "    verbose=verbose,\n",
    "    wandb_logging=True,\n",
    "    wandb_config=wandb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d3751",
   "metadata": {},
   "source": [
    "## SVM Model Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8933f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (2000, 1811, 1), y_samples shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Stacked groups -> individual trajectory samples\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "    L, K = Xg.shape\n",
    "    for k in range(K):\n",
    "        X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "        y_samples.append(yg)            # or some other per-trajectory label\n",
    "X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "y_samples = np.array(y_samples)\n",
    "print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345e059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation:\n",
      "  Train groups: 1280, Val groups: 320, Test groups: 400\n"
     ]
    }
   ],
   "source": [
    "# # Train/val/test with stratify on group label\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    ")\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33fcab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1280, 1811, 1)\n",
      "X_val shape: (320, 1811, 1)\n",
      "X_test shape: (400, 1811, 1)\n"
     ]
    }
   ],
   "source": [
    "# === Standardise features (across time*batch, per-channel) ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape 3D data to 2D for scaling\n",
    "original_shape_train = X_train.shape\n",
    "original_shape_val = X_val.shape\n",
    "original_shape_test = X_test.shape\n",
    "\n",
    "# Reshape to 2D: (batch * seq_len, features)\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "# Scale the data\n",
    "X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "X_val_2d = scaler.transform(X_val_2d)\n",
    "X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train = X_train_2d.reshape(original_shape_train)\n",
    "X_val = X_val_2d.reshape(original_shape_val)\n",
    "X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cce7d3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1811, 1]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Torch loaders\n",
    "batch_size = 64\n",
    "\n",
    "# === Convert to tensors and loaders ===\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# check the data loaders\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17d561e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.94 ===\n"
     ]
    }
   ],
   "source": [
    "from classifiers.svm_classifier import svm_classifier\n",
    "\n",
    "# Flatten the time series data for SVM (reshape from (n_samples, seq_len, features) to (n_samples, seq_len * features))\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train_svm,\n",
    "    X_test_svm,\n",
    "    y_train,\n",
    "    y_test,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
