{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea9088f",
   "metadata": {},
   "source": [
    "# IY011 Contrastive Learning: Model Training\n",
    "Randomly pick pairs of samples from the dataset, randomly assign labels to each, and train a model to distinguish them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8ed081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from visualisation.plots import plot_mRNA_dist, plot_mRNA_trajectory\n",
    "# simulation\n",
    "from simulation.julia_simulate_telegraph_model import simulate_telegraph_model\n",
    "# ml\n",
    "import torch, itertools\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from classifiers.transformer_classifier import transformer_classifier\n",
    "from models.simple_transformer import SimpleTransformer\n",
    "from models.transformer import TransformerClassifier, train_model, evaluate_model\n",
    "# data handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.load_data import load_and_split_data\n",
    "from utils.data_processing import add_binary_labels\n",
    "from utils.standardise_time_series import standardise_time_series\n",
    "from utils.steady_state import find_steady_state\n",
    "\n",
    "# Build groups\n",
    "from utils.data_processing import build_groups\n",
    "\n",
    "import wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71487214",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/data\")\n",
    "RESULTS_PATH = DATA_ROOT / \"IY011_simulation_parameters_sobol.csv\" #  this csv file stores all the simulation parameters used\n",
    "df_params = pd.read_csv(RESULTS_PATH) \n",
    "# TRAJ_PATH = [DATA_ROOT / f\"mRNA_trajectories_mu{row['mu_target']:.3f}_cv{row['cv_target']:.3f}_tac{row['t_ac_target']:.3f}.csv\" for idx, row in df_params.iterrows()] # the trajectories \n",
    "TRAJ_PATH = [DATA_ROOT / df_params['trajectory_filename'].values[i] for i in range(len(df_params))]\n",
    "TRAJ_NPZ_PATH = [traj_file.with_suffix('.npz') for traj_file in TRAJ_PATH]\n",
    "\n",
    "# extract meta data\n",
    "parameter_sets = [{\n",
    "    'sigma_b': row['sigma_b'],\n",
    "    'sigma_u': row['sigma_u'],\n",
    "    'rho': row['rho'],\n",
    "    'd': row['d'],\n",
    "    'label': 0\n",
    "} for idx, row in df_params.iterrows()]\n",
    "time_points = np.arange(0, 3000, 1.0)\n",
    "size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8409a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building positive groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 237.45it/s]\n",
      "Building negative groups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 223.68it/s]\n"
     ]
    }
   ],
   "source": [
    "num_traj = 500\n",
    "NUM_GROUPS = 6  # a pair: 1 pos, 1 neg\n",
    "groups = build_groups(TRAJ_NPZ_PATH, num_groups=NUM_GROUPS, num_traj=num_traj) # list of tuples (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f6285f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[ 448.,  640., 3033., ...,  791., 2449., 4989.],\n",
       "         [ 429.,  615., 2939., ...,  768., 2360., 4821.],\n",
       "         [ 417.,  598., 2843., ...,  741., 2291., 4654.],\n",
       "         ...,\n",
       "         [1646., 1872.,   48., ..., 3108., 1965., 3051.],\n",
       "         [1595., 1812.,   46., ..., 3001., 1910., 2960.],\n",
       "         [1544., 1752.,   46., ..., 2912., 1851., 2869.]],\n",
       "        shape=(1811, 500), dtype=float32),\n",
       "  1),\n",
       " (array([[ 9576., 11189., 17969., ...,  2469.,     0.,  1050.],\n",
       "         [ 9465., 11053., 17789., ...,  2264.,     0.,   935.],\n",
       "         [ 9365., 10931., 17589., ...,  2060.,     0.,   853.],\n",
       "         ...,\n",
       "         [ 4240.,  1386.,  5725., ...,   253.,  1576., 31987.],\n",
       "         [ 4199.,  1373.,  5661., ...,   218.,  1433., 28943.],\n",
       "         [ 4164.,  1352.,  5615., ...,   209.,  1306., 26245.]],\n",
       "        shape=(1811, 500), dtype=float32),\n",
       "  0),\n",
       " (array([[2254.,   96., 1108., ..., 4067., 2857.,  924.],\n",
       "         [2229.,   95., 1098., ..., 4028., 2835.,  915.],\n",
       "         [2207.,   91., 1084., ..., 3973., 2812.,  907.],\n",
       "         ...,\n",
       "         [1673.,  481.,  193., ..., 2443.,  601., 1269.],\n",
       "         [1653.,  480.,  459., ..., 2424.,  593., 1255.],\n",
       "         [1640.,  473.,  455., ..., 2403.,  587., 1236.]],\n",
       "        shape=(1811, 500), dtype=float32),\n",
       "  1),\n",
       " (array([[ 9076.,  2602.,  8146., ..., 14241.,    40.,   150.],\n",
       "         [ 8970.,  2567.,  8063., ..., 14111.,    40.,   147.],\n",
       "         [ 8865.,  2543.,  7976., ..., 14003.,    40.,   145.],\n",
       "         ...,\n",
       "         [ 4664.,  3842., 10207., ...,  7188.,   202.,   437.],\n",
       "         [ 4623.,  3802., 10098., ...,  7113.,   201.,   434.],\n",
       "         [ 4566.,  3766.,  9989., ...,  7044.,   199.,   434.]],\n",
       "        shape=(1811, 500), dtype=float32),\n",
       "  0),\n",
       " (array([[1.530e+02, 1.225e+03, 1.000e+00, ..., 1.500e+01, 1.067e+03,\n",
       "          3.000e+01],\n",
       "         [1.480e+02, 1.202e+03, 1.000e+00, ..., 1.400e+01, 1.050e+03,\n",
       "          3.000e+01],\n",
       "         [1.470e+02, 1.177e+03, 1.000e+00, ..., 1.400e+01, 1.030e+03,\n",
       "          3.000e+01],\n",
       "         ...,\n",
       "         [7.700e+01, 4.300e+01, 6.060e+02, ..., 1.530e+02, 3.770e+02,\n",
       "          5.810e+02],\n",
       "         [7.600e+01, 4.300e+01, 5.960e+02, ..., 1.510e+02, 3.740e+02,\n",
       "          5.710e+02],\n",
       "         [7.500e+01, 4.200e+01, 5.880e+02, ..., 1.500e+02, 3.670e+02,\n",
       "          5.630e+02]], shape=(1811, 500), dtype=float32),\n",
       "  1),\n",
       " (array([[ 5321., 19609.,  1601., ...,  8748.,  2668.,  2791.],\n",
       "         [ 5021., 18539.,  1534., ...,  8621.,  2626.,  2748.],\n",
       "         [ 4753., 17588.,  1463., ...,  8505.,  2595.,  2708.],\n",
       "         ...,\n",
       "         [10808.,  2966.,   358., ..., 12435.,  7497.,  3441.],\n",
       "         [10242.,  2791.,   334., ..., 12256.,  7399.,  3390.],\n",
       "         [ 9705.,  2630.,   311., ..., 12074.,  7294.,  3339.]],\n",
       "        shape=(1811, 500), dtype=float32),\n",
       "  0)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3172cb",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38807e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (3000, 1811, 1), y_samples shape: (3000,)\n",
      "Data preparation:\n",
      "  Train groups: 1920, Val groups: 480, Test groups: 600\n",
      "X_train shape: (1920, 1811, 1)\n",
      "X_val shape: (480, 1811, 1)\n",
      "X_test shape: (600, 1811, 1)\n",
      "torch.Size([6, 1811, 1]) torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "def data_prep(groups, NUM_GROUPS):\n",
    "    # Stacked groups -> individual trajectory samples\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "    for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "        L, K = Xg.shape\n",
    "        for k in range(K):\n",
    "            X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "            y_samples.append(yg)            # or some other per-trajectory label\n",
    "    X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "    y_samples = np.array(y_samples)\n",
    "    print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')\n",
    "\n",
    "    # with the stacked samples\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    "    )\n",
    "    X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    print(\"Data preparation:\")\n",
    "    print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n",
    "    # === Standardise features (across time*batch, per-channel) ===\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Reshape 3D data to 2D for scaling\n",
    "    original_shape_train = X_train.shape\n",
    "    original_shape_val = X_val.shape\n",
    "    original_shape_test = X_test.shape\n",
    "\n",
    "    # Reshape to 2D: (batch * seq_len, features)\n",
    "    X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "    X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "    # Scale the data\n",
    "    X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "    X_val_2d = scaler.transform(X_val_2d)\n",
    "    X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "    # Reshape back to 3D\n",
    "    X_train = X_train_2d.reshape(original_shape_train)\n",
    "    X_val = X_val_2d.reshape(original_shape_val)\n",
    "    X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_val shape:\", X_val.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    \n",
    "    # Torch loaders\n",
    "    batch_size = NUM_GROUPS\n",
    "\n",
    "    # === Convert to tensors and loaders ===\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "    X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "    y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "    val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # check the data loaders\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        print(X_batch.shape, y_batch.shape)\n",
    "        break \n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "    \n",
    "train_loader, val_loader, test_loader = data_prep(groups, NUM_GROUPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f277332",
   "metadata": {},
   "source": [
    "## Transformer Model Eval\n",
    " Start a new wandb run to track this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9631659a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (input_proj): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (pe): PositionalEncoding()\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.001, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.001, inplace=False)\n",
       "        (dropout2): Dropout(p=0.001, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.001, inplace=False)\n",
       "  (head): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# === Model hyperparams ===\n",
    "input_size = 1\n",
    "num_classes = 2\n",
    "d_model=64\n",
    "nhead=4\n",
    "num_layers=2\n",
    "dropout=0.001\n",
    "use_conv1d=False \n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    dropout=dropout, \n",
    "    use_conv1d=use_conv1d \n",
    ")\n",
    "# === Model hyperparams ===\n",
    "\n",
    "# === Training hyperparams ===\n",
    "epochs = 50\n",
    "patience = 10\n",
    "lr = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "grad_clip = 1.0\n",
    "save_path = None\n",
    "verbose = True\n",
    "\n",
    "model.to(device)\n",
    "# === Training hyperparams ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82897fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/wandb/run-20251112_150454-olgdixnb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/olgdixnb' target=\"_blank\">groups_6_traj_500_random_neg_split</a></strong> to <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/olgdixnb' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/olgdixnb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Init wandb run ===\n",
    "wandb_config = {\n",
    "    \"dataset\": DATA_ROOT.name,\n",
    "    \"input_size\": input_size,\n",
    "    \"d_model\": d_model,\n",
    "    \"nhead\": nhead,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"dropout\": dropout,\n",
    "    \"use_conv1d\": use_conv1d,\n",
    "    \"epochs\": epochs,\n",
    "    \"patience\": patience,\n",
    "    \"lr\": lr,\n",
    "    \"optimizer\": type(optimizer).__name__,\n",
    "    \"loss_fn\": type(loss_fn).__name__,\n",
    "    \"model\": type(model).__name__,\n",
    "    \"batch_size\": getattr(train_loader, \"batch_size\", None),\n",
    "    \"num_traj_per_group\": num_traj,\n",
    "    \"num_groups\": NUM_GROUPS\n",
    "}\n",
    "run = wandb.init(entity=\"grignard-reagent\",\n",
    "                 project=\"IY011-contrastive-learning\",\n",
    "                 name=f\"groups_{NUM_GROUPS}_traj_{num_traj}_random_neg_split\",\n",
    "                 config=wandb_config)\n",
    "# === Init wandb run ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1ef374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | train_loss 0.2729 | train_acc 0.9578 | val_loss 1.5516 | val_acc 0.9104\n",
      "Epoch [2/50] | train_loss 0.2439 | train_acc 0.9714 | val_loss 0.2718 | val_acc 0.9667\n",
      "Epoch [3/50] | train_loss 0.1548 | train_acc 0.9755 | val_loss 0.1564 | val_acc 0.9750\n",
      "No improvement (1/10).\n",
      "Epoch [4/50] | train_loss 0.1901 | train_acc 0.9651 | val_loss 0.1385 | val_acc 0.9667\n",
      "No improvement (2/10).\n",
      "Epoch [5/50] | train_loss 0.2637 | train_acc 0.9734 | val_loss 0.2000 | val_acc 0.9708\n",
      "No improvement (3/10).\n",
      "Epoch [6/50] | train_loss 0.1432 | train_acc 0.9760 | val_loss 0.2723 | val_acc 0.9688\n",
      "No improvement (4/10).\n",
      "Epoch [7/50] | train_loss 0.1896 | train_acc 0.9714 | val_loss 0.2721 | val_acc 0.9729\n",
      "No improvement (5/10).\n",
      "Epoch [8/50] | train_loss 0.1550 | train_acc 0.9797 | val_loss 0.2406 | val_acc 0.9729\n",
      "No improvement (6/10).\n",
      "Epoch [9/50] | train_loss 0.1239 | train_acc 0.9802 | val_loss 0.1635 | val_acc 0.9708\n",
      "No improvement (7/10).\n",
      "Epoch [10/50] | train_loss 0.1294 | train_acc 0.9786 | val_loss 0.1575 | val_acc 0.9708\n",
      "No improvement (8/10).\n",
      "Epoch [11/50] | train_loss 0.1451 | train_acc 0.9797 | val_loss 0.1931 | val_acc 0.9479\n",
      "No improvement (9/10).\n",
      "Epoch [12/50] | train_loss 0.2735 | train_acc 0.9766 | val_loss 0.1820 | val_acc 0.9708\n",
      "No improvement (10/10).\n",
      "üõë Early stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>grad/norm</td><td>‚ñÅ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>lr</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÇ‚ñÑ‚ñà‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñà</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñà</td></tr><tr><td>val/loss</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0.975</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>grad/norm</td><td>0.08168</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>train/acc</td><td>0.97656</td></tr><tr><td>train/loss</td><td>0.27348</td></tr><tr><td>training_time_sec</td><td>156.78121</td></tr><tr><td>val/acc</td><td>0.97083</td></tr><tr><td>val/loss</td><td>0.18197</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">groups_6_traj_500_random_neg_split</strong> at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/olgdixnb' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning/runs/olgdixnb</a><br> View project at: <a href='https://wandb.ai/grignard-reagent/IY011-contrastive-learning' target=\"_blank\">https://wandb.ai/grignard-reagent/IY011-contrastive-learning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251112_150454-olgdixnb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "best_val_acc = -1.0\n",
    "epochs_no_improve = 0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        # adjust targets if BCE-type loss\n",
    "        y_batch_mod = y_batch\n",
    "        if isinstance(loss_fn, (nn.BCEWithLogitsLoss, nn.BCELoss)):\n",
    "            y_batch_mod = y_batch_mod.float().unsqueeze(1) if y_batch_mod.dim() == 1 else y_batch_mod.float()\n",
    "            if outputs.dim() == 2 and outputs.size(1) == 2 and y_batch_mod.size(1) == 1:\n",
    "                outputs = outputs[:, 1].unsqueeze(1)\n",
    "\n",
    "        loss = loss_fn(outputs, y_batch_mod)\n",
    "        loss.backward()\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        # compute accuracy\n",
    "        if isinstance(loss_fn, (nn.BCEWithLogitsLoss, nn.BCELoss)):\n",
    "            probs = torch.sigmoid(outputs).view(-1)\n",
    "            preds = (probs > 0.5).long()\n",
    "            tgt = y_batch.view(-1).long()\n",
    "        else:\n",
    "            preds = outputs.argmax(1)\n",
    "            tgt = y_batch\n",
    "        correct += (preds == tgt).sum().item()\n",
    "        total += tgt.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_acc = correct / total\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc = (None, None)\n",
    "    if val_loader is not None:\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc is not None and val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            if save_path:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"‚úÖ Model saved at {save_path} (Best Val Acc: {best_val_acc:.4f})\")\n",
    "                # also upload to wandb\n",
    "                try:\n",
    "                    wandb.save(save_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if verbose:\n",
    "                print(f\"No improvement ({epochs_no_improve}/{patience}).\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"üõë Early stopping.\")\n",
    "            break\n",
    "\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    # Log metrics to wandb each epoch\n",
    "    log_dict = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/acc\": train_acc,\n",
    "    }\n",
    "    if val_loss is not None:\n",
    "        log_dict.update({\"val/loss\": val_loss, \"val/acc\": val_acc})\n",
    "    # also log current LR\n",
    "    try:\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        log_dict[\"lr\"] = current_lr\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # optional: log gradient norm (approx)\n",
    "    total_grad_norm = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            total_grad_norm += p.grad.data.norm(2).item() ** 2\n",
    "    total_grad_norm = total_grad_norm ** 0.5 if total_grad_norm > 0 else 0.0\n",
    "    log_dict[\"grad/norm\"] = total_grad_norm\n",
    "\n",
    "    run.log(log_dict)\n",
    "\n",
    "    if verbose:\n",
    "        msg = f\"Epoch [{epoch+1}/{epochs}] | train_loss {train_loss:.4f} | train_acc {train_acc:.4f}\"\n",
    "        if val_loader is not None:\n",
    "            msg += f\" | val_loss {val_loss:.4f} | val_acc {val_acc:.4f}\"\n",
    "        print(msg)\n",
    "\n",
    "# final save + finish wandb run\n",
    "elapsed = time.time() - start_time\n",
    "run.summary[\"training_time_sec\"] = elapsed\n",
    "run.summary[\"best_val_acc\"] = best_val_acc\n",
    "run.finish()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d3751",
   "metadata": {},
   "source": [
    "## SVM Model Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8933f0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (3000, 1811, 1), y_samples shape: (3000,)\n"
     ]
    }
   ],
   "source": [
    "# Stacked groups -> individual trajectory samples\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "    L, K = Xg.shape\n",
    "    for k in range(K):\n",
    "        X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "        y_samples.append(yg)            # or some other per-trajectory label\n",
    "X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "y_samples = np.array(y_samples)\n",
    "print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "345e059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation:\n",
      "  Train groups: 1920, Val groups: 480, Test groups: 600\n"
     ]
    }
   ],
   "source": [
    "# # Train/val/test with stratify on group label\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    ")\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33fcab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1920, 1811, 1)\n",
      "X_val shape: (480, 1811, 1)\n",
      "X_test shape: (600, 1811, 1)\n"
     ]
    }
   ],
   "source": [
    "# === Standardise features (across time*batch, per-channel) ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape 3D data to 2D for scaling\n",
    "original_shape_train = X_train.shape\n",
    "original_shape_val = X_val.shape\n",
    "original_shape_test = X_test.shape\n",
    "\n",
    "# Reshape to 2D: (batch * seq_len, features)\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "# Scale the data\n",
    "X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "X_val_2d = scaler.transform(X_val_2d)\n",
    "X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train = X_train_2d.reshape(original_shape_train)\n",
    "X_val = X_val_2d.reshape(original_shape_val)\n",
    "X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cce7d3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1811, 1]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Torch loaders\n",
    "batch_size = 64\n",
    "\n",
    "# === Convert to tensors and loaders ===\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# check the data loaders\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17d561e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.98 ===\n"
     ]
    }
   ],
   "source": [
    "from classifiers.svm_classifier import svm_classifier\n",
    "\n",
    "# Flatten the time series data for SVM (reshape from (n_samples, seq_len, features) to (n_samples, seq_len * features))\n",
    "X_train_svm = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_svm = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train_svm,\n",
    "    X_test_svm,\n",
    "    y_train,\n",
    "    y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade41d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b1fc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
