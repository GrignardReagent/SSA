{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0234a8f3",
   "metadata": {},
   "source": [
    "# IY011 Contrastive Learning: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b35245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, itertools\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "# from classifiers.transformer_classifier import transformer_classifier\n",
    "from models.simple_transformer import SimpleTransformer\n",
    "from models.transformer import TransformerClassifier, train_model, evaluate_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Build groups\n",
    "from utils.data_processing import build_groups\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179e6ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"/home/ianyang/stochastic_simulations/experiments/EXP-25-IY011/data\")\n",
    "RESULTS_PATH = DATA_ROOT / \"IY011_simulation_parameters_sobol.csv\" #  this csv file stores all the simulation parameters used\n",
    "df_params = pd.read_csv(RESULTS_PATH) \n",
    "# TRAJ_PATH = [DATA_ROOT / f\"mRNA_trajectories_mu{row['mu_target']:.3f}_cv{row['cv_target']:.3f}_tac{row['t_ac_target']:.3f}.csv\" for idx, row in df_params.iterrows()] # the trajectories \n",
    "TRAJ_PATH = [DATA_ROOT / df_params['trajectory_filename'].values[i] for i in range(len(df_params))]\n",
    "TRAJ_NPZ_PATH = [traj_file.with_suffix('.npz') for traj_file in TRAJ_PATH]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3505a0d",
   "metadata": {},
   "source": [
    "## extract steady state trajectories only, then save to .npz files for faster loading:\n",
    "\n",
    "```python\n",
    "# extract only steady state part of the trajs \n",
    "ss_index_list = []\n",
    "for traj_file in TRAJ_PATH:\n",
    "    parameter_subsets = parameter_sets[TRAJ_PATH.index(traj_file)]\n",
    "    _, ss_index = find_steady_state(parameter_subsets)\n",
    "    ss_index_list.append(ss_index)\n",
    "# find the maximum steady state time across all trajectories\n",
    "max_ss_index = max(ss_index for ss_index in ss_index_list)\n",
    "new_time_points = time_points[max_ss_index:]\n",
    "\n",
    "for traj_file, params in zip(TRAJ_PATH, parameter_sets):\n",
    "    df_traj = pd.read_csv(traj_file)\n",
    "    df_traj = df_traj.drop(columns=['label'], errors='ignore') # drop in-place if it exists, do nothing otherwise\n",
    "    # ensure same length across all trajs by truncating to the maximum steady state time\n",
    "    df_traj = df_traj.iloc[:, max_ss_index:]\n",
    "    trajectories = df_traj.values\n",
    "    trajectory_data = {\n",
    "            'trajectories': trajectories.astype(np.float32),\n",
    "            'time_points': new_time_points.astype(np.float32),\n",
    "            'size': int(size),\n",
    "            'parameters': params,\n",
    "    }\n",
    "    try:\n",
    "        np.savez_compressed(\n",
    "        traj_file.with_suffix('.npz'),\n",
    "        trajectories=trajectories.astype(np.float32),\n",
    "        time_points=new_time_points.astype(np.float32),\n",
    "        size=size,\n",
    "        parameters=params,\n",
    "        )\n",
    "    except PermissionError:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.npz') as tmp_file:\n",
    "            tmp_path = tmp_file.name\n",
    "            np.savez_compressed(tmp_path, **trajectory_data)\n",
    "        # Move temp file to final location with sudo\n",
    "        subprocess.run(['sudo', 'mv', tmp_path, traj_file.with_suffix('.npz')], check=True)\n",
    "        subprocess.run(['sudo', 'chown', f'{os.getenv(\"USER\")}:{os.getenv(\"USER\")}', traj_file.with_suffix('.npz')], check=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2c2545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2053.  2023.  1994. ...  1433.  1411.  1386.]\n",
      " [ 8422.  8272.  8149. ... 23341. 22966. 22593.]\n",
      " [ 3147.  3098.  3047. ...   114.   114.   113.]\n",
      " ...\n",
      " [ 4381.  4308. 11701. ... 27395. 26960. 26544.]\n",
      " [14966. 14723. 14475. ... 44179. 43516. 42803.]\n",
      " [ 2659.  2624.  2583. ...  1603.  1570.  1541.]]\n",
      "1811\n",
      "1000\n",
      "{'sigma_b': 0.0118563038378397, 'sigma_u': 0.9881436961621602, 'rho': 11958.995981515953, 'd': 0.0163008923214477, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# load back the data (the first simulation) from the npz file\n",
    "data = np.load(TRAJ_NPZ_PATH[1], allow_pickle=True)\n",
    "print(data['trajectories'])\n",
    "print(len(data['time_points']))\n",
    "print(data['size'])\n",
    "print(data['parameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9212ce",
   "metadata": {},
   "source": [
    "instead of binarily labelling them based on stats, we can do a **pairwise contrastive learning**, where we take random pairs of trajectories for classification (this will eliminate issues with class imbalance too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e24676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building positive groups: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 666.08it/s]\n",
      "Building negative groups: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 530.05it/s]\n"
     ]
    }
   ],
   "source": [
    "num_traj = 100\n",
    "NUM_GROUPS = 2  # a pair: 1 pos, 1 neg\n",
    "groups = build_groups(TRAJ_NPZ_PATH, num_groups=NUM_GROUPS, num_traj=num_traj) # list of tuples (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fac8d2",
   "metadata": {},
   "source": [
    "stack or flatten?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e10d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_samples shape: (200, 1811, 1), y_samples shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Stacked groups -> individual trajectory samples\n",
    "X_samples = []\n",
    "y_samples = []\n",
    "for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "    L, K = Xg.shape\n",
    "    for k in range(K):\n",
    "        X_samples.append(Xg[:, k:k+1])  # (seq_len, 1)\n",
    "        y_samples.append(yg)            # or some other per-trajectory label\n",
    "X_samples = np.stack(X_samples, 0)      # (N_samples, seq_len, 1)\n",
    "y_samples = np.array(y_samples)\n",
    "print(f'X_samples shape: {X_samples.shape}, y_samples shape: {y_samples.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b7ead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_flat shape: (200, 1811), y_flat shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Flatten groups -> individual 1D trajectory samples (seq_len,)\n",
    "X_flat = []\n",
    "y_flat = []\n",
    "for Xg, yg in groups:          # Xg shape (seq_len, K)\n",
    "    L, K = Xg.shape\n",
    "    for k in range(K):\n",
    "        traj = Xg[:, k].astype(np.float32)   # shape (seq_len,)\n",
    "        X_flat.append(traj)\n",
    "        y_flat.append(int(yg))\n",
    "        # X_flat.extend(traj)\n",
    "        # y_flat.extend([int(yg)] * L)\n",
    "X_flat = np.stack(X_flat, 0)      # (N_samples, seq_len)\n",
    "y_flat = np.array(y_flat, dtype=np.int64)\n",
    "print(f'X_flat shape: {X_flat.shape}, y_flat shape: {y_flat.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6a256",
   "metadata": {},
   "source": [
    "Train/Val/Test split with stratify on group label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c45b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation:\n",
      "  Train groups: 128, Val groups: 32, Test groups: 40\n"
     ]
    }
   ],
   "source": [
    "# # with the stacked samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_samples, y_samples, test_size=0.2, random_state=42, stratify=y_samples\n",
    ")\n",
    "X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# # with the flattened samples\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X_flat, y_flat, test_size=0.2, random_state=42, stratify=y_flat\n",
    "# )\n",
    "# X_train, X_val,  y_train, y_val  = train_test_split(\n",
    "#     X_train, y_train, test_size=0.2, random_sta\n",
    "# te=42, stratify=y_train\n",
    "# )\n",
    "\n",
    "print(\"Data preparation:\")\n",
    "print(f\"  Train groups: {len(y_train)}, Val groups: {len(y_val)}, Test groups: {len(y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951c4e1",
   "metadata": {},
   "source": [
    "Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1269ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (128, 1811, 1)\n",
      "X_val shape: (32, 1811, 1)\n",
      "X_test shape: (40, 1811, 1)\n"
     ]
    }
   ],
   "source": [
    "# === Standardise features (across time*batch, per-channel) ===\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape 3D data to 2D for scaling\n",
    "original_shape_train = X_train.shape\n",
    "original_shape_val = X_val.shape\n",
    "original_shape_test = X_test.shape\n",
    "\n",
    "# Reshape to 2D: (batch * seq_len, features)\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_2d = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "# Scale the data\n",
    "X_train_2d = scaler.fit_transform(X_train_2d)\n",
    "X_val_2d = scaler.transform(X_val_2d)\n",
    "X_test_2d = scaler.transform(X_test_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train = X_train_2d.reshape(original_shape_train)\n",
    "X_val = X_val_2d.reshape(original_shape_val)\n",
    "X_test = X_test_2d.reshape(original_shape_test)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da902563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1811, 1]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Torch loaders\n",
    "batch_size = NUM_GROUPS\n",
    "\n",
    "# === Convert to tensors and loaders ===\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# check the data loaders\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b32964",
   "metadata": {},
   "source": [
    "trying out this data structure with the training (loss_fn=CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8dcbcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/50] | train_loss 0.34 | train_acc 0.93 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (1/10).\n",
      "Epoch [2/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (2/10).\n",
      "Epoch [3/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (3/10).\n",
      "Epoch [4/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (4/10).\n",
      "Epoch [5/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (5/10).\n",
      "Epoch [6/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (6/10).\n",
      "Epoch [7/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (7/10).\n",
      "Epoch [8/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (8/10).\n",
      "Epoch [9/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (9/10).\n",
      "Epoch [10/50] | train_loss 0.00 | train_acc 1.00 | val_loss 0.00 | val_acc 1.00\n",
      "No improvement (10/10).\n",
      "ðŸ›‘ Early stopping.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# choose feature dim from training data (works for both flows)\n",
    "# X_train shape must be (N, seq_len, features)\n",
    "# assert X_train.ndim == 3, \"expect (N, seq_len, features)\"\n",
    "# feature_dim = X_train.shape[-1]   # 1 for per-trajectory samples, =num_traj for group-level\n",
    "\n",
    "input_size = 1\n",
    "num_classes = len(set(y_test))\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=64,\n",
    "    nhead=4,\n",
    "    num_layers=2, \n",
    "    num_classes=num_classes,\n",
    "    dropout=0.001, \n",
    "    use_conv1d=False \n",
    ")\n",
    "\n",
    "train_history = train_model(model, train_loader, val_loader, epochs=50, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91716820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test â€” loss: N/A | acc: 1.00\n",
      "Group-level Test Accuracy (num_traj=100): 1.0000\n"
     ]
    }
   ],
   "source": [
    "loss, acc = evaluate_model(model, test_loader)\n",
    "print(f\"Group-level Test Accuracy (num_traj={num_traj}): {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
