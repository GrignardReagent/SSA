{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2e390a",
   "metadata": {},
   "source": [
    "# Use SVM to classify the dataset generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17a54cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "# Import custom modules\n",
    "from classifiers.svm_classifier import svm_classifier\n",
    "from models.TF_transformer import TFTransformer, ModelCfg\n",
    "# from models.transfer_learning.data_processing import add_pair_labels, prepare_dataset\n",
    "# from models.transfer_learning.data_loading import load_synthetic_dataset\n",
    "from utils.data_processing import add_pair_labels\n",
    "\n",
    "# Import sklearn modules for SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5841d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_binary_labels(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a copy of ``df`` with a new ``label`` column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        DataFrame containing the simulation results.\n",
    "    column:\n",
    "        Column on which to base the 50/50 split.  Values greater than the\n",
    "        halfway point of the sorted column are labelled ``1``; the rest are\n",
    "        labelled ``0``.\n",
    "    \"\"\"\n",
    "    labelled = df.copy()\n",
    "    \n",
    "    # Sort the values in the specified column to find the median split point\n",
    "    order = np.argsort(labelled[column].values)\n",
    "    \n",
    "    # Calculate the midpoint index for a 50/50 binary classification split\n",
    "    midpoint = len(labelled) // 2\n",
    "    \n",
    "    # Initialize all labels as 0 (lower half of sorted values)\n",
    "    labels = np.zeros(len(labelled), dtype=int)\n",
    "    \n",
    "    # Assign label 1 to the upper half (values above median)\n",
    "    labels[order[midpoint:]] = 1\n",
    "    labelled[\"label\"] = labels\n",
    "    return labelled\n",
    "\n",
    "\n",
    "def grid_search_svm(\n",
    "    df: pd.DataFrame,\n",
    "    param_grid: Optional[Dict[str, list[float]]] = None,\n",
    ") -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Grid search SVM hyperparameters for RBF and linear kernels.\"\"\"\n",
    "   \n",
    "    # Extract labels\n",
    "    y = df[\"label\"].values\n",
    "\n",
    "    # Extract features (all columns except 'label')\n",
    "    X = df.drop(columns=[\"label\"]).values\n",
    "    \n",
    "    # Split data into training and testing sets with stratification to maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Define default hyperparameter grid if none provided\n",
    "    # C: regularization parameter (controls overfitting)\n",
    "    # gamma: kernel coefficient for RBF kernel (controls decision boundary complexity)\n",
    "    param_grid = param_grid or {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [0.01, 0.1, 1],\n",
    "    }\n",
    "    \n",
    "    results: Dict[str, Dict[str, float]] = {}\n",
    "    \n",
    "    # Test both RBF (radial basis function) and linear kernels\n",
    "    for kernel in (\"rbf\", \"linear\"):\n",
    "        # Perform 5-fold cross-validation grid search to find optimal hyperparameters\n",
    "        search = GridSearchCV(SVC(kernel=kernel), param_grid, cv=5)\n",
    "        \n",
    "        # Fit the grid search on training data\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on test set using the best found parameters\n",
    "        predictions = search.best_estimator_.predict(X_test)\n",
    "        \n",
    "        # Store results including accuracy and best hyperparameters\n",
    "        results[kernel] = {\n",
    "            \"accuracy\": accuracy_score(y_test, predictions),\n",
    "            \"C\": float(search.best_params_[\"C\"]),\n",
    "            \"gamma\": float(search.best_params_[\"gamma\"]),\n",
    "        }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950e75d",
   "metadata": {},
   "source": [
    "Load the data, deal with variable-length time series, add labels and then concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66bb4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Set up directory paths for data loading ######\n",
    "BASE_DIR = Path.cwd().parent  # Gets one directory up from current working directory\n",
    "OUT_DIR = BASE_DIR\n",
    "SYNTHETIC_DIR = BASE_DIR / \"data_6\" \n",
    "RESULTS_CSV = \"IY010_simulation_parameters_6.csv\"\n",
    "results_csv_path = BASE_DIR / RESULTS_CSV # results csv file, helps to label\n",
    "results = pd.read_csv(results_csv_path)\n",
    "# Keep only successful simulations with complete statistics\n",
    "results = results[results[\"success\"]].dropna(\n",
    "    subset=[\"mu_observed\", \"cv_observed\", \"t_ac_observed\"]\n",
    ")\n",
    "# take the first 10 rows for a smaller dataset\n",
    "# results = results.head(10)\n",
    "\n",
    "# Create binary labels based on the specified target column (e.g., mu_target), this converts continuous target values into a binary classification problem\n",
    "label_column = \"mu_target\"\n",
    "labelled_results = add_binary_labels(results, label_column)\n",
    "##### Set up directory paths for data loading ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1995f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Trajectory statistics:\n",
      "   Minimum length: 50 rows\n",
      "   Maximum length: 50 rows\n",
      "   Column count range: 144 - 1999\n",
      "   Minimum columns: 144\n",
      "\n",
      "üîß Creating standardized dataset...\n",
      "   NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "##### Deal with variable-length time series, add labels, concatenate data ######\n",
    "min_length = float('inf')\n",
    "max_length = 0\n",
    "column_structures = {}\n",
    "\n",
    "# find minimum length across all trajs\n",
    "for i in range(len(results)):\n",
    "    # Path to the data CSV file \n",
    "    trajectory_filename = results[\"trajectory_filename\"].values[i]\n",
    "    DATA_CSV = SYNTHETIC_DIR / trajectory_filename\n",
    "    data = pd.read_csv(DATA_CSV)\n",
    "    \n",
    "    # Track length and column structure\n",
    "    length = len(data)\n",
    "    cols = list(data.columns)\n",
    "    \n",
    "    min_length = min(min_length, length)\n",
    "    max_length = max(max_length, length)\n",
    "    column_structures[trajectory_filename] = {\n",
    "        'length': length,\n",
    "        'columns': cols,\n",
    "        'num_cols': len(cols)\n",
    "    }\n",
    "\n",
    "print(f\"üìè Trajectory statistics:\")\n",
    "print(f\"   Minimum length: {min_length} rows\")\n",
    "print(f\"   Maximum length: {max_length} rows\")\n",
    "print(f\"   Column count range: {min([s['num_cols'] for s in column_structures.values()])} - {max([s['num_cols'] for s in column_structures.values()])}\")\n",
    "\n",
    "# Find the minimum number of columns across all files\n",
    "min_columns = min([s['num_cols'] for s in column_structures.values()])\n",
    "print(f\"   Minimum columns: {min_columns}\")\n",
    "\n",
    "# Create standardized dataset with the same length, with consistent column labels (t_0, t_1, t_2, etc.), then assign a label column\n",
    "print(f\"\\nüîß Creating standardized dataset...\")\n",
    "labelled_data_list = []\n",
    "for i in range(len(results)):\n",
    "    # Path to the data CSV file \n",
    "    trajectory_filename = results[\"trajectory_filename\"].values[i]\n",
    "    DATA_CSV = SYNTHETIC_DIR / trajectory_filename\n",
    "    data = pd.read_csv(DATA_CSV)\n",
    "    \n",
    "    # Truncate columns to minimum (take last N columns - to make sure steady state)\n",
    "    data_standardised = data.iloc[:, -min_columns:].copy()\n",
    "    \n",
    "    # Rename columns to be consistent (t_0, t_1, t_2, etc.) - this is crucial so that the resulting df doesnt contain missing data.\n",
    "    new_columns = [f\"t_{j}\" for j in range(min_columns)]\n",
    "    data_standardised.columns = new_columns\n",
    "    \n",
    "    # Find the label for this trajectory filename from labelled_results, and add to the standardised data\n",
    "    label_value = labelled_results[labelled_results['trajectory_filename'] == trajectory_filename]['label'].iloc[0]\n",
    "    data_standardised['label'] = label_value\n",
    "    labelled_data_list.append(data_standardised)\n",
    "    \n",
    "# Concatenate all standardized data\n",
    "labelled_data = pd.concat(labelled_data_list, ignore_index=True)\n",
    "# Verify no NaN values\n",
    "nan_count = labelled_data.isnull().sum().sum()\n",
    "print(f\"   NaN values: {nan_count}\")\n",
    "\n",
    "##### Deal with variable-length time series, add labels, concatenate data ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfc14e",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7732c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation for SVM:\n",
      "  Feature matrix shape: (49000, 144)\n",
      "  Labels shape: (49000,)\n",
      "  Number of classes: 2\n",
      "  Class distribution: [24500 24500]\n",
      "  Memory usage: 53.83 MB\n",
      "‚úÖ Data ready for SVM classification!\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Prepare Features and Labels for SVM\n",
    "# =========================================================\n",
    "df = labelled_data.copy()\n",
    "# Extract labels\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Extract features (all columns except 'label')\n",
    "X = df.drop(columns=[\"label\"]).values\n",
    "\n",
    "print(f\"Data preparation for SVM:\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Labels shape: {y.shape}\")\n",
    "print(f\"  Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"  Class distribution: {np.bincount(y)}\")\n",
    "print(f\"  Memory usage: {X.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check for any NaN or infinite values\n",
    "if np.any(np.isnan(X)):\n",
    "    print(\"‚ö†Ô∏è  Warning: NaN values detected in features\")\n",
    "if np.any(np.isinf(X)):\n",
    "    print(\"‚ö†Ô∏è  Warning: Infinite values detected in features\")\n",
    "    \n",
    "print(\"‚úÖ Data ready for SVM classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aeb4a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.75 ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.76      4900\n",
      "           1       0.79      0.68      0.73      4900\n",
      "\n",
      "    accuracy                           0.75      9800\n",
      "   macro avg       0.75      0.75      0.75      9800\n",
      "weighted avg       0.75      0.75      0.75      9800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4003  897]\n",
      " [1565 3335]]\n",
      "‚è±Ô∏è  SVM (rbf) training and evaluation time: 167.68 seconds\n"
     ]
    }
   ],
   "source": [
    "# SVM Parameters (using defaults from svm_classifier function)\n",
    "SVM_C = 1.0           # Regularization parameter\n",
    "SVM_GAMMA = 'scale'   # Kernel coefficient \n",
    "SVM_KERNEL = 'rbf'    # Kernel type\n",
    "\n",
    "# Train/test split ratio\n",
    "TEST_SPLIT = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SPLIT, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Ensure balanced split across classes\n",
    ")\n",
    "\n",
    "# Record training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train SVM using the imported svm_classifier function\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    svm_C=SVM_C,\n",
    "    svm_gamma=SVM_GAMMA, \n",
    "    svm_kernel=SVM_KERNEL,\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  SVM ({SVM_KERNEL}) training and evaluation time: {training_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a219c6f",
   "metadata": {},
   "source": [
    "Does it make a difference if we apply scaling before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c951c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.75 ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.76      4900\n",
      "           1       0.79      0.68      0.73      4900\n",
      "\n",
      "    accuracy                           0.75      9800\n",
      "   macro avg       0.75      0.75      0.75      9800\n",
      "weighted avg       0.75      0.75      0.75      9800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4003  897]\n",
      " [1565 3335]]\n",
      "‚è±Ô∏è  SVM (rbf) training and evaluation time: 166.12 seconds\n"
     ]
    }
   ],
   "source": [
    "# Scale features before SVM training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Record training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train SVM using the imported svm_classifier function\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    svm_C=SVM_C,\n",
    "    svm_gamma=SVM_GAMMA, \n",
    "    svm_kernel=SVM_KERNEL,\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  SVM ({SVM_KERNEL}) training and evaluation time: {training_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f8d61",
   "metadata": {},
   "source": [
    "\n",
    "# =========================================================\n",
    "# Experiment: Does Temporal Order Matter for Classification?\n",
    "What if we shuffle the time series before testing? Does it affect the SVM performance?\n",
    "# =========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d333c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.74 ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76      4900\n",
      "           1       0.79      0.66      0.72      4900\n",
      "\n",
      "    accuracy                           0.74      9800\n",
      "   macro avg       0.75      0.74      0.74      9800\n",
      "weighted avg       0.75      0.74      0.74      9800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4028  872]\n",
      " [1666 3234]]\n",
      "‚è±Ô∏è  SVM (rbf) training and evaluation time: 234.29 seconds\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Experiment: Does Temporal Order Matter for Classification?\n",
    "# =========================================================\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create a copy of the dataframe for shuffling\n",
    "df_shuffled = df.copy()\n",
    "\n",
    "# Shuffle each row (time series) individually, keeping the label column intact\n",
    "for i in range(df_shuffled.shape[0]):\n",
    "    # Get the feature columns (exclude 'label' column)\n",
    "    feature_cols = [col for col in df_shuffled.columns if col != 'label']\n",
    "    \n",
    "    # Extract the row's feature values as numpy array\n",
    "    row_features = df_shuffled.loc[i, feature_cols].values\n",
    "    \n",
    "    # Shuffle the features in-place\n",
    "    np.random.shuffle(row_features)\n",
    "    \n",
    "    # Assign the shuffled features back to the dataframe\n",
    "    df_shuffled.loc[i, feature_cols] = row_features\n",
    "\n",
    "y_shuffled = df_shuffled[\"label\"].values\n",
    "X_shuffled = df_shuffled.drop(columns=[\"label\"]).values\n",
    "# split the data\n",
    "X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled = train_test_split(\n",
    "    X_shuffled, \n",
    "    y_shuffled, \n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_shuffled  # Ensure balanced split across classes\n",
    ")\n",
    "# Record training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train SVM using X_train_shuffled and y_train_shuffled but test with shuffled data\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled,\n",
    "    svm_C=SVM_C,\n",
    "    svm_gamma=SVM_GAMMA, \n",
    "    svm_kernel=SVM_KERNEL,\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  SVM ({SVM_KERNEL}) training and evaluation time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78356035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
