{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2e390a",
   "metadata": {},
   "source": [
    "# Use SVM to Classify Time Series, Shuffling the Timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a54cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Add src directory to Python path\n",
    "sys.path.append(str(Path.cwd().parent.parent.parent / \"src\"))\n",
    "\n",
    "# Import custom modules\n",
    "from classifiers.svm_classifier import svm_classifier, grid_search_svm\n",
    "from utils.data_processing import add_binary_labels, add_nearest_neighbour_labels\n",
    "from models.TF_transformer import TFTransformer, ModelCfg\n",
    "# Import sklearn modules for SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4950e75d",
   "metadata": {},
   "source": [
    "Load the data, deal with variable-length time series, add labels and then concatenate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bb4f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Set up directory paths for data loading ######\n",
    "BASE_DIR = Path.cwd().parent  # Gets one directory up from current working directory\n",
    "OUT_DIR = BASE_DIR\n",
    "SYNTHETIC_DIR = BASE_DIR / \"data_6\" \n",
    "RESULTS_CSV = \"IY010_simulation_parameters_6.csv\"\n",
    "results_csv_path = BASE_DIR / RESULTS_CSV # results csv file, helps to label\n",
    "results = pd.read_csv(results_csv_path)\n",
    "# Keep only successful simulations with complete statistics\n",
    "results = results[results[\"success\"]].dropna(\n",
    "    subset=[\"mu_observed\", \"cv_observed\", \"t_ac_observed\"]\n",
    ")\n",
    "# take the first 10 rows for a smaller dataset\n",
    "# results = results.head(10)\n",
    "\n",
    "# Create binary labels based on the specified target column (e.g., mu_target), this converts continuous target values into a binary classification problem\n",
    "label_column = \"mu_target\"\n",
    "labelled_results = add_binary_labels(results, label_column)\n",
    "##### Set up directory paths for data loading ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1995f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Trajectory statistics:\n",
      "   Minimum length: 50 rows\n",
      "   Maximum length: 50 rows\n",
      "   Column count range: 144 - 1999\n",
      "   Minimum columns: 144\n",
      "\n",
      "üîß Creating standardized dataset...\n",
      "   NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "##### Deal with variable-length time series, add labels, concatenate data ######\n",
    "min_length = float('inf')\n",
    "max_length = 0\n",
    "column_structures = {}\n",
    "\n",
    "# find minimum length across all trajs\n",
    "for i in range(len(results)):\n",
    "    # Path to the data CSV file \n",
    "    trajectory_filename = results[\"trajectory_filename\"].values[i]\n",
    "    DATA_CSV = SYNTHETIC_DIR / trajectory_filename\n",
    "    data = pd.read_csv(DATA_CSV)\n",
    "    \n",
    "    # Track length and column structure\n",
    "    length = len(data)\n",
    "    cols = list(data.columns)\n",
    "    \n",
    "    min_length = min(min_length, length)\n",
    "    max_length = max(max_length, length)\n",
    "    column_structures[trajectory_filename] = {\n",
    "        'length': length,\n",
    "        'columns': cols,\n",
    "        'num_cols': len(cols)\n",
    "    }\n",
    "\n",
    "print(f\"üìè Trajectory statistics:\")\n",
    "print(f\"   Minimum length: {min_length} rows\")\n",
    "print(f\"   Maximum length: {max_length} rows\")\n",
    "print(f\"   Column count range: {min([s['num_cols'] for s in column_structures.values()])} - {max([s['num_cols'] for s in column_structures.values()])}\")\n",
    "\n",
    "# Find the minimum number of columns across all files\n",
    "min_columns = min([s['num_cols'] for s in column_structures.values()])\n",
    "print(f\"   Minimum columns: {min_columns}\")\n",
    "\n",
    "# Create standardized dataset with the same length, with consistent column labels (t_0, t_1, t_2, etc.), then assign a label column\n",
    "print(f\"\\nüîß Creating standardized dataset...\")\n",
    "labelled_data_list = []\n",
    "for i in range(len(results)):\n",
    "    # Path to the data CSV file \n",
    "    trajectory_filename = results[\"trajectory_filename\"].values[i]\n",
    "    DATA_CSV = SYNTHETIC_DIR / trajectory_filename\n",
    "    data = pd.read_csv(DATA_CSV)\n",
    "    \n",
    "    # Truncate columns to minimum (take last N columns - to make sure steady state)\n",
    "    data_standardised = data.iloc[:, -min_columns:].copy()\n",
    "    \n",
    "    # Rename columns to be consistent (t_0, t_1, t_2, etc.) - this is crucial so that the resulting df doesnt contain missing data.\n",
    "    new_columns = [f\"t_{j}\" for j in range(min_columns)]\n",
    "    data_standardised.columns = new_columns\n",
    "    \n",
    "    # Find the label for this trajectory filename from labelled_results, and add to the standardised data\n",
    "    label_value = labelled_results[labelled_results['trajectory_filename'] == trajectory_filename]['label'].iloc[0]\n",
    "    data_standardised['label'] = label_value\n",
    "    labelled_data_list.append(data_standardised)\n",
    "    \n",
    "# Concatenate all standardized data\n",
    "labelled_data = pd.concat(labelled_data_list, ignore_index=True)\n",
    "# Verify no NaN values\n",
    "nan_count = labelled_data.isnull().sum().sum()\n",
    "print(f\"   NaN values: {nan_count}\")\n",
    "\n",
    "##### Deal with variable-length time series, add labels, concatenate data ######"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfc14e",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7732c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation for SVM:\n",
      "  Feature matrix shape: (49000, 144)\n",
      "  Labels shape: (49000,)\n",
      "  Number of classes: 2\n",
      "  Class distribution: [24500 24500]\n",
      "  Memory usage: 53.83 MB\n",
      "‚úÖ Data ready for SVM classification!\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Prepare Features and Labels for SVM\n",
    "# =========================================================\n",
    "df = labelled_data.copy()\n",
    "# Extract labels\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Extract features (all columns except 'label')\n",
    "X = df.drop(columns=[\"label\"]).values\n",
    "\n",
    "print(f\"Data preparation for SVM:\")\n",
    "print(f\"  Feature matrix shape: {X.shape}\")\n",
    "print(f\"  Labels shape: {y.shape}\")\n",
    "print(f\"  Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"  Class distribution: {np.bincount(y)}\")\n",
    "print(f\"  Memory usage: {X.nbytes / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check for any NaN or infinite values\n",
    "if np.any(np.isnan(X)):\n",
    "    print(\"‚ö†Ô∏è  Warning: NaN values detected in features\")\n",
    "if np.any(np.isinf(X)):\n",
    "    print(\"‚ö†Ô∏è  Warning: Infinite values detected in features\")\n",
    "    \n",
    "print(\"‚úÖ Data ready for SVM classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb4a76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.75 ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.76      4900\n",
      "           1       0.79      0.68      0.73      4900\n",
      "\n",
      "    accuracy                           0.75      9800\n",
      "   macro avg       0.75      0.75      0.75      9800\n",
      "weighted avg       0.75      0.75      0.75      9800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4003  897]\n",
      " [1565 3335]]\n",
      "‚è±Ô∏è  SVM (rbf) training and evaluation time: 180.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# SVM Parameters (using defaults from svm_classifier function)\n",
    "SVM_C = 1.0           # Regularization parameter\n",
    "SVM_GAMMA = 'scale'   # Kernel coefficient \n",
    "SVM_KERNEL = 'rbf'    # Kernel type\n",
    "\n",
    "# Train/test split ratio\n",
    "TEST_SPLIT = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SPLIT, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Ensure balanced split across classes\n",
    ")\n",
    "\n",
    "# Record training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train SVM using the imported svm_classifier function\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    svm_C=SVM_C,\n",
    "    svm_gamma=SVM_GAMMA, \n",
    "    svm_kernel=SVM_KERNEL,\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  SVM ({SVM_KERNEL}) training and evaluation time: {training_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a219c6f",
   "metadata": {},
   "source": [
    "Does it make a difference if we apply scaling before training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c951c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.75 ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.76      4900\n",
      "           1       0.79      0.68      0.73      4900\n",
      "\n",
      "    accuracy                           0.75      9800\n",
      "   macro avg       0.75      0.75      0.75      9800\n",
      "weighted avg       0.75      0.75      0.75      9800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4003  897]\n",
      " [1565 3335]]\n",
      "‚è±Ô∏è  SVM (rbf) training and evaluation time: 182.01 seconds\n"
     ]
    }
   ],
   "source": [
    "# Scale features before SVM training\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Record training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train SVM using the imported svm_classifier function\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    svm_C=SVM_C,\n",
    "    svm_gamma=SVM_GAMMA, \n",
    "    svm_kernel=SVM_KERNEL,\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  SVM ({SVM_KERNEL}) training and evaluation time: {training_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f8d61",
   "metadata": {},
   "source": [
    "\n",
    "# =========================================================\n",
    "# Experiment: Does Temporal Order Matter for Classification?\n",
    "What if we shuffle the time series before testing? Does it affect the SVM performance?\n",
    "# =========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d333c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SVM (RBF Kernel) Classification Accuracy: 0.74 ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.82      0.76      4900\n",
      "           1       0.79      0.66      0.72      4900\n",
      "\n",
      "    accuracy                           0.74      9800\n",
      "   macro avg       0.75      0.74      0.74      9800\n",
      "weighted avg       0.75      0.74      0.74      9800\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4028  872]\n",
      " [1666 3234]]\n",
      "‚è±Ô∏è  SVM (rbf) training and evaluation time: 248.76 seconds\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Experiment: Does Temporal Order Matter for Classification?\n",
    "# =========================================================\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create a copy of the dataframe for shuffling\n",
    "df_shuffled = df.copy()\n",
    "\n",
    "# Shuffle each row (time series) individually, keeping the label column intact\n",
    "for i in range(df_shuffled.shape[0]):\n",
    "    # Get the feature columns (exclude 'label' column)\n",
    "    feature_cols = [col for col in df_shuffled.columns if col != 'label']\n",
    "    \n",
    "    # Extract the row's feature values as numpy array\n",
    "    row_features = df_shuffled.loc[i, feature_cols].values\n",
    "    \n",
    "    # Shuffle the features in-place\n",
    "    np.random.shuffle(row_features)\n",
    "    \n",
    "    # Assign the shuffled features back to the dataframe\n",
    "    df_shuffled.loc[i, feature_cols] = row_features\n",
    "\n",
    "y_shuffled = df_shuffled[\"label\"].values\n",
    "X_shuffled = df_shuffled.drop(columns=[\"label\"]).values\n",
    "# split the data\n",
    "X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled = train_test_split(\n",
    "    X_shuffled, \n",
    "    y_shuffled, \n",
    "    test_size=TEST_SPLIT,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_shuffled  # Ensure balanced split across classes\n",
    ")\n",
    "# Record training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train SVM using X_train_shuffled and y_train_shuffled but test with shuffled data\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train_shuffled, X_test_shuffled, y_train_shuffled, y_test_shuffled,\n",
    "    svm_C=SVM_C,\n",
    "    svm_gamma=SVM_GAMMA, \n",
    "    svm_kernel=SVM_KERNEL,\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  SVM ({SVM_KERNEL}) training and evaluation time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da0963",
   "metadata": {},
   "source": [
    "Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78356035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search, then use the best C and gamma\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"gamma\": [0.01, 0.1, 1,'scale'],\n",
    "}\n",
    "grid_search_results = grid_search_svm(df, param_grid=param_grid)\n",
    "#print out the best parameters\n",
    "print(\"Best parameters from grid search:\")\n",
    "print(grid_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best parameters from grid search\n",
    "svm_accuracy = svm_classifier(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    svm_C=grid_search_results[\"rbf\"][\"C\"],\n",
    "    svm_gamma=grid_search_results[\"rbf\"][\"gamma\"],\n",
    "    svm_kernel=\"rbf\",\n",
    "    print_classification_report=True,\n",
    "    print_confusion_matrix=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
