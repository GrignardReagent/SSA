{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c49668",
   "metadata": {},
   "source": [
    "# Train transformer model on transformed experimental time series data\n",
    "\n",
    "Using the preprocessed time series data from the `transformed_exp_time_series_data/` folder:\n",
    "- 19316_2020_10_26_steadystate_glucose_144m_2w2_00 (label: 0)\n",
    "- 20213_2021_09_07_steady_0p01glc_1344_1346_1347_00 (label: 1)\n",
    "\n",
    "Each file contains individual cell time series data ready for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "297bd156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 CSV files\n",
      "Number of sequences: 6\n",
      "Labels distribution: [3 3]\n",
      "Sample sequence shape: (241, 451)\n",
      "Number of features per timepoint: 451\n",
      "Number of sequences: 6\n",
      "Labels distribution: [3 3]\n",
      "Sample sequence shape: (241, 451)\n",
      "Number of features per timepoint: 451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load time series data from transformed_exp_time_series_data folder\n",
    "data_path = \"transformed_exp_time_series_data/\"\n",
    "csv_files = glob.glob(data_path + \"*.csv\")\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "# Load and combine all time series data\n",
    "all_sequences = []\n",
    "all_labels = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Extract experiment type from filename for labeling\n",
    "    if \"19316_2020_10_26\" in file:\n",
    "        label = 0  # glucose condition\n",
    "    elif \"20213_2021_09_07\" in file:\n",
    "        label = 1  # low glucose condition\n",
    "    else:\n",
    "        continue  # skip other experiments for now\n",
    "    \n",
    "    # Convert to numpy array (excluding any non-numeric columns if present)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    sequence = df[numeric_cols].values\n",
    "    \n",
    "    all_sequences.append(sequence)\n",
    "    all_labels.append(label)\n",
    "\n",
    "X = np.array(all_sequences, dtype=object)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "print(f\"Number of sequences: {len(X)}\")\n",
    "print(f\"Labels distribution: {np.bincount(y)}\")\n",
    "print(f\"Sample sequence shape: {X[0].shape}\")\n",
    "print(f\"Number of features per timepoint: {X[0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78009d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths - Min: 71, Max: 275, Mean: 163.33\n"
     ]
    }
   ],
   "source": [
    "# Prepare time series sequences for transformer training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the data as-is since it's already properly formatted\n",
    "sequence_lengths = [seq.shape[0] for seq in X]\n",
    "print(f\"Sequence lengths - Min: {min(sequence_lengths)}, Max: {max(sequence_lengths)}, Mean: {np.mean(sequence_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d479ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 450) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(padded_sequences)\n\u001b[1;32m     13\u001b[0m target_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m450\u001b[39m\n\u001b[0;32m---> 14\u001b[0m X_padded \u001b[38;5;241m=\u001b[39m \u001b[43mpad_or_truncate_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Split the data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(X_padded, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m, in \u001b[0;36mpad_or_truncate_sequences\u001b[0;34m(sequences, target_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m         padded_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([seq, padding])\n\u001b[1;32m     10\u001b[0m     padded_sequences\u001b[38;5;241m.\u001b[39mappend(padded_seq)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 450) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Prepare fixed-length sequences for transformer\n",
    "def pad_or_truncate_sequences(sequences, target_length=450):\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > target_length:\n",
    "            padded_seq = seq[:target_length]\n",
    "        else:\n",
    "            padding = np.zeros((target_length - len(seq), seq.shape[1]))\n",
    "            padded_seq = np.vstack([seq, padding])\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return np.array(padded_sequences)\n",
    "\n",
    "target_length = 450\n",
    "X_padded = pad_or_truncate_sequences(X, target_length)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_padded, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Training: {X_train.shape}, Validation: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baece673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sequence length: 450\n",
      "Padded sequences shape: (941, 450, 57)\n",
      "Training set: (564, 450, 57), labels: [140 424]\n",
      "Validation set: (188, 450, 57), labels: [ 46 142]\n",
      "Test set: (189, 450, 57), labels: [ 47 142]\n",
      "\n",
      "Feature ranges (first few features):\n",
      "  Feature 0: min=0.00, max=164.00, mean=126.20\n",
      "  Feature 1: min=0.00, max=1493.00, mean=718.41\n",
      "  Feature 2: min=0.00, max=291.72, mean=88.37\n",
      "  Feature 3: min=0.00, max=1144.25, mean=624.63\n",
      "  Feature 4: min=0.00, max=1164.44, mean=600.34\n"
     ]
    }
   ],
   "source": [
    "# Train transformer classifier\n",
    "from classifiers.transformer_classifier import transformer_classifier\n",
    "\n",
    "hyperparams = {\n",
    "    'input_size': X_train.shape[2],\n",
    "    'd_model': 128,\n",
    "    'nhead': 4,\n",
    "    'num_layers': 2,\n",
    "    'output_size': 2,\n",
    "    'dropout_rate': 0.1,\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 64,\n",
    "    'epochs': 50,\n",
    "    'patience': 5,\n",
    "    'optimizer': 'Adam',\n",
    "    'use_conv1d': False,\n",
    "    'use_auxiliary': False,\n",
    "    'pooling_strategy': 'last',\n",
    "    'use_mask': False,\n",
    "    'gradient_clip': 1.0\n",
    "}\n",
    "\n",
    "test_accuracy = transformer_classifier(X_train, X_val, X_test, y_train, y_val, y_test, **hyperparams)\n",
    "print(f\"Transformer Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc288661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "  input_size: 57\n",
      "  d_model: 128\n",
      "  nhead: 4\n",
      "  num_layers: 2\n",
      "  output_size: 2\n",
      "  dropout_rate: 0.1\n",
      "  learning_rate: 0.01\n",
      "  batch_size: 64\n",
      "  epochs: 50\n",
      "  patience: 5\n",
      "  optimizer: Adam\n",
      "  use_conv1d: False\n",
      "  use_auxiliary: False\n",
      "  pooling_strategy: last\n",
      "  use_mask: False\n",
      "  gradient_clip: 1.0\n",
      "  save_path: /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt\n",
      "\n",
      "Starting transformer training...\n",
      "✅ Model saved at /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt (Best Val Acc: 0.4628)\n",
      "✅ Model saved at /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt (Best Val Acc: 0.8617)\n",
      "✅ Model saved at /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt (Best Val Acc: 0.9787)\n",
      "✅ Model saved at /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt (Best Val Acc: 0.9840)\n",
      "✅ Model saved at /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt (Best Val Acc: 0.9894)\n",
      "✅ Model saved at /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt (Best Val Acc: 1.0000)\n",
      "=== Vanilla Transformer Accuracy: 1.00 ===\n",
      "\n",
      "Final Test Accuracy: 1.0000\n",
      "Model saved to: /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010A_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "# Benchmark all classifiers\n",
    "from classifiers.svm_classifier import svm_classifier\n",
    "from classifiers.random_forest_classifier import random_forest_classifier  \n",
    "from classifiers.logistic_regression_classifier import logistic_regression_classifier\n",
    "from classifiers.mlp_classifier import mlp_classifier\n",
    "from classifiers.random_classifier import random_classifier\n",
    "\n",
    "# Extract statistical features for traditional ML models\n",
    "def extract_statistical_features(X):\n",
    "    features = []\n",
    "    for seq in X:\n",
    "        seq_features = []\n",
    "        for feature_idx in range(seq.shape[1]):\n",
    "            feature_series = seq[:, feature_idx]\n",
    "            non_zero_mask = feature_series != 0\n",
    "            if np.any(non_zero_mask):\n",
    "                clean_series = feature_series[non_zero_mask]\n",
    "                seq_features.extend([np.mean(clean_series), np.std(clean_series), \n",
    "                                   np.min(clean_series), np.max(clean_series), np.median(clean_series)])\n",
    "            else:\n",
    "                seq_features.extend([0, 0, 0, 0, 0])\n",
    "        features.append(seq_features)\n",
    "    return np.array(features)\n",
    "\n",
    "X_train_features = extract_statistical_features(X_train)\n",
    "X_test_features = extract_statistical_features(X_test)\n",
    "X_val_features = extract_statistical_features(X_val)\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = {'Transformer': test_accuracy}\n",
    "results['SVM (RBF)'] = svm_classifier(X_train_features, X_test_features, y_train, y_test, svm_kernel='rbf')\n",
    "results['SVM (Linear)'] = svm_classifier(X_train_features, X_test_features, y_train, y_test, svm_kernel='linear')\n",
    "results['Random Forest'] = random_forest_classifier(X_train_features, X_test_features, y_train, y_test)\n",
    "results['Logistic Regression'] = logistic_regression_classifier(X_train_features, X_test_features, y_train, y_test)\n",
    "results['MLP'] = mlp_classifier(X_train_features, X_val_features, X_test_features, y_train, y_val, y_test)\n",
    "results['Random'] = random_classifier(y_test)\n",
    "\n",
    "# Display results\n",
    "for model, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model}: {acc:.4f}\")\n",
    "\n",
    "# Simple visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = list(results.keys())\n",
    "accuracies = list(results.values())\n",
    "plt.bar(models, accuracies)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
