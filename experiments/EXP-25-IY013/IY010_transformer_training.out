/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True
  warnings.warn(
âœ… CUDA detected, using GPU: NVIDIA GeForce RTX 2080 SUPER
ğŸ”§ Initializing TFTransformer with config: ModelCfg(n_classes=3, d_model=64, n_heads=4, n_layers=2, d_ff=128, dropout=0.1, max_len=8192, verbose=True)
ğŸ–¥ï¸  Using device: cuda
ğŸš€ CUDA available with 1 GPU(s)
   GPU 0: NVIDIA GeForce RTX 2080 SUPER
â„¹ï¸  Single GPU detected, using single GPU training
ğŸ“Š Model initialized with 67,331 total parameters (67,331 trainable)
=== Pre-training on synthetic data ===
ğŸ”„ Starting forward pass (batch size: 32, seq length: 2000)
ğŸ­ Applied attention mask with 4729 masked positions
ğŸ“¤ Forward pass complete, output shape: torch.Size([32, 3])
ğŸ­ Applied attention mask with 1613 masked positions
ğŸ“¤ Forward pass complete, output shape: torch.Size([32, 3])
ğŸ­ Applied attention mask with 4182 masked positions
ğŸ“¤ Forward pass complete, output shape: torch.Size([32, 3])
epoch 01 loss=nan acc=0.500
epoch 02 loss=nan acc=0.500
epoch 03 loss=nan acc=0.500
epoch 04 loss=nan acc=0.500
epoch 05 loss=nan acc=0.500
ğŸ’¾ Saving model to /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010_pretrained.pt
âœ… Model saved successfully
=== Fine-tuning on experimental data ===
ğŸ”’ Freezing encoder components for transfer learning
ğŸ“Š 27/29 parameters frozen
ğŸ”„ Resetting classifier head weights
âœ… Classifier head reset complete
epoch 01 loss=nan acc=0.305
epoch 02 loss=nan acc=0.305
epoch 03 loss=nan acc=0.305
epoch 04 loss=nan acc=0.305
epoch 05 loss=nan acc=0.305
ğŸ’¾ Saving model to /home/ianyang/stochastic_simulations/experiments/EXP-25-IY010/IY010_finetuned.pt
âœ… Model saved successfully
