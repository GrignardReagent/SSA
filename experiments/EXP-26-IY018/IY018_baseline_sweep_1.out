wandb: Agent Starting Run: ukscnjes with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.019866793676733996
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004147218210183237
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Currently logged in as: grignardreagent (grignard-reagent) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run ukscnjes
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_090605-ukscnjes
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ukscnjes
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    val_acc ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 16
wandb:         lr 0.00104
wandb:  train_acc 0.518
wandb: train_loss 0.69265
wandb:    val_acc 0.515
wandb:   val_loss 0.69276
wandb: 
wandb: üöÄ View run [Baseline] L4-H2-D32-drop0.019866793676733996-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ukscnjes
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_090605-ukscnjes/logs
wandb: Agent Starting Run: u0llhsag with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.19623026370231503
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00464983486845908
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run u0llhsag
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_092535-u0llhsag
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mild-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u0llhsag
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb: train_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb:    val_acc ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñÜ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:   val_loss ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:      epoch 23
wandb:         lr 0.00116
wandb:  train_acc 0.66
wandb: train_loss 0.6142
wandb:    val_acc 0.615
wandb:   val_loss 0.64338
wandb: 
wandb: üöÄ View run [Baseline] L3-H8-D128-drop0.19623026370231503-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u0llhsag
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_092535-u0llhsag/logs
wandb: Agent Starting Run: uo6k8suf with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.4474112356041381
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009423703361981964
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run uo6k8suf
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_104614-uo6k8suf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uo6k8suf
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.15 GiB is free. Including non-PyTorch memory, this process has 372.00 MiB memory in use. Of the allocated memory 89.17 MiB is allocated by PyTorch, and 124.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L4-H8-D16-drop0.4474112356041381-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uo6k8suf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_104614-uo6k8suf/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.15 GiB is free. Including non-PyTorch memory, this process has 372.00 MiB memory in use. Of the allocated memory 89.17 MiB is allocated by PyTorch, and 124.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run uo6k8suf errored: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.15 GiB is free. Including non-PyTorch memory, this process has 372.00 MiB memory in use. Of the allocated memory 89.17 MiB is allocated by PyTorch, and 124.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: iq23kc8c with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.1735424395765627
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0057230411098553365
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run iq23kc8c
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_105018-iq23kc8c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iq23kc8c
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading history steps 0-22, summary, console lines 42-45
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ
wandb: train_loss ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:    val_acc ‚ñÉ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÉ
wandb:   val_loss ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÇ
wandb: 
wandb: Run summary:
wandb:      epoch 23
wandb:         lr 0.00286
wandb:  train_acc 0.69
wandb: train_loss 0.5638
wandb:    val_acc 0.61333
wandb:   val_loss 0.65665
wandb: 
wandb: üöÄ View run [Baseline] L3-H4-D128-drop0.1735424395765627 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iq23kc8c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_105018-iq23kc8c/logs
wandb: Agent Starting Run: 3kbab430 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.02760127948149177
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00393248736050981
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 3kbab430
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_113749-3kbab430
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3kbab430
Create sweep with ID: b2au71fa
Sweep URL: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
Loading data for experiment: Baseline
üìÇ Loading static data from ../EXP-25-IY011/data/IY011_static_train.pt...
üìÇ Loading static data from ../EXP-25-IY011/data/IY011_static_val.pt...
Starting training...
Epoch [1/100] | train_loss 0.7190 | train_acc 0.4993 | val_loss 0.7103 | val_acc 0.4850
Epoch [2/100] | train_loss 0.6964 | train_acc 0.4980 | val_loss 0.6927 | val_acc 0.5150
No improvement (1/15).
Epoch [3/100] | train_loss 0.6952 | train_acc 0.5060 | val_loss 0.6927 | val_acc 0.5150
No improvement (2/15).
Epoch [4/100] | train_loss 0.6943 | train_acc 0.5050 | val_loss 0.6927 | val_acc 0.5150
No improvement (3/15).
Epoch [5/100] | train_loss 0.6943 | train_acc 0.5093 | val_loss 0.6927 | val_acc 0.5150
No improvement (4/15).
Epoch [6/100] | train_loss 0.6944 | train_acc 0.5053 | val_loss 0.6927 | val_acc 0.5150
No improvement (5/15).
Epoch [7/100] | train_loss 0.6936 | train_acc 0.5093 | val_loss 0.6928 | val_acc 0.5150
No improvement (6/15).
Epoch [8/100] | train_loss 0.6930 | train_acc 0.5180 | val_loss 0.6930 | val_acc 0.5150
No improvement (7/15).
Epoch [9/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (8/15).
Epoch [10/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (9/15).
Epoch [11/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (10/15).
Epoch [12/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (11/15).
Epoch [13/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (12/15).
Epoch [14/100] | train_loss 0.6927 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (13/15).
Epoch [15/100] | train_loss 0.6927 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (14/15).
Epoch [16/100] | train_loss 0.6927 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 51.50%
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.9230 | train_acc 0.4943 | val_loss 0.7175 | val_acc 0.4850
Epoch [2/100] | train_loss 0.6718 | train_acc 0.5810 | val_loss 0.7686 | val_acc 0.5683
Epoch [3/100] | train_loss 0.6040 | train_acc 0.6677 | val_loss 0.5769 | val_acc 0.6750
No improvement (1/15).
Epoch [4/100] | train_loss 0.5623 | train_acc 0.7143 | val_loss 0.6361 | val_acc 0.6383
No improvement (2/15).
Epoch [5/100] | train_loss 0.5980 | train_acc 0.6853 | val_loss 0.5955 | val_acc 0.6383
Epoch [6/100] | train_loss 0.5334 | train_acc 0.7353 | val_loss 0.5538 | val_acc 0.7117
Epoch [7/100] | train_loss 0.5376 | train_acc 0.7487 | val_loss 0.5441 | val_acc 0.7283
No improvement (1/15).
Epoch [8/100] | train_loss 0.6384 | train_acc 0.6680 | val_loss 0.5932 | val_acc 0.7083
Epoch [9/100] | train_loss 0.5435 | train_acc 0.7427 | val_loss 0.5292 | val_acc 0.7450
No improvement (1/15).
Epoch [10/100] | train_loss 0.5060 | train_acc 0.7620 | val_loss 0.5384 | val_acc 0.7283
No improvement (2/15).
Epoch [11/100] | train_loss 0.5179 | train_acc 0.7577 | val_loss 0.6117 | val_acc 0.6867
No improvement (3/15).
Epoch [12/100] | train_loss 0.5757 | train_acc 0.7187 | val_loss 0.6560 | val_acc 0.6150
No improvement (4/15).
Epoch [13/100] | train_loss 0.5521 | train_acc 0.7330 | val_loss 0.6371 | val_acc 0.6917
No improvement (5/15).
Epoch [14/100] | train_loss 0.6593 | train_acc 0.7060 | val_loss 1.1029 | val_acc 0.5567
No improvement (6/15).
Epoch [15/100] | train_loss 0.6829 | train_acc 0.6087 | val_loss 0.6558 | val_acc 0.5883
No improvement (7/15).
Epoch [16/100] | train_loss 0.6096 | train_acc 0.6603 | val_loss 0.6388 | val_acc 0.6283
No improvement (8/15).
Epoch [17/100] | train_loss 0.6024 | train_acc 0.6620 | val_loss 0.6212 | val_acc 0.6833
No improvement (9/15).
Epoch [18/100] | train_loss 0.6100 | train_acc 0.6837 | val_loss 0.6450 | val_acc 0.6317
No improvement (10/15).
Epoch [19/100] | train_loss 0.6161 | train_acc 0.6653 | val_loss 0.6409 | val_acc 0.6217
No improvement (11/15).
Epoch [20/100] | train_loss 0.6205 | train_acc 0.6600 | val_loss 0.6469 | val_acc 0.6150
No improvement (12/15).
Epoch [21/100] | train_loss 0.6188 | train_acc 0.6617 | val_loss 0.6455 | val_acc 0.6250
No improvement (13/15).
Epoch [22/100] | train_loss 0.6166 | train_acc 0.6600 | val_loss 0.6442 | val_acc 0.6200
No improvement (14/15).
Epoch [23/100] | train_loss 0.6142 | train_acc 0.6600 | val_loss 0.6434 | val_acc 0.6150
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 74.50%
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.15 GiB is free. Including non-PyTorch memory, this process has 372.00 MiB memory in use. Of the allocated memory 89.17 MiB is allocated by PyTorch, and 124.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.9540 | train_acc 0.5330 | val_loss 0.6559 | val_acc 0.6067
Epoch [2/100] | train_loss 0.6488 | train_acc 0.6237 | val_loss 0.6408 | val_acc 0.6150
No improvement (1/15).
Epoch [3/100] | train_loss 0.6291 | train_acc 0.6413 | val_loss 0.7394 | val_acc 0.5917
Epoch [4/100] | train_loss 0.6037 | train_acc 0.6637 | val_loss 0.6262 | val_acc 0.6483
Epoch [5/100] | train_loss 0.5865 | train_acc 0.6857 | val_loss 0.6354 | val_acc 0.6617
No improvement (1/15).
Epoch [6/100] | train_loss 0.5883 | train_acc 0.6787 | val_loss 0.7752 | val_acc 0.6133
No improvement (2/15).
Epoch [7/100] | train_loss 0.5741 | train_acc 0.6800 | val_loss 0.6643 | val_acc 0.6067
No improvement (3/15).
Epoch [8/100] | train_loss 0.5617 | train_acc 0.6970 | val_loss 0.6225 | val_acc 0.6300
Epoch [9/100] | train_loss 0.5500 | train_acc 0.7003 | val_loss 0.5728 | val_acc 0.6850
No improvement (1/15).
Epoch [10/100] | train_loss 0.5183 | train_acc 0.7357 | val_loss 0.6030 | val_acc 0.6833
No improvement (2/15).
Epoch [11/100] | train_loss 0.5574 | train_acc 0.7080 | val_loss 0.7785 | val_acc 0.6267
No improvement (3/15).
Epoch [12/100] | train_loss 0.5554 | train_acc 0.7173 | val_loss 0.6389 | val_acc 0.6250
No improvement (4/15).
Epoch [13/100] | train_loss 0.5416 | train_acc 0.7283 | val_loss 0.7013 | val_acc 0.6167
No improvement (5/15).
Epoch [14/100] | train_loss 0.5716 | train_acc 0.7123 | val_loss 0.6647 | val_acc 0.6283
No improvement (6/15).
Epoch [15/100] | train_loss 0.5787 | train_acc 0.6943 | val_loss 0.9663 | val_acc 0.5783
No improvement (7/15).
Epoch [16/100] | train_loss 0.6112 | train_acc 0.7010 | val_loss 0.6845 | val_acc 0.6583
No improvement (8/15).
Epoch [17/100] | train_loss 0.6067 | train_acc 0.6960 | val_loss 0.7162 | val_acc 0.6433
No improvement (9/15).
Epoch [18/100] | train_loss 0.6252 | train_acc 0.6720 | val_loss 0.6401 | val_acc 0.6117
No improvement (10/15).
Epoch [19/100] | train_loss 0.5880 | train_acc 0.6917 | val_loss 0.6179 | val_acc 0.6600
No improvement (11/15).
Epoch [20/100] | train_loss 0.5870 | train_acc 0.6707 | val_loss 0.5888 | val_acc 0.6800
No improvement (12/15).
Epoch [21/100] | train_loss 0.5904 | train_acc 0.6810 | val_loss 0.6691 | val_acc 0.6117
No improvement (13/15).
Epoch [22/100] | train_loss 0.5828 | train_acc 0.6747 | val_loss 0.7703 | val_acc 0.5783
No improvement (14/15).
Epoch [23/100] | train_loss 0.5638 | train_acc 0.6900 | val_loss 0.6566 | val_acc 0.6133
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 68.50%
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.7274 | train_acc 0.5177 | val_loss 0.6872 | val_acc 0.5583
Epoch [2/100] | train_loss 0.6736 | train_acc 0.5767 | val_loss 0.6734 | val_acc 0.5733wandb: updating run metadata
wandb: uploading wandb-summary.json
wandb: uploading history steps 0-22, summary, console lines 40-43
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: train_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    val_acc ‚ñÅ‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñà‚ñà‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb: 
wandb: Run summary:
wandb:      epoch 23
wandb:         lr 0.00049
wandb:  train_acc 0.86867
wandb: train_loss 0.31084
wandb:    val_acc 0.76167
wandb:   val_loss 0.52647
wandb: 
wandb: üöÄ View run [Baseline] L4-H4-D32-drop0.02760127948149177 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3kbab430
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_113749-3kbab430/logs
wandb: Agent Starting Run: 7uocksol with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.43291149834741566
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005891624534635359
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 7uocksol
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_122835-7uocksol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7uocksol
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.13 GiB is free. Including non-PyTorch memory, this process has 390.00 MiB memory in use. Of the allocated memory 105.41 MiB is allocated by PyTorch, and 126.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H8-D16-drop0.43291149834741566-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7uocksol
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_122835-7uocksol/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.13 GiB is free. Including non-PyTorch memory, this process has 390.00 MiB memory in use. Of the allocated memory 105.41 MiB is allocated by PyTorch, and 126.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7uocksol errored: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.13 GiB is free. Including non-PyTorch memory, this process has 390.00 MiB memory in use. Of the allocated memory 105.41 MiB is allocated by PyTorch, and 126.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: r1ehbi7a with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.34935461859319245
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00492384128687423
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run r1ehbi7a
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_123141-r1ehbi7a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-7
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/r1ehbi7a
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.05 GiB is free. Including non-PyTorch memory, this process has 478.00 MiB memory in use. Of the allocated memory 192.86 MiB is allocated by PyTorch, and 127.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading summary, console lines 1-41
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.34935461859319245-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/r1ehbi7a
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_123141-r1ehbi7a/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.05 GiB is free. Including non-PyTorch memory, this process has 478.00 MiB memory in use. Of the allocated memory 192.86 MiB is allocated by PyTorch, and 127.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run r1ehbi7a errored: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.05 GiB is free. Including non-PyTorch memory, this process has 478.00 MiB memory in use. Of the allocated memory 192.86 MiB is allocated by PyTorch, and 127.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: uqqjegkv with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.2217520766388349
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004225174763467246
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_123345-uqqjegkv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-sweep-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uqqjegkv
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: train_loss ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    val_acc ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 16
wandb:         lr 0.00106
wandb:  train_acc 0.518
wandb: train_loss 0.69263
wandb:    val_acc 0.515
wandb:   val_loss 0.69274
wandb: 
wandb: üöÄ View run [Baseline] L2-H8-D64-drop0.2217520766388349-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uqqjegkv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_123345-uqqjegkv/logs
wandb: Agent Starting Run: j4gust3i with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.3150213027244187
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0017918090402077546
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run j4gust3i
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_130935-j4gust3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-9
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j4gust3i
wandb: updating run metadata
wandb: updating run metadata; uploading wandb-summary.json
wandb: uploading history steps 0-14, summary, console lines 30-33
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ
wandb:  train_acc ‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    val_acc ‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 15
wandb:         lr 0.00045
wandb:  train_acc 0.518
wandb: train_loss 0.69281
wandb:    val_acc 0.515
wandb:   val_loss 0.69273
wandb: 
wandb: üöÄ View run [Baseline] L4-H8-D64-drop0.3150213027244187-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j4gust3i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_130935-j4gust3i/logs
wandb: Agent Starting Run: yoje6aws with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.0958657079513512
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0037324648966208097
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_141648-yoje6aws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/yoje6aws
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.08 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 152.49 MiB is allocated by PyTorch, and 129.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H8-D16-drop0.0958657079513512 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/yoje6aws
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_141648-yoje6aws/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 91, in train_model
    val_loss, val_acc = evaluate_model(model, val_loader, loss_fn=loss_fn, device=device, verbose=False)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/eval.py", line 45, in evaluate_model
    outputs = model(X_batch) if mask is None else model(X_batch, src_key_padding_mask=mask.to(device))  #TODO: Correctly Handle X1, X2, y batches (this currently doesnt recognise correctly X1, X2, y)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 881, in forward
    return torch._transformer_encoder_layer_fwd(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.08 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 152.49 MiB is allocated by PyTorch, and 129.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run yoje6aws errored: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.08 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 152.49 MiB is allocated by PyTorch, and 129.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: rdg9lbaz with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.003320859265587617
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005142906026392837
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_141954-rdg9lbaz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sweep-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rdg9lbaz

Epoch [3/100] | train_loss 0.5841 | train_acc 0.6987 | val_loss 0.6027 | val_acc 0.7000
Epoch [4/100] | train_loss 0.5062 | train_acc 0.7583 | val_loss 0.5158 | val_acc 0.7417
No improvement (1/15).
Epoch [5/100] | train_loss 0.4860 | train_acc 0.7690 | val_loss 0.5305 | val_acc 0.7133
No improvement (2/15).
Epoch [6/100] | train_loss 0.4643 | train_acc 0.7843 | val_loss 0.5533 | val_acc 0.7300
Epoch [7/100] | train_loss 0.4719 | train_acc 0.7750 | val_loss 0.4996 | val_acc 0.7517
Epoch [8/100] | train_loss 0.4231 | train_acc 0.8020 | val_loss 0.5156 | val_acc 0.7650
Epoch [9/100] | train_loss 0.4165 | train_acc 0.8130 | val_loss 0.4945 | val_acc 0.7767
No improvement (1/15).
Epoch [10/100] | train_loss 0.4038 | train_acc 0.8153 | val_loss 0.5138 | val_acc 0.7650
No improvement (2/15).
Epoch [11/100] | train_loss 0.3981 | train_acc 0.8193 | val_loss 0.5050 | val_acc 0.7750
No improvement (3/15).
Epoch [12/100] | train_loss 0.3914 | train_acc 0.8280 | val_loss 0.5260 | val_acc 0.7667
No improvement (4/15).
Epoch [13/100] | train_loss 0.3831 | train_acc 0.8307 | val_loss 0.5252 | val_acc 0.7583
No improvement (5/15).
Epoch [14/100] | train_loss 0.3840 | train_acc 0.8273 | val_loss 0.4827 | val_acc 0.7667
No improvement (6/15).
Epoch [15/100] | train_loss 0.3755 | train_acc 0.8420 | val_loss 0.4909 | val_acc 0.7667
No improvement (7/15).
Epoch [16/100] | train_loss 0.3698 | train_acc 0.8420 | val_loss 0.4950 | val_acc 0.7717
No improvement (8/15).
Epoch [17/100] | train_loss 0.3649 | train_acc 0.8427 | val_loss 0.5003 | val_acc 0.7717
No improvement (9/15).
Epoch [18/100] | train_loss 0.3540 | train_acc 0.8470 | val_loss 0.5138 | val_acc 0.7567
No improvement (10/15).
Epoch [19/100] | train_loss 0.3508 | train_acc 0.8523 | val_loss 0.5123 | val_acc 0.7733
No improvement (11/15).
Epoch [20/100] | train_loss 0.3303 | train_acc 0.8597 | val_loss 0.5078 | val_acc 0.7700
No improvement (12/15).
Epoch [21/100] | train_loss 0.3269 | train_acc 0.8603 | val_loss 0.5124 | val_acc 0.7683
No improvement (13/15).
Epoch [22/100] | train_loss 0.3184 | train_acc 0.8633 | val_loss 0.5175 | val_acc 0.7633
No improvement (14/15).
Epoch [23/100] | train_loss 0.3108 | train_acc 0.8687 | val_loss 0.5265 | val_acc 0.7617
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 77.67%
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.13 GiB is free. Including non-PyTorch memory, this process has 390.00 MiB memory in use. Of the allocated memory 105.41 MiB is allocated by PyTorch, and 126.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.05 GiB is free. Including non-PyTorch memory, this process has 478.00 MiB memory in use. Of the allocated memory 192.86 MiB is allocated by PyTorch, and 127.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.7332 | train_acc 0.5040 | val_loss 0.7210 | val_acc 0.4850
Epoch [2/100] | train_loss 0.6948 | train_acc 0.5147 | val_loss 0.6927 | val_acc 0.5150
No improvement (1/15).
Epoch [3/100] | train_loss 0.6961 | train_acc 0.5027 | val_loss 0.6927 | val_acc 0.5150
No improvement (2/15).
Epoch [4/100] | train_loss 0.6955 | train_acc 0.5047 | val_loss 0.6927 | val_acc 0.5150
No improvement (3/15).
Epoch [5/100] | train_loss 0.6948 | train_acc 0.5073 | val_loss 0.6927 | val_acc 0.5150
No improvement (4/15).
Epoch [6/100] | train_loss 0.6936 | train_acc 0.5053 | val_loss 0.6927 | val_acc 0.5150
No improvement (5/15).
Epoch [7/100] | train_loss 0.6931 | train_acc 0.5180 | val_loss 0.6927 | val_acc 0.5150
No improvement (6/15).
Epoch [8/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (7/15).
Epoch [9/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (8/15).
Epoch [10/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (9/15).
Epoch [11/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (10/15).
Epoch [12/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (11/15).
Epoch [13/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (12/15).
Epoch [14/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6927 | val_acc 0.5150
No improvement (13/15).
Epoch [15/100] | train_loss 0.6926 | train_acc 0.5180 | val_loss 0.6927 | val_acc 0.5150
No improvement (14/15).
Epoch [16/100] | train_loss 0.6926 | train_acc 0.5180 | val_loss 0.6927 | val_acc 0.5150
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 51.50%
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.7424 | train_acc 0.4933 | val_loss 0.6930 | val_acc 0.5150
No improvement (1/15).
Epoch [2/100] | train_loss 0.7005 | train_acc 0.4887 | val_loss 0.7020 | val_acc 0.4850
No improvement (2/15).
Epoch [3/100] | train_loss 0.6958 | train_acc 0.5073 | val_loss 0.6927 | val_acc 0.5150
No improvement (3/15).
Epoch [4/100] | train_loss 0.6956 | train_acc 0.5040 | val_loss 0.6928 | val_acc 0.5150
No improvement (4/15).
Epoch [5/100] | train_loss 0.6958 | train_acc 0.5097 | val_loss 0.6927 | val_acc 0.5150
No improvement (5/15).
Epoch [6/100] | train_loss 0.6950 | train_acc 0.4990 | val_loss 0.6927 | val_acc 0.5150
No improvement (6/15).
Epoch [7/100] | train_loss 0.6942 | train_acc 0.5100 | val_loss 0.6927 | val_acc 0.5150
No improvement (7/15).
Epoch [8/100] | train_loss 0.6937 | train_acc 0.5120 | val_loss 0.6927 | val_acc 0.5150
No improvement (8/15).
Epoch [9/100] | train_loss 0.6931 | train_acc 0.5080 | val_loss 0.6929 | val_acc 0.5150
No improvement (9/15).
Epoch [10/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (10/15).
Epoch [11/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (11/15).
Epoch [12/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (12/15).
Epoch [13/100] | train_loss 0.6929 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5150
No improvement (13/15).
Epoch [14/100] | train_loss 0.6930 | train_acc 0.5180 | val_loss 0.6928 | val_acc 0.5150
No improvement (14/15).
Epoch [15/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6927 | val_acc 0.5150
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 51.50%
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 25.04 GiB. GPU 0 has a total capacity of 7.60 GiB of which 7.08 GiB is free. Including non-PyTorch memory, this process has 440.00 MiB memory in use. Of the allocated memory 152.49 MiB is allocated by PyTorch, and 129.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.9063 | train_acc 0.5210 | val_loss 0.7345 | val_acc 0.5267
Epoch [2/100] | train_loss 0.6431 | train_acc 0.6190 | val_loss 0.6675 | val_acc 0.6083
Epoch [3/100] | train_loss 0.6296 | train_acc 0.6587 | val_loss 0.6231 | val_acc 0.6650wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: train_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    val_acc ‚ñÅ‚ñÑ‚ñÖ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá
wandb:   val_loss ‚ñá‚ñÖ‚ñÑ‚ñá‚ñà‚ñÑ‚ñÉ‚ñá‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ
wandb: 
wandb: Run summary:
wandb:      epoch 40
wandb:         lr 0.00016
wandb:  train_acc 0.86933
wandb: train_loss 0.30124
wandb:    val_acc 0.74
wandb:   val_loss 0.63355
wandb: 
wandb: üöÄ View run [Baseline] L2-H8-D128-drop0.003320859265587617 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rdg9lbaz
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_141954-rdg9lbaz/logs
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: b3yntzhf with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.4635553521326133
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0027160038592413355
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run b3yntzhf
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_155146-b3yntzhf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/b3yntzhf

No improvement (1/15).
Epoch [4/100] | train_loss 0.6500 | train_acc 0.6437 | val_loss 0.7150 | val_acc 0.5183
No improvement (2/15).
Epoch [5/100] | train_loss 0.6710 | train_acc 0.6440 | val_loss 0.7590 | val_acc 0.6133
No improvement (3/15).
Epoch [6/100] | train_loss 0.5544 | train_acc 0.7303 | val_loss 0.6238 | val_acc 0.6500
Epoch [7/100] | train_loss 0.5458 | train_acc 0.7147 | val_loss 0.5989 | val_acc 0.7000
No improvement (1/15).
Epoch [8/100] | train_loss 0.5526 | train_acc 0.7183 | val_loss 0.7182 | val_acc 0.6250
Epoch [9/100] | train_loss 0.5687 | train_acc 0.7127 | val_loss 0.5810 | val_acc 0.7050
No improvement (1/15).
Epoch [10/100] | train_loss 0.5926 | train_acc 0.6890 | val_loss 0.6528 | val_acc 0.6567
No improvement (2/15).
Epoch [11/100] | train_loss 0.5217 | train_acc 0.7410 | val_loss 0.5964 | val_acc 0.6850
No improvement (3/15).
Epoch [12/100] | train_loss 0.5159 | train_acc 0.7453 | val_loss 0.6038 | val_acc 0.6733
No improvement (4/15).
Epoch [13/100] | train_loss 0.4869 | train_acc 0.7510 | val_loss 0.6190 | val_acc 0.7000
Epoch [14/100] | train_loss 0.5117 | train_acc 0.7477 | val_loss 0.5751 | val_acc 0.7200
No improvement (1/15).
Epoch [15/100] | train_loss 0.4833 | train_acc 0.7687 | val_loss 0.6057 | val_acc 0.6883
No improvement (2/15).
Epoch [16/100] | train_loss 0.5107 | train_acc 0.7447 | val_loss 0.5727 | val_acc 0.7183
No improvement (3/15).
Epoch [17/100] | train_loss 0.4674 | train_acc 0.7673 | val_loss 0.5831 | val_acc 0.7150
No improvement (4/15).
Epoch [18/100] | train_loss 0.4580 | train_acc 0.7823 | val_loss 0.5863 | val_acc 0.7133
No improvement (5/15).
Epoch [19/100] | train_loss 0.4474 | train_acc 0.7833 | val_loss 0.5860 | val_acc 0.7133
No improvement (6/15).
Epoch [20/100] | train_loss 0.4429 | train_acc 0.7920 | val_loss 0.5635 | val_acc 0.7167
Epoch [21/100] | train_loss 0.4466 | train_acc 0.7830 | val_loss 0.5550 | val_acc 0.7383
No improvement (1/15).
Epoch [22/100] | train_loss 0.4352 | train_acc 0.7940 | val_loss 0.6778 | val_acc 0.7083
No improvement (2/15).
Epoch [23/100] | train_loss 0.4719 | train_acc 0.7743 | val_loss 0.6449 | val_acc 0.6517
No improvement (3/15).
Epoch [24/100] | train_loss 0.4333 | train_acc 0.7913 | val_loss 0.5439 | val_acc 0.7383
Epoch [25/100] | train_loss 0.4024 | train_acc 0.8067 | val_loss 0.5527 | val_acc 0.7467
Epoch [26/100] | train_loss 0.3882 | train_acc 0.8223 | val_loss 0.5510 | val_acc 0.7600
No improvement (1/15).
Epoch [27/100] | train_loss 0.3830 | train_acc 0.8257 | val_loss 0.5655 | val_acc 0.7483
No improvement (2/15).
Epoch [28/100] | train_loss 0.3804 | train_acc 0.8313 | val_loss 0.5704 | val_acc 0.7500
No improvement (3/15).
Epoch [29/100] | train_loss 0.3766 | train_acc 0.8330 | val_loss 0.5905 | val_acc 0.7433
No improvement (4/15).
Epoch [30/100] | train_loss 0.3542 | train_acc 0.8363 | val_loss 0.5980 | val_acc 0.7483
No improvement (5/15).
Epoch [31/100] | train_loss 0.3489 | train_acc 0.8430 | val_loss 0.5870 | val_acc 0.7533
No improvement (6/15).
Epoch [32/100] | train_loss 0.3451 | train_acc 0.8450 | val_loss 0.5939 | val_acc 0.7433
No improvement (7/15).
Epoch [33/100] | train_loss 0.3327 | train_acc 0.8543 | val_loss 0.5865 | val_acc 0.7467
No improvement (8/15).
Epoch [34/100] | train_loss 0.3396 | train_acc 0.8523 | val_loss 0.6131 | val_acc 0.7200
No improvement (9/15).
Epoch [35/100] | train_loss 0.3300 | train_acc 0.8537 | val_loss 0.5861 | val_acc 0.7467
No improvement (10/15).
Epoch [36/100] | train_loss 0.3164 | train_acc 0.8637 | val_loss 0.6128 | val_acc 0.7483
No improvement (11/15).
Epoch [37/100] | train_loss 0.3141 | train_acc 0.8657 | val_loss 0.6151 | val_acc 0.7517
No improvement (12/15).
Epoch [38/100] | train_loss 0.3100 | train_acc 0.8670 | val_loss 0.6175 | val_acc 0.7500
No improvement (13/15).
Epoch [39/100] | train_loss 0.3059 | train_acc 0.8710 | val_loss 0.6256 | val_acc 0.7467
No improvement (14/15).
Epoch [40/100] | train_loss 0.3012 | train_acc 0.8693 | val_loss 0.6336 | val_acc 0.7400
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 76.00%
üßπ Cleared CUDA Memory
Starting training...
Epoch [1/100] | train_loss 0.6998 | train_acc 0.5140 | val_loss 0.6921 | val_acc 0.5150
Epoch [2/100] | train_loss 0.6952 | train_acc 0.5013 | val_loss 0.6920 | val_acc 0.5700
No improvement (1/15).
Epoch [3/100] | train_loss 0.6947 | train_acc 0.5050 | val_loss 0.6922 | val_acc 0.5583
No improvement (2/15).
Epoch [4/100] | train_loss 0.6932 | train_acc 0.5147 | val_loss 0.6903 | val_acc 0.5633
No improvement (3/15).
Epoch [5/100] | train_loss 0.6915 | train_acc 0.5300 | val_loss 0.6862 | val_acc 0.5500
No improvement (4/15).
Epoch [6/100] | train_loss 0.6865 | train_acc 0.5317 | val_loss 0.6755 | val_acc 0.5550
Epoch [7/100] | train_loss 0.6570 | train_acc 0.5690 | val_loss 0.6552 | val_acc 0.5867
Epoch [8/100] | train_loss 0.6005 | train_acc 0.6540 | val_loss 0.5596 | val_acc 0.7067
No improvement (1/15).
Epoch [9/100] | train_loss 0.5415 | train_acc 0.7293 | val_loss 0.5645 | val_acc 0.7017
Epoch [10/100] | train_loss 0.5075 | train_acc 0.7537 | val_loss 0.5586 | val_acc 0.7117
Epoch [11/100] | train_loss 0.5028 | train_acc 0.7567 | val_loss 0.5519 | val_acc 0.7200
No improvement (1/15).
Epoch [12/100] | train_loss 0.4981 | train_acc 0.7643 | val_loss 0.5471 | val_acc 0.7200
Epoch [13/100] | train_loss 0.4953 | train_acc 0.7637 | val_loss 0.5487 | val_acc 0.7233
No improvement (1/15).
Epoch [14/100] | train_loss 0.4938 | train_acc 0.7670 | val_loss 0.5503 | val_acc 0.7200
Epoch [15/100] | train_loss 0.4912 | train_acc 0.7687 | val_loss 0.5488 | val_acc 0.7267
No improvement (1/15).
Epoch [16/100] | train_loss 0.4860 | train_acc 0.7733 | val_loss 0.5510 | val_acc 0.7217
No improvement (2/15).
Epoch [17/100] | train_loss 0.4847 | train_acc 0.7743 | val_loss 0.5508 | val_acc 0.7217
Epoch [18/100] | train_loss 0.4838 | train_acc 0.7743 | val_loss 0.5484 | val_acc 0.7283
No improvement (1/15).
Epoch [19/100] | train_loss 0.4822 | train_acc 0.7753 | val_loss 0.5492 | val_acc 0.7250
No improvement (2/15).
Epoch [20/100] | train_loss 0.4819 | train_acc 0.7787 | val_loss 0.5477 | val_acc 0.7283
Epoch [21/100] | train_loss 0.4810 | train_acc 0.7760 | val_loss 0.5471 | val_acc 0.7317
Epoch [22/100] | train_loss 0.4775 | train_acc 0.7797 | val_loss 0.5401 | val_acc 0.7333
Epoch [23/100] | train_loss 0.4769 | train_acc 0.7803 | val_loss 0.5408 | val_acc 0.7350
No improvement (1/15).
Epoch [24/100] | train_loss 0.4776 | train_acc 0.7797 | val_loss 0.5417 | val_acc 0.7350
No improvement (2/15).
Epoch [25/100] | train_loss 0.4762 | train_acc 0.7783 | val_loss 0.5415 | val_acc 0.7317
No improvement (3/15).
Epoch [26/100] | train_loss 0.4760 | train_acc 0.7790 | val_loss 0.5421 | val_acc 0.7317
No improvement (4/15).
Epoch [27/100] | train_loss 0.4757 | train_acc 0.7813 | val_loss 0.5415 | val_acc 0.7350
Epoch [28/100] | train_loss 0.4744 | train_acc 0.7800 | val_loss 0.5406 | val_acc 0.7417
No improvement (1/15).
Epoch [29/100] | train_loss 0.4750 | train_acc 0.7797 | val_loss 0.5403 | val_acc 0.7367
No improvement (2/15).
Epoch [30/100] | train_loss 0.4740 | train_acc 0.7833 | val_loss 0.5409 | val_acc 0.7333
No improvement (3/15).
Epoch [31/100] | train_loss 0.4741 | train_acc 0.7787 | val_loss 0.5407 | val_acc 0.7350
No improvement (4/15).
Epoch [32/100] | train_loss 0.4726 | train_acc 0.7833 | val_loss 0.5411 | val_acc 0.7350
No improvement (5/15).
Epoch [33/100] | train_loss 0.4739 | train_acc 0.7813 | val_loss 0.5419 | val_acc 0.7333
No improvement (6/15).
Epoch [34/100] | train_loss 0.4727 | train_acc 0.7810 | val_loss 0.5453 | val_acc 0.7333
No improvement (7/15).
Epoch [35/100] | train_loss 0.4707 | train_acc 0.7810 | val_loss 0.5451 | val_acc 0.7333
No improvement (8/15).
Epoch [36/100] | train_loss 0.4699 | train_acc 0.7847 | val_loss 0.5460 | val_acc 0.7350
No improvement (9/15).
Epoch [37/100] | train_loss 0.4715 | train_acc 0.7807 | val_loss 0.5448 | val_acc 0.7300
No improvement (10/15).
Epoch [38/100] | train_loss 0.4705 | train_acc 0.7817 | val_loss 0.5460 | val_acc 0.7333
No improvement (11/15).
Epoch [39/100] | train_loss 0.4698 | train_acc 0.7837 | val_loss 0.5459 | val_acc 0.7333wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading history steps 0-41, summary, console lines 72-75
wandb: 
wandb: Run history:
wandb:      epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:         lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:  train_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: train_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    val_acc ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:   val_loss ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:      epoch 42
wandb:         lr 4e-05
wandb:  train_acc 0.78533
wandb: train_loss 0.46862
wandb:    val_acc 0.73
wandb:   val_loss 0.5482
wandb: 
wandb: üöÄ View run [Baseline] L2-H2-D32-drop0.4635553521326133-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/b3yntzhf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_155146-b3yntzhf/logs
wandb: Agent Starting Run: hbwx58ge with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.2908857865946974
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004377150403000461
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run hbwx58ge
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161617-hbwx58ge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lucky-sweep-13
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hbwx58ge
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 941, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 454.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 316.00 MiB is free. Including non-PyTorch memory, this process has 5.93 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 85.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H8-D128-drop0.2908857865946974-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hbwx58ge
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161617-hbwx58ge/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 941, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 454.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 316.00 MiB is free. Including non-PyTorch memory, this process has 5.93 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 85.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run hbwx58ge errored: CUDA out of memory. Tried to allocate 454.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 316.00 MiB is free. Including non-PyTorch memory, this process has 5.93 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 85.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7qq65dbr with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.26293826428585815
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003197382597346367
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 7qq65dbr
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161623-7qq65dbr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sweep-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7qq65dbr
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 941, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.99 GiB is allocated by PyTorch, and 72.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-38
wandb: üöÄ View run [Baseline] L3-H4-D64-drop0.26293826428585815 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7qq65dbr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161623-7qq65dbr/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 910, in forward
    x = x + self._ff_block(self.norm2(x))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 941, in _ff_block
    x = self.linear2(self.dropout(self.activation(self.linear1(x))))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.99 GiB is allocated by PyTorch, and 72.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7qq65dbr errored: CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.99 GiB is allocated by PyTorch, and 72.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 1lzh75mv with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.2146162685483873
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006940176374823269
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 1lzh75mv
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161629-1lzh75mv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1lzh75mv
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5614, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 16.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.00 GiB is allocated by PyTorch, and 67.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt; uploading console lines 0-0
wandb: updating run metadata; uploading console lines 0-0; uploading output.log
wandb: uploading summary, console lines 1-53
wandb: üöÄ View run [Baseline] L1-H4-D128-drop0.2146162685483873-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1lzh75mv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161629-1lzh75mv/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5614, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 16.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.00 GiB is allocated by PyTorch, and 67.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1lzh75mv errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 16.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.00 GiB is allocated by PyTorch, and 67.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 4mgblo71 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.0025957983986830135
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0006634417465924706
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 4mgblo71
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161634-4mgblo71
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sweep-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4mgblo71
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 12.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 66.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-26
wandb: üöÄ View run [Baseline] L4-H4-D128-drop0.0025957983986830135 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4mgblo71
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161634-4mgblo71/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 12.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 66.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4mgblo71 errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 12.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 66.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: rwyqm8rk with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.4319258272297097
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007875013218207159
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run rwyqm8rk
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161640-rwyqm8rk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run light-sweep-17
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rwyqm8rk
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 10.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-26
wandb: üöÄ View run [Baseline] L3-H8-D64-drop0.4319258272297097 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rwyqm8rk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161640-rwyqm8rk/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 10.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run rwyqm8rk errored: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 10.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 880z2tc5 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.12535997520220032
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004271164829745543
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 880z2tc5
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161646-880z2tc5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/880z2tc5
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading summary, console lines 0-26
wandb: üöÄ View run [Baseline] L3-H8-D64-drop0.12535997520220032 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/880z2tc5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161646-880z2tc5/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 880z2tc5 errored: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: a3jr2vzt with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.22455380399359096
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007293687394085875
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161651-a3jr2vzt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dazzling-sweep-19
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/a3jr2vzt
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading config.yaml
wandb: uploading summary, console lines 0-26
wandb: üöÄ View run [Baseline] L4-H8-D64-drop0.22455380399359096 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/a3jr2vzt
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161651-a3jr2vzt/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] ‚Üí unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run a3jr2vzt errored: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 52meklz2 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.1689904572690245
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0003105875186385786
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 52meklz2
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161657-52meklz2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/52meklz2
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 87, in forward
    x = self.stem(x)
        ^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading wandb-summary.json; uploading output.log
wandb: üöÄ View run [Baseline] L2-H2-D32-drop0.1689904572690245-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/52meklz2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161657-52meklz2/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 87, in forward
    x = self.stem(x)
        ^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 52meklz2 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: npe4k92x with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.0224852722471911
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00804403214238588
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run npe4k92x
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161703-npe4k92x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-21
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/npe4k92x
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-8
wandb: üöÄ View run [Baseline] L2-H4-D32-drop0.0224852722471911 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/npe4k92x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161703-npe4k92x/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run npe4k92x errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 8n72lzqi with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.24425423634640264
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00921122489495708
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 8n72lzqi
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161708-8n72lzqi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-sweep-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8n72lzqi
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H8-D32-drop0.24425423634640264-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8n72lzqi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161708-8n72lzqi/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8n72lzqi errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: h7y0or9d with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.48834388562319264
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009889543872060551
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run h7y0or9d
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161714-h7y0or9d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-23
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/h7y0or9d
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L3-H8-D32-drop0.48834388562319264-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/h7y0or9d
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161714-h7y0or9d/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run h7y0or9d errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 5rehtnvu with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.3695307418972322
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0024440574330874677
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 5rehtnvu
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161720-5rehtnvu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5rehtnvu
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-8
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.3695307418972322 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5rehtnvu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161720-5rehtnvu/logs

No improvement (12/15).
Epoch [40/100] | train_loss 0.4711 | train_acc 0.7830 | val_loss 0.5480 | val_acc 0.7300
No improvement (13/15).
Epoch [41/100] | train_loss 0.4704 | train_acc 0.7837 | val_loss 0.5483 | val_acc 0.7300
No improvement (14/15).
Epoch [42/100] | train_loss 0.4686 | train_acc 0.7853 | val_loss 0.5482 | val_acc 0.7300
No improvement (15/15).
üõë Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 74.17%
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 454.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 316.00 MiB is free. Including non-PyTorch memory, this process has 5.93 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.69 GiB is allocated by PyTorch, and 85.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 228.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 5.99 GiB is allocated by PyTorch, and 72.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 16.00 MiB is free. Including non-PyTorch memory, this process has 6.22 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.00 GiB is allocated by PyTorch, and 67.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 12.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 66.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 10.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.01 GiB is allocated by PyTorch, and 65.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 6.00 MiB is free. Including non-PyTorch memory, this process has 6.23 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 64.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5rehtnvu errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7ya4826y with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.16908855530651962
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008215616162758512
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 7ya4826y
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161725-7ya4826y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-25
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7ya4826y
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 1-8
wandb: üöÄ View run [Baseline] L1-H4-D16-drop0.16908855530651962 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7ya4826y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161725-7ya4826y/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 124, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7ya4826y errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 2o6dgo6t with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.4546894340078893
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007668102770989669
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 2o6dgo6t
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161731-2o6dgo6t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clear-sweep-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2o6dgo6t
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H2-D32-drop0.4546894340078893-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2o6dgo6t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161731-2o6dgo6t/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2o6dgo6t errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 8ff7agzv with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.25775726039074
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0056922902710051125
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 8ff7agzv
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161736-8ff7agzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8ff7agzv
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L2-H4-D64-drop0.25775726039074-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8ff7agzv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161736-8ff7agzv/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8ff7agzv errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: yb35j56n with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.32089384929198256
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0027767097781114655
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run yb35j56n
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161742-yb35j56n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run honest-sweep-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/yb35j56n
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L1-H8-D64-drop0.32089384929198256-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/yb35j56n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161742-yb35j56n/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run yb35j56n errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 33lj8bl8 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.4617613208565079
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00371525670485502
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161748-33lj8bl8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sweep-29
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/33lj8bl8
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.4617613208565079 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/33lj8bl8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161748-33lj8bl8/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 33lj8bl8 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: t8ce7jr5 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.4928011567751961
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0006275477359215394
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run t8ce7jr5
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161753-t8ce7jr5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-sweep-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t8ce7jr5
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading summary, console lines 0-19
wandb: üöÄ View run [Baseline] L3-H2-D128-drop0.4928011567751961-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t8ce7jr5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161753-t8ce7jr5/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run t8ce7jr5 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ntbw9739 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.37913636850612986
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007681111536911982
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run ntbw9739
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161759-ntbw9739
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-31
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ntbw9739
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L4-H8-D32-drop0.37913636850612986-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ntbw9739
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161759-ntbw9739/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ntbw9739 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 47ksuikb with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.16499183902078146
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0005987560583625548
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 47ksuikb
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161805-47ksuikb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/47ksuikb
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H8-D32-drop0.16499183902078146 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/47ksuikb
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161805-47ksuikb/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 47ksuikb errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ltqebyv0 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.2304847286042744
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009158768554070572
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run ltqebyv0
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161810-ltqebyv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-33
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ltqebyv0
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L1-H2-D32-drop0.2304847286042744 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ltqebyv0
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161810-ltqebyv0/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ltqebyv0 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 00zhjnyr with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.003247549309051856
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008401413965960386
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 00zhjnyr
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161816-00zhjnyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rare-sweep-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/00zhjnyr
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L2-H4-D64-drop0.003247549309051856-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/00zhjnyr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161816-00zhjnyr/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 00zhjnyr errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: n54imkp2 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.2393387886114025
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0019867530025087948
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run n54imkp2
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161822-n54imkp2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-sweep-35
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/n54imkp2
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H4-D64-drop0.2393387886114025 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/n54imkp2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161822-n54imkp2/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n54imkp2 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: vs1v9hu7 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.08090701530510669
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0011554885694274744
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run vs1v9hu7
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161828-vs1v9hu7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vs1v9hu7
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L4-H8-D16-drop0.08090701530510669 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vs1v9hu7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161828-vs1v9hu7/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vs1v9hu7 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: uqn76448 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.4590347413697869
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00538089744547321
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161834-uqn76448
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-sweep-37
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uqn76448
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: üöÄ View run [Baseline] L2-H8-D16-drop0.4590347413697869 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uqn76448
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161834-uqn76448/logs

üßπ Cleared CUDA Memory
Starting training...
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 63.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run uqn76448 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: zh64oq3e with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.36722554015570935
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004676173616724071
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161840-zh64oq3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sweep-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zh64oq3e
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 2-14
wandb: üöÄ View run [Baseline] L2-H2-D64-drop0.36722554015570935 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zh64oq3e
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161840-zh64oq3e/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zh64oq3e errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ooct7r8j with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.451150630109204
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007047442656885168
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run ooct7r8j
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161845-ooct7r8j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-sweep-39
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ooct7r8j
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: uploading summary, console lines 0-19
wandb: üöÄ View run [Baseline] L3-H8-D128-drop0.451150630109204-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ooct7r8j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161845-ooct7r8j/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ooct7r8j errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: iq17efly with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.0018930642337048888
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003327457799287298
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run iq17efly
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161851-iq17efly
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sweep-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iq17efly
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L3-H8-D32-drop0.0018930642337048888 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iq17efly
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161851-iq17efly/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run iq17efly errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: jknuf0vm with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.4812971059036999
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0026065800546408546
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161856-jknuf0vm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-sweep-41
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jknuf0vm
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.4812971059036999-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jknuf0vm
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161856-jknuf0vm/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jknuf0vm errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ohaf5rno with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.25040917397213425
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0037703313903648264
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run ohaf5rno
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161902-ohaf5rno
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run laced-sweep-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ohaf5rno
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: üöÄ View run [Baseline] L2-H8-D16-drop0.25040917397213425 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ohaf5rno
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161902-ohaf5rno/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ohaf5rno errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 1lueujcc with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.21939464261846192
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007582189693215974
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 1lueujcc
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161908-1lueujcc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-sweep-43
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1lueujcc
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L4-H4-D64-drop0.21939464261846192-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1lueujcc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161908-1lueujcc/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1lueujcc errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: jqm01qho with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.4062176397485136
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008298077166834745
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run jqm01qho
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161913-jqm01qho
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jqm01qho
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H2-D128-drop0.4062176397485136 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jqm01qho
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161913-jqm01qho/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jqm01qho errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 16z6jxdr with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.34108864596416943
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0005122778483241985
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 16z6jxdr
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161919-16z6jxdr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-sweep-45
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/16z6jxdr
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 9-14
wandb: üöÄ View run [Baseline] L3-H2-D16-drop0.34108864596416943-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/16z6jxdr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161919-16z6jxdr/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 16z6jxdr errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 5eay3s4k with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.3167463105161891
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003618191335312088
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161924-5eay3s4k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-sweep-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5eay3s4k
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H4-D16-drop0.3167463105161891-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5eay3s4k
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161924-5eay3s4k/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5eay3s4k errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: qxsnq1f2 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.3340427482580145
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00010259666891059116
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161931-qxsnq1f2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-47
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qxsnq1f2
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H2-D128-drop0.3340427482580145 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qxsnq1f2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161931-qxsnq1f2/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qxsnq1f2 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: a6hb093j with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.1826136277360851
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00043993554554223587
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run a6hb093j
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161936-a6hb093j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-48
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/a6hb093j
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json; uploading requirements.txt
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H8-D64-drop0.1826136277360851 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/a6hb093j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161936-a6hb093j/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run a6hb093j errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: afw3igo5 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.21138172466148947
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004362745060103798
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run afw3igo5
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161942-afw3igo5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-49
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/afw3igo5
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L4-H2-D16-drop0.21138172466148947-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/afw3igo5
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161942-afw3igo5/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run afw3igo5 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: i7mjh03q with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.3039200776438209
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006398888257164943
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run i7mjh03q
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161948-i7mjh03q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-sweep-50
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i7mjh03q
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.3039200776438209-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i7mjh03q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161948-i7mjh03q/logs

üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 61.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.02 GiB is allocated by PyTorch, and 59.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 57.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i7mjh03q errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 5exq570f with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.3163938126107682
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0035317758486229896
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 5exq570f
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161953-5exq570f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-sweep-51
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5exq570f
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H4-D16-drop0.3163938126107682-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5exq570f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161953-5exq570f/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5exq570f errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: fdewoxhp with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.24943501512949484
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005941230195868143
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run fdewoxhp
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_161959-fdewoxhp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-52
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fdewoxhp
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 3-19
wandb: üöÄ View run [Baseline] L3-H8-D128-drop0.24943501512949484 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fdewoxhp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_161959-fdewoxhp/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run fdewoxhp errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 1mzwwio8 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.4986195202003701
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.002724214698309644
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 1mzwwio8
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162005-1mzwwio8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-sweep-53
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1mzwwio8
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L3-H2-D16-drop0.4986195202003701 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1mzwwio8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162005-1mzwwio8/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1mzwwio8 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 0mpgm84w with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.1959325452827253
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0020173603736273163
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 0mpgm84w
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162011-0mpgm84w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run helpful-sweep-54
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0mpgm84w
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L1-H8-D64-drop0.1959325452827253 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0mpgm84w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162011-0mpgm84w/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0mpgm84w errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: httnqm0p with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.05268487726791965
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006300280421944473
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run httnqm0p
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162016-httnqm0p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run gentle-sweep-55
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/httnqm0p
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H2-D32-drop0.05268487726791965-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/httnqm0p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162016-httnqm0p/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run httnqm0p errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: akvvt9ay with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.10256397233702769
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003805684467799608
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run akvvt9ay
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162022-akvvt9ay
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-56
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/akvvt9ay
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L3-H2-D32-drop0.10256397233702769-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/akvvt9ay
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162022-akvvt9ay/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run akvvt9ay errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: h2cik8pa with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.005286856680612517
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.002540259834486832
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run h2cik8pa
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162028-h2cik8pa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-sweep-57
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/h2cik8pa
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.005286856680612517 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/h2cik8pa
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162028-h2cik8pa/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run h2cik8pa errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 44wkufk1 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.15489547897603018
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004915175081798887
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162034-44wkufk1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-58
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/44wkufk1
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L2-H2-D64-drop0.15489547897603018 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/44wkufk1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162034-44wkufk1/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 44wkufk1 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: t586phyp with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.08511768306519141
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008943969061367024
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run t586phyp
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162040-t586phyp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-59
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t586phyp
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H2-D64-drop0.08511768306519141 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t586phyp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162040-t586phyp/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run t586phyp errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: v2m5ewu1 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.05403534773213753
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009396297884263063
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162045-v2m5ewu1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-60
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v2m5ewu1
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L1-H8-D16-drop0.05403534773213753-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v2m5ewu1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162045-v2m5ewu1/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v2m5ewu1 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: da2ddgg9 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.15777110793151822
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003753123965519797
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run da2ddgg9
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162051-da2ddgg9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-sweep-61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/da2ddgg9
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-19
wandb: üöÄ View run [Baseline] L2-H2-D128-drop0.15777110793151822-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/da2ddgg9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162051-da2ddgg9/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run da2ddgg9 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: wu9g3ywi with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.07577488242668917
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009414220066171175
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run wu9g3ywi
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162057-wu9g3ywi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-62
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wu9g3ywi
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L2-H4-D64-drop0.07577488242668917-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wu9g3ywi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162057-wu9g3ywi/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wu9g3ywi errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: a0bu070i with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.15897503907505955
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009648234424033276
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162102-a0bu070i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-63
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/a0bu070i
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 2-19
wandb: üöÄ View run [Baseline] L4-H4-D128-drop0.15897503907505955 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/a0bu070i
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162102-a0bu070i/logs

üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 55.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 53.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 52.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run a0bu070i errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: eg3qxl2c with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.1447934134658323
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009472263877669798
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run eg3qxl2c
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162108-eg3qxl2c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-64
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/eg3qxl2c
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L4-H2-D32-drop0.1447934134658323-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/eg3qxl2c
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162108-eg3qxl2c/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run eg3qxl2c errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 5cxbcr9h with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.4881046256021091
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0016619365728730074
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 5cxbcr9h
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162114-5cxbcr9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sweep-65
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5cxbcr9h
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L4-H4-D16-drop0.4881046256021091 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5cxbcr9h
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162114-5cxbcr9h/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5cxbcr9h errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7yp2mz6u with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.1043208273277807
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005225549737617961
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 7yp2mz6u
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162119-7yp2mz6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-66
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7yp2mz6u
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: üöÄ View run [Baseline] L4-H2-D128-drop0.1043208273277807 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7yp2mz6u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162119-7yp2mz6u/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7yp2mz6u errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 8dfn5avn with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.3424833327369376
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0014983179923742282
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162125-8dfn5avn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-67
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8dfn5avn
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L1-H2-D32-drop0.3424833327369376 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8dfn5avn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162125-8dfn5avn/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8dfn5avn errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: u0giim99 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.2403235246488241
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00163928166315322
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run u0giim99
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162131-u0giim99
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run skilled-sweep-68
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u0giim99
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L3-H2-D32-drop0.2403235246488241-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u0giim99
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162131-u0giim99/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run u0giim99 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: kkozz4xe with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.2300587347088604
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004682381573891748
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run kkozz4xe
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162136-kkozz4xe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-69
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/kkozz4xe
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading console lines 0-0
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H2-D32-drop0.2300587347088604-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/kkozz4xe
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162136-kkozz4xe/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run kkozz4xe errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: f6iyptsl with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.366444356063068
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0019321670732148231
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162142-f6iyptsl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-sweep-70
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f6iyptsl
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-19
wandb: üöÄ View run [Baseline] L4-H8-D128-drop0.366444356063068 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f6iyptsl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162142-f6iyptsl/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f6iyptsl errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ly18timw with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.15239062533634018
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.002851225735224127
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162148-ly18timw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-71
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ly18timw
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L1-H2-D64-drop0.15239062533634018 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ly18timw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162148-ly18timw/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ly18timw errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: fig5qaaw with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.1015086136489982
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004822899410101936
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run fig5qaaw
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162154-fig5qaaw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-72
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fig5qaaw
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H4-D16-drop0.1015086136489982-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fig5qaaw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162154-fig5qaaw/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run fig5qaaw errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: zfaq1iic with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.44766182004024646
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007150935480163482
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run zfaq1iic
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162159-zfaq1iic
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sweep-73
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zfaq1iic
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading console lines 0-0; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L4-H8-D32-drop0.44766182004024646 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zfaq1iic
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162159-zfaq1iic/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zfaq1iic errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: s3yt5gzp with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.3838857412403716
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.001233820498622023
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run s3yt5gzp
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162205-s3yt5gzp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devout-sweep-74
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/s3yt5gzp
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L4-H8-D16-drop0.3838857412403716-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/s3yt5gzp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162205-s3yt5gzp/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run s3yt5gzp errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 9sd9fg74 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.08307313670025662
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007372238684105916
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 9sd9fg74
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162211-9sd9fg74
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fanciful-sweep-75
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9sd9fg74
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-19
wandb: üöÄ View run [Baseline] L3-H4-D128-drop0.08307313670025662-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9sd9fg74
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162211-9sd9fg74/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9sd9fg74 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 4kxw661n with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.31025844145965276
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00025792341145293527
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162217-4kxw661n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-76
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4kxw661n
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L2-H8-D16-drop0.31025844145965276-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4kxw661n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162217-4kxw661n/logs

üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 50.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.03 GiB is allocated by PyTorch, and 48.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 46.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4kxw661n errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 44.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: vu7ekde1 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.2652107803450165
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.000529053538179635
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run vu7ekde1
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162222-vu7ekde1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run northern-sweep-77
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vu7ekde1
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-19
wandb: üöÄ View run [Baseline] L3-H2-D128-drop0.2652107803450165 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vu7ekde1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162222-vu7ekde1/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vu7ekde1 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 2wnuwzys with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.33016569577167176
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003941108580468074
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 2wnuwzys
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162228-2wnuwzys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-78
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2wnuwzys
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.33016569577167176-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2wnuwzys
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162228-2wnuwzys/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2wnuwzys errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: cxtdjnqv with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.4728188906703462
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0025848973053963957
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run cxtdjnqv
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162234-cxtdjnqv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pious-sweep-79
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/cxtdjnqv
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 0-19
wandb: üöÄ View run [Baseline] L4-H2-D128-drop0.4728188906703462 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/cxtdjnqv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162234-cxtdjnqv/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run cxtdjnqv errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ssqxjsl7 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.3087516913419221
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007788266609982141
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run ssqxjsl7
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162240-ssqxjsl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-80
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ssqxjsl7
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L3-H2-D32-drop0.3087516913419221-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ssqxjsl7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162240-ssqxjsl7/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ssqxjsl7 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: v392oasy with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.0028647386937241115
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009410706926613385
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162245-v392oasy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-81
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v392oasy
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 2-14
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.0028647386937241115-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v392oasy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162245-v392oasy/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v392oasy errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: v7f7e293 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.2817279582433036
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007046399932199327
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162251-v7f7e293
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-82
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v7f7e293
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L3-H4-D16-drop0.2817279582433036 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v7f7e293
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162251-v7f7e293/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v7f7e293 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7qqsd5ed with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.17554553987299226
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006048039921750625
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162257-7qqsd5ed
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-sweep-83
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7qqsd5ed
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.17554553987299226-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7qqsd5ed
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162257-7qqsd5ed/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7qqsd5ed errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: jy5yc0ky with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.3760711447794265
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0045190051287546384
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run jy5yc0ky
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162303-jy5yc0ky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-84
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jy5yc0ky
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H2-D16-drop0.3760711447794265 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jy5yc0ky
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162303-jy5yc0ky/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jy5yc0ky errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ki9asa7t with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.1423194341175869
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005444281967516382
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run ki9asa7t
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162308-ki9asa7t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-85
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ki9asa7t
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L4-H8-D32-drop0.1423194341175869 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ki9asa7t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162308-ki9asa7t/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ki9asa7t errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: jd1ob1mo with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.1647340181542271
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007929009549471596
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run jd1ob1mo
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162314-jd1ob1mo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-sweep-86
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jd1ob1mo
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L4-H4-D16-drop0.1647340181542271 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jd1ob1mo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162314-jd1ob1mo/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jd1ob1mo errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ogccyiqs with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.4564043283997442
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00903409423165562
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run ogccyiqs
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162320-ogccyiqs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-sweep-87
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ogccyiqs
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.4564043283997442 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ogccyiqs
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162320-ogccyiqs/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ogccyiqs errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: r0pheedd with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.2930800148851733
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005527636026936612
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run r0pheedd
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162325-r0pheedd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-88
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/r0pheedd
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt
wandb: uploading wandb-summary.json
wandb: üöÄ View run [Baseline] L4-H2-D128-drop0.2930800148851733-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/r0pheedd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162325-r0pheedd/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run r0pheedd errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: nkub889o with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.45774069522542704
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0099808732953094
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run nkub889o
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162331-nkub889o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-sweep-89
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nkub889o
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading summary, console lines 1-14
wandb: üöÄ View run [Baseline] L2-H4-D16-drop0.45774069522542704-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nkub889o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162331-nkub889o/logs

üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 43.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 41.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nkub889o errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: i4zwfsb8 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.2861605584855624
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004497386442336286
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run i4zwfsb8
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162337-i4zwfsb8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-sweep-90
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i4zwfsb8
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L1-H4-D64-drop0.2861605584855624 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i4zwfsb8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162337-i4zwfsb8/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i4zwfsb8 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: qb5ndfx3 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.0805386241416905
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008640717377370294
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run qb5ndfx3
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162342-qb5ndfx3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run major-sweep-91
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qb5ndfx3
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading data
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L2-H4-D64-drop0.0805386241416905-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qb5ndfx3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162342-qb5ndfx3/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qb5ndfx3 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 0keepudu with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.1673550280049218
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005696524062279923
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162348-0keepudu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-sweep-92
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0keepudu
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: üöÄ View run [Baseline] L2-H2-D16-drop0.1673550280049218 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0keepudu
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162348-0keepudu/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0keepudu errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7stl4u1f with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.0539282610677822
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007976020250023703
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162354-7stl4u1f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-sweep-93
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7stl4u1f
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-16
wandb: üöÄ View run [Baseline] L1-H4-D64-drop0.0539282610677822-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7stl4u1f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162354-7stl4u1f/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7stl4u1f errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ehk11yxg with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.3586525925744816
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.001008391203139787
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run ehk11yxg
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162400-ehk11yxg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-sweep-94
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ehk11yxg
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 0-14
wandb: üöÄ View run [Baseline] L4-H8-D64-drop0.3586525925744816 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ehk11yxg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162400-ehk11yxg/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ehk11yxg errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: jz91r8tl with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.22255002013907088
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005945520730671605
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run jz91r8tl
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162406-jz91r8tl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-sweep-95
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jz91r8tl
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-16
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.22255002013907088-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/jz91r8tl
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162406-jz91r8tl/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run jz91r8tl errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: aqsjeig1 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.3057025106668758
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009546222221690014
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run aqsjeig1
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162411-aqsjeig1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-96
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/aqsjeig1
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L3-H2-D128-drop0.3057025106668758-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/aqsjeig1
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162411-aqsjeig1/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run aqsjeig1 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: xrcvzz1r with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.27882549947261703
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0035744253140748707
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run xrcvzz1r
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162417-xrcvzz1r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run feasible-sweep-97
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xrcvzz1r
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading data
wandb: üöÄ View run [Baseline] L1-H2-D128-drop0.27882549947261703 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xrcvzz1r
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162417-xrcvzz1r/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xrcvzz1r errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: wmt6ipfi with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.22795455594016223
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004638848016454744
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run wmt6ipfi
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162423-wmt6ipfi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-sweep-98
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wmt6ipfi
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading data
wandb: uploading summary, console lines 0-16
wandb: üöÄ View run [Baseline] L3-H4-D128-drop0.22795455594016223-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wmt6ipfi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162423-wmt6ipfi/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wmt6ipfi errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: hc0d8nk7 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.05990994706765573
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0020694727862595624
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run hc0d8nk7
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162429-hc0d8nk7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sweep-99
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hc0d8nk7
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-16
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.05990994706765573-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hc0d8nk7
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162429-hc0d8nk7/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run hc0d8nk7 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 5yzklbbf with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.4371047047452393
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004797290013885885
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 5yzklbbf
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_162434-5yzklbbf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ruby-sweep-100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: üßπ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/b2au71fa
wandb: üöÄ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5yzklbbf
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading config.yaml
wandb: üöÄ View run [Baseline] L2-H8-D32-drop0.4371047047452393-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/5yzklbbf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_162434-5yzklbbf/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 163, in run_sweep_agent
    raise e  # Re-raise so WandB knows it failed
    ^^^^^^^
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 5yzklbbf errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
‚ùå Run Failed with error: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 6.24 GiB memory in use. Process 1819041 has 1.27 GiB memory in use. Of the allocated memory 6.04 GiB is allocated by PyTorch, and 39.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
üßπ Cleared CUDA Memory
