[I 2026-02-01 14:18:34,469] A new study created in memory with name: IY018_Bayesian
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/ianyang/.netrc.
wandb: Currently logged in as: grignardreagent (grignard-reagent) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run l0as6y65
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_141835-l0as6y65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial0_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/l0as6y65
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–…â–…â–„â–‡â–â–ˆâ–‚â–‚â–‚â–ƒâ–ƒâ–‡â–ˆâ–„â–â–â–‡â–†â–‚â–„
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–‡â–†â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–†â–ƒâ–‡â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95667
wandb:             epoch 38
wandb:         grad/norm 0.50589
wandb:                lr 4e-05
wandb:         train/acc 0.99233
wandb:        train/loss 0.02783
wandb: training_time_sec 16413.08817
wandb:           val/acc 0.94
wandb:          val/loss 0.29877
wandb: 
wandb: ğŸš€ View run Trial0_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/l0as6y65
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_141835-l0as6y65/logs
[I 2026-02-01 18:52:10,855] Trial 0 finished with value: 0.9566666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0013662606357508903, 'dropout': 0.3964200883071475}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run d961snp6
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_185211-d961snp6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial1_[CV-Variation]_L4_H2_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d961snp6
ğŸš€ Starting Optuna Bayesian Sweep for 100 rounds...
ğŸ“‚ Saving results to IY018_optuna_sweep_results_cv.csv
ğŸ“‚ Loading data for experiment: CV-Variation
ğŸ“‚ Loading static data from ../EXP-25-IY011/data_cv_variation/IY011_static_train.pt...
ğŸ“‚ Loading static data from ../EXP-25-IY011/data_cv_variation/IY011_static_val.pt...
Starting training...
Epoch [1/100] | train_loss 0.7545 | train_acc 0.5277 | val_loss 0.6847 | val_acc 0.5967
Epoch [2/100] | train_loss 0.6255 | train_acc 0.6497 | val_loss 0.5345 | val_acc 0.7017
Epoch [3/100] | train_loss 0.3752 | train_acc 0.8397 | val_loss 0.3347 | val_acc 0.8900
No improvement (1/15).
Epoch [4/100] | train_loss 0.2416 | train_acc 0.9033 | val_loss 0.6235 | val_acc 0.8550
No improvement (2/15).
Epoch [5/100] | train_loss 0.1870 | train_acc 0.9360 | val_loss 0.7246 | val_acc 0.7783
Epoch [6/100] | train_loss 0.1467 | train_acc 0.9420 | val_loss 0.2406 | val_acc 0.9167
Epoch [7/100] | train_loss 0.1363 | train_acc 0.9497 | val_loss 0.2117 | val_acc 0.9283
Epoch [8/100] | train_loss 0.1236 | train_acc 0.9550 | val_loss 0.2131 | val_acc 0.9317
Epoch [9/100] | train_loss 0.1139 | train_acc 0.9590 | val_loss 0.2510 | val_acc 0.9350
Epoch [10/100] | train_loss 0.1107 | train_acc 0.9643 | val_loss 0.2019 | val_acc 0.9400
Epoch [11/100] | train_loss 0.1003 | train_acc 0.9663 | val_loss 0.1479 | val_acc 0.9550
No improvement (1/15).
Epoch [12/100] | train_loss 0.0835 | train_acc 0.9707 | val_loss 0.2304 | val_acc 0.9350
No improvement (2/15).
Epoch [13/100] | train_loss 0.0843 | train_acc 0.9677 | val_loss 0.2035 | val_acc 0.9433
No improvement (3/15).
Epoch [14/100] | train_loss 0.0791 | train_acc 0.9717 | val_loss 0.2064 | val_acc 0.9450
No improvement (4/15).
Epoch [15/100] | train_loss 0.0731 | train_acc 0.9743 | val_loss 0.1723 | val_acc 0.9450
No improvement (5/15).
Epoch [16/100] | train_loss 0.0717 | train_acc 0.9743 | val_loss 0.2716 | val_acc 0.9283
No improvement (6/15).
Epoch [17/100] | train_loss 0.0721 | train_acc 0.9750 | val_loss 0.2947 | val_acc 0.9350
No improvement (7/15).
Epoch [18/100] | train_loss 0.0716 | train_acc 0.9757 | val_loss 0.2041 | val_acc 0.9450
No improvement (8/15).
Epoch [19/100] | train_loss 0.0649 | train_acc 0.9770 | val_loss 0.2305 | val_acc 0.9450
No improvement (9/15).
Epoch [20/100] | train_loss 0.0640 | train_acc 0.9787 | val_loss 0.2166 | val_acc 0.9500
No improvement (10/15).
Epoch [21/100] | train_loss 0.0610 | train_acc 0.9807 | val_loss 0.2105 | val_acc 0.9467
No improvement (11/15).
Epoch [22/100] | train_loss 0.0589 | train_acc 0.9810 | val_loss 0.2241 | val_acc 0.9433
No improvement (12/15).
Epoch [23/100] | train_loss 0.0544 | train_acc 0.9813 | val_loss 0.2305 | val_acc 0.9450
Epoch [24/100] | train_loss 0.0539 | train_acc 0.9803 | val_loss 0.1952 | val_acc 0.9567
No improvement (1/15).
Epoch [25/100] | train_loss 0.0612 | train_acc 0.9773 | val_loss 0.2039 | val_acc 0.9467
No improvement (2/15).
Epoch [26/100] | train_loss 0.0589 | train_acc 0.9807 | val_loss 0.1958 | val_acc 0.9533
No improvement (3/15).
Epoch [27/100] | train_loss 0.0562 | train_acc 0.9800 | val_loss 0.2072 | val_acc 0.9483
No improvement (4/15).
Epoch [28/100] | train_loss 0.0525 | train_acc 0.9813 | val_loss 0.2140 | val_acc 0.9533
No improvement (5/15).
Epoch [29/100] | train_loss 0.0474 | train_acc 0.9847 | val_loss 0.2191 | val_acc 0.9517
No improvement (6/15).
Epoch [30/100] | train_loss 0.0391 | train_acc 0.9863 | val_loss 0.2421 | val_acc 0.9433
No improvement (7/15).
Epoch [31/100] | train_loss 0.0385 | train_acc 0.9863 | val_loss 0.2413 | val_acc 0.9417
No improvement (8/15).
Epoch [32/100] | train_loss 0.0351 | train_acc 0.9880 | val_loss 0.2534 | val_acc 0.9417
No improvement (9/15).
Epoch [33/100] | train_loss 0.0328 | train_acc 0.9893 | val_loss 0.2598 | val_acc 0.9450
No improvement (10/15).
Epoch [34/100] | train_loss 0.0334 | train_acc 0.9890 | val_loss 0.2694 | val_acc 0.9433
No improvement (11/15).
Epoch [35/100] | train_loss 0.0317 | train_acc 0.9907 | val_loss 0.2790 | val_acc 0.9433
No improvement (12/15).
Epoch [36/100] | train_loss 0.0293 | train_acc 0.9923 | val_loss 0.2860 | val_acc 0.9433
No improvement (13/15).
Epoch [37/100] | train_loss 0.0278 | train_acc 0.9920 | val_loss 0.2929 | val_acc 0.9417
No improvement (14/15).
Epoch [38/100] | train_loss 0.0278 | train_acc 0.9923 | val_loss 0.2988 | val_acc 0.9400
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 0 Finished. Best Val Acc: 95.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7983 | train_acc 0.4847 | val_loss 0.6946 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6925 | train_acc 0.5183 | val_loss 0.6931 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6923 | train_acc 0.5210 | val_loss 0.6928 | val_acc 0.4900
Epoch [4/100] | train_loss 0.6915 | train_acc 0.5190 | val_loss 0.6921 | val_acc 0.4967
Epoch [5/100] | train_loss 0.6908 | train_acc 0.5410 | val_loss 0.6913 | val_acc 0.5217
Epoch [6/100] | train_loss 0.6899 | train_acc 0.5530 | val_loss 0.6903 | val_acc 0.5833
Epoch [7/100] | train_loss 0.6888 | train_acc 0.5617 | val_loss 0.6892 | val_acc 0.6033
No improvement (1/15).
Epoch [8/100] | train_loss 0.6876 | train_acc 0.5617 | val_loss 0.6899 | val_acc 0.5267
No improvement (2/15).
Epoch [9/100] | train_loss 0.6866 | train_acc 0.5680 | val_loss 0.6890 | val_acc 0.5567
No improvement (3/15).
Epoch [10/100] | train_loss 0.6858 | train_acc 0.5743 | val_loss 0.6882 | val_acc 0.5767
No improvement (4/15).
Epoch [11/100] | train_loss 0.6847 | train_acc 0.5883 | val_loss 0.6873 | val_acc 0.5867
No improvement (5/15).
Epoch [12/100] | train_loss 0.6835 | train_acc 0.6043 | val_loss 0.6862 | val_acc 0.6000
No improvement (6/15).
Epoch [13/100] | train_loss 0.6821 | train_acc 0.6127 | val_loss 0.6849 | val_acc 0.5983
No improvement (7/15).
Epoch [14/100] | train_loss 0.6808 | train_acc 0.6200 | val_loss 0.6849 | val_acc 0.6033
No improvement (8/15).
Epoch [15/100] | train_loss 0.6799 | train_acc 0.6247 | val_loss 0.6842 | val_acc 0.5967
No improvement (9/15).
Epoch [16/100] | train_loss 0.6789 | train_acc 0.6253 | val_loss 0.6834 | val_acc 0.6017
No improvement (10/15).
Epoch [17/100] | train_loss 0.6778 | train_acc 0.6290 | val_loss 0.6826 | val_acc 0.6033
Epoch [18/100] | train_loss 0.6768 | train_acc 0.6257 | val_loss 0.6817 | val_acc 0.6083
Epoch [19/100] | train_loss 0.6757 | train_acc 0.6290 | val_loss 0.6808 | val_acc 0.6117
No improvement (1/15).
Epoch [20/100] | train_loss 0.6746 | train_acc 0.6270 | val_loss 0.6805 | val_acc 0.6100
No improvement (2/15).
Epoch [21/100] | train_loss 0.6740 | train_acc 0.6293 | val_loss 0.6801 | val_acc 0.6067
No improvement (3/15).
Epoch [22/100] | train_loss 0.6733 | train_acc 0.6300 | val_loss 0.6796 | val_acc 0.6100
No improvement (4/15).
Epoch [23/100] | train_loss 0.6727 | train_acc 0.6290 | val_loss 0.6791 | val_acc 0.6100
Epoch [24/100] | train_loss 0.6720 | train_acc 0.6300 | val_loss 0.6786 | val_acc 0.6133
No improvement (1/15).
Epoch [25/100] | train_loss 0.6713 | train_acc 0.6293 | val_loss 0.6781 | val_acc 0.6100
No improvement (2/15).
Epoch [26/100] | train_loss 0.6706 | train_acc 0.6297 | val_loss 0.6778 | val_acc 0.6117
No improvement (3/15).
Epoch [27/100] | train_loss 0.6704 | train_acc 0.6317 | val_loss 0.6776 | val_acc 0.6133
No improvement (4/15).
Epoch [28/100] | train_loss 0.6699 | train_acc 0.6323 | val_loss 0.6774 | val_acc 0.6117
No improvement (5/15).
Epoch [29/100] | train_loss 0.6695 | train_acc 0.6333 | val_loss 0.6771 | val_acc 0.6117
No improvement (6/15).
Epoch [30/100] | train_loss 0.6692 | train_acc 0.6357 | val_loss 0.6768 | val_acc 0.6117
No improvement (7/15).
Epoch [31/100] | train_loss 0.6689 | train_acc 0.6323 | val_loss 0.6766 | val_acc 0.6117
No improvement (8/15).
Epoch [32/100] | train_loss 0.6684 | train_acc 0.6320 | val_loss 0.6764 | val_acc 0.6133
No improvement (9/15).
Epoch [33/100] | train_loss 0.6683 | train_acc 0.6330 | val_loss 0.6763 | val_acc 0.6117
No improvement (10/15).
Epoch [34/100] | train_loss 0.6681 | train_acc 0.6327 | val_loss 0.6762 | val_acc 0.6117
No improvement (11/15).
Epoch [35/100] | train_loss 0.6680 | train_acc 0.6323 | val_loss 0.6760 | val_acc 0.6117wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–ƒâ–…â–„â–…â–†â–‡â–ˆâ–â–‚â–ƒâ–„â–„â–…â–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–â–ƒâ–†â–‡â–ƒâ–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.61333
wandb:             epoch 38
wandb:         grad/norm 0.22093
wandb:                lr 0.0
wandb:         train/acc 0.63133
wandb:        train/loss 0.66745
wandb: training_time_sec 4108.9268
wandb:           val/acc 0.61333
wandb:          val/loss 0.67569
wandb: 
wandb: ğŸš€ View run Trial1_[CV-Variation]_L4_H2_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d961snp6
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_185211-d961snp6/logs
[I 2026-02-01 20:00:41,771] Trial 1 finished with value: 0.6133333333333333 and parameters: {'nhead': 2, 'num_layers': 4, 'd_model': 16, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.00013238117853619496, 'dropout': 0.17704478524400225}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run 03b879et
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_200042-03b879et
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial2_[CV-Variation]_L2_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/03b879et
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–â–â–â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–â–â–â–‚â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–‚â–‚â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.88167
wandb:             epoch 47
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.94033
wandb:        train/loss 0.15902
wandb: training_time_sec 5415.36334
wandb:           val/acc 0.87833
wandb:          val/loss 0.42271
wandb: 
wandb: ğŸš€ View run Trial2_[CV-Variation]_L2_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/03b879et
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_200042-03b879et/logs
[I 2026-02-01 21:30:59,134] Trial 2 finished with value: 0.8816666666666667 and parameters: {'nhead': 4, 'num_layers': 2, 'd_model': 64, 'batch_size': 32, 'use_conv1d': True, 'lr': 0.0006810800698326676, 'dropout': 0.3605609685905193}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run maymdgmp
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_213059-maymdgmp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial3_[CV-Variation]_L1_H4_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/maymdgmp

No improvement (12/15).
Epoch [36/100] | train_loss 0.6677 | train_acc 0.6310 | val_loss 0.6759 | val_acc 0.6117
No improvement (13/15).
Epoch [37/100] | train_loss 0.6677 | train_acc 0.6317 | val_loss 0.6758 | val_acc 0.6133
No improvement (14/15).
Epoch [38/100] | train_loss 0.6675 | train_acc 0.6313 | val_loss 0.6757 | val_acc 0.6133
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 1 Finished. Best Val Acc: 61.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7003 | train_acc 0.5010 | val_loss 0.6944 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6942 | train_acc 0.5073 | val_loss 0.6940 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6928 | train_acc 0.5137 | val_loss 0.6936 | val_acc 0.4900
Epoch [4/100] | train_loss 0.6912 | train_acc 0.5233 | val_loss 0.6878 | val_acc 0.5383
Epoch [5/100] | train_loss 0.6616 | train_acc 0.5943 | val_loss 0.6683 | val_acc 0.5700
Epoch [6/100] | train_loss 0.6071 | train_acc 0.6253 | val_loss 0.5455 | val_acc 0.7000
Epoch [7/100] | train_loss 0.4947 | train_acc 0.7330 | val_loss 0.6432 | val_acc 0.7333
Epoch [8/100] | train_loss 0.3658 | train_acc 0.8357 | val_loss 0.4175 | val_acc 0.8350
Epoch [9/100] | train_loss 0.2897 | train_acc 0.8820 | val_loss 0.3853 | val_acc 0.8400
Epoch [10/100] | train_loss 0.2552 | train_acc 0.8943 | val_loss 0.4058 | val_acc 0.8567
Epoch [11/100] | train_loss 0.2353 | train_acc 0.9013 | val_loss 0.3766 | val_acc 0.8583
No improvement (1/15).
Epoch [12/100] | train_loss 0.2288 | train_acc 0.9093 | val_loss 0.3925 | val_acc 0.8583
Epoch [13/100] | train_loss 0.2143 | train_acc 0.9157 | val_loss 0.4145 | val_acc 0.8650
No improvement (1/15).
Epoch [14/100] | train_loss 0.2064 | train_acc 0.9173 | val_loss 0.4237 | val_acc 0.8517
No improvement (2/15).
Epoch [15/100] | train_loss 0.2022 | train_acc 0.9170 | val_loss 0.4283 | val_acc 0.8533
No improvement (3/15).
Epoch [16/100] | train_loss 0.2038 | train_acc 0.9190 | val_loss 0.4301 | val_acc 0.8517
No improvement (4/15).
Epoch [17/100] | train_loss 0.1993 | train_acc 0.9247 | val_loss 0.4444 | val_acc 0.8550
No improvement (5/15).
Epoch [18/100] | train_loss 0.1931 | train_acc 0.9213 | val_loss 0.4465 | val_acc 0.8533
No improvement (6/15).
Epoch [19/100] | train_loss 0.1969 | train_acc 0.9253 | val_loss 0.4250 | val_acc 0.8533
No improvement (7/15).
Epoch [20/100] | train_loss 0.1935 | train_acc 0.9220 | val_loss 0.3963 | val_acc 0.8633
No improvement (8/15).
Epoch [21/100] | train_loss 0.1855 | train_acc 0.9243 | val_loss 0.3947 | val_acc 0.8650
No improvement (9/15).
Epoch [22/100] | train_loss 0.1837 | train_acc 0.9257 | val_loss 0.4008 | val_acc 0.8650
Epoch [23/100] | train_loss 0.1808 | train_acc 0.9250 | val_loss 0.4007 | val_acc 0.8667
Epoch [24/100] | train_loss 0.1790 | train_acc 0.9273 | val_loss 0.4093 | val_acc 0.8700
No improvement (1/15).
Epoch [25/100] | train_loss 0.1785 | train_acc 0.9293 | val_loss 0.4139 | val_acc 0.8683
Epoch [26/100] | train_loss 0.1711 | train_acc 0.9307 | val_loss 0.4159 | val_acc 0.8733
No improvement (1/15).
Epoch [27/100] | train_loss 0.1687 | train_acc 0.9327 | val_loss 0.4146 | val_acc 0.8733
Epoch [28/100] | train_loss 0.1680 | train_acc 0.9347 | val_loss 0.4177 | val_acc 0.8750
No improvement (1/15).
Epoch [29/100] | train_loss 0.1673 | train_acc 0.9327 | val_loss 0.4236 | val_acc 0.8750
No improvement (2/15).
Epoch [30/100] | train_loss 0.1678 | train_acc 0.9320 | val_loss 0.4262 | val_acc 0.8733
No improvement (3/15).
Epoch [31/100] | train_loss 0.1659 | train_acc 0.9330 | val_loss 0.4294 | val_acc 0.8733
Epoch [32/100] | train_loss 0.1680 | train_acc 0.9353 | val_loss 0.4252 | val_acc 0.8800
Epoch [33/100] | train_loss 0.1648 | train_acc 0.9363 | val_loss 0.4301 | val_acc 0.8817
No improvement (1/15).
Epoch [34/100] | train_loss 0.1677 | train_acc 0.9370 | val_loss 0.4312 | val_acc 0.8800
No improvement (2/15).
Epoch [35/100] | train_loss 0.1632 | train_acc 0.9377 | val_loss 0.4289 | val_acc 0.8817
No improvement (3/15).
Epoch [36/100] | train_loss 0.1633 | train_acc 0.9360 | val_loss 0.4280 | val_acc 0.8817
No improvement (4/15).
Epoch [37/100] | train_loss 0.1643 | train_acc 0.9370 | val_loss 0.4312 | val_acc 0.8817
No improvement (5/15).
Epoch [38/100] | train_loss 0.1625 | train_acc 0.9357 | val_loss 0.4227 | val_acc 0.8800
No improvement (6/15).
Epoch [39/100] | train_loss 0.1637 | train_acc 0.9377 | val_loss 0.4241 | val_acc 0.8800
No improvement (7/15).
Epoch [40/100] | train_loss 0.1601 | train_acc 0.9370 | val_loss 0.4230 | val_acc 0.8783
No improvement (8/15).
Epoch [41/100] | train_loss 0.1621 | train_acc 0.9347 | val_loss 0.4248 | val_acc 0.8817
No improvement (9/15).
Epoch [42/100] | train_loss 0.1623 | train_acc 0.9377 | val_loss 0.4257 | val_acc 0.8800
No improvement (10/15).
Epoch [43/100] | train_loss 0.1647 | train_acc 0.9363 | val_loss 0.4207 | val_acc 0.8783
No improvement (11/15).
Epoch [44/100] | train_loss 0.1607 | train_acc 0.9353 | val_loss 0.4210 | val_acc 0.8783
No improvement (12/15).
Epoch [45/100] | train_loss 0.1601 | train_acc 0.9383 | val_loss 0.4226 | val_acc 0.8783
No improvement (13/15).
Epoch [46/100] | train_loss 0.1581 | train_acc 0.9353 | val_loss 0.4213 | val_acc 0.8783
No improvement (14/15).
Epoch [47/100] | train_loss 0.1590 | train_acc 0.9403 | val_loss 0.4227 | val_acc 0.8783
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 2 Finished. Best Val Acc: 88.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6931 | train_acc 0.5190 | val_loss 0.6935 | val_acc 0.4883
Epoch [2/100] | train_loss 0.6895 | train_acc 0.5367 | val_loss 0.6917 | val_acc 0.5517
Epoch [3/100] | train_loss 0.6853 | train_acc 0.5610 | val_loss 0.6890 | val_acc 0.5767
Epoch [4/100] | train_loss 0.6696 | train_acc 0.6123 | val_loss 0.6764 | val_acc 0.5833
Epoch [5/100] | train_loss 0.6346 | train_acc 0.6527 | val_loss 0.6574 | val_acc 0.5950
Epoch [6/100] | train_loss 0.6054 | train_acc 0.6740 | val_loss 0.6356 | val_acc 0.6467
Epoch [7/100] | train_loss 0.5779 | train_acc 0.7110 | val_loss 0.6197 | val_acc 0.7083
Epoch [8/100] | train_loss 0.5579 | train_acc 0.7370 | val_loss 0.6145 | val_acc 0.7200
Epoch [9/100] | train_loss 0.5507 | train_acc 0.7417 | val_loss 0.6116 | val_acc 0.7233
Epoch [10/100] | train_loss 0.5459 | train_acc 0.7480 | val_loss 0.6090 | val_acc 0.7267
No improvement (1/15).
Epoch [11/100] | train_loss 0.5421 | train_acc 0.7513 | val_loss 0.6069 | val_acc 0.7233
No improvement (2/15).
Epoch [12/100] | train_loss 0.5390 | train_acc 0.7527 | val_loss 0.6052 | val_acc 0.7233
Epoch [13/100] | train_loss 0.5362 | train_acc 0.7570 | val_loss 0.6038 | val_acc 0.7283
Epoch [14/100] | train_loss 0.5331 | train_acc 0.7623 | val_loss 0.6024 | val_acc 0.7300
No improvement (1/15).
Epoch [15/100] | train_loss 0.5321 | train_acc 0.7620 | val_loss 0.6020 | val_acc 0.7300
No improvement (2/15).
Epoch [16/100] | train_loss 0.5310 | train_acc 0.7643 | val_loss 0.6016 | val_acc 0.7300
No improvement (3/15).
Epoch [17/100] | train_loss 0.5302 | train_acc 0.7643 | val_loss 0.6012 | val_acc 0.7300
No improvement (4/15).
Epoch [18/100] | train_loss 0.5292 | train_acc 0.7657 | val_loss 0.6009 | val_acc 0.7300
No improvement (5/15).
Epoch [19/100] | train_loss 0.5284 | train_acc 0.7653 | val_loss 0.6005 | val_acc 0.7300
No improvement (6/15).
Epoch [20/100] | train_loss 0.5273 | train_acc 0.7623 | val_loss 0.6007 | val_acc 0.7283
No improvement (7/15).
Epoch [21/100] | train_loss 0.5269 | train_acc 0.7627 | val_loss 0.6007 | val_acc 0.7283
No improvement (8/15).
Epoch [22/100] | train_loss 0.5266 | train_acc 0.7633 | val_loss 0.6007 | val_acc 0.7283
No improvement (9/15).
Epoch [23/100] | train_loss 0.5261 | train_acc 0.7623 | val_loss 0.6006 | val_acc 0.7300
No improvement (10/15).
Epoch [24/100] | train_loss 0.5257 | train_acc 0.7633 | val_loss 0.6006 | val_acc 0.7300
No improvement (11/15).
Epoch [25/100] | train_loss 0.5254 | train_acc 0.7623 | val_loss 0.6005 | val_acc 0.7300
Epoch [26/100] | train_loss 0.5246 | train_acc 0.7637 | val_loss 0.6010 | val_acc 0.7333
No improvement (1/15).
Epoch [27/100] | train_loss 0.5243 | train_acc 0.7637 | val_loss 0.6009 | val_acc 0.7317wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–‚â–â–‚â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–‚â–„â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–‡â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–„â–„â–„â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–‡â–…â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.73333
wandb:             epoch 40
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.76467
wandb:        train/loss 0.52245
wandb: training_time_sec 2617.46994
wandb:           val/acc 0.72667
wandb:          val/loss 0.60077
wandb: 
wandb: ğŸš€ View run Trial3_[CV-Variation]_L1_H4_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/maymdgmp
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_213059-maymdgmp/logs
[I 2026-02-01 22:14:38,624] Trial 3 finished with value: 0.7333333333333333 and parameters: {'nhead': 4, 'num_layers': 1, 'd_model': 128, 'batch_size': 32, 'use_conv1d': True, 'lr': 0.00012112664646114145, 'dropout': 0.33327072265204977}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run rtovkt5h
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_221438-rtovkt5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial4_[CV-Variation]_L1_H4_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rtovkt5h
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆ
wandb:  grad/norm â–â–ˆâ–†â–‡â–‡â–‡â–‡â–†â–…â–…â–…â–…â–…â–…â–…
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–
wandb:  train/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: train/loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:   val/loss â–ˆâ–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.49
wandb:             epoch 15
wandb:         grad/norm 0.09705
wandb:                lr 3e-05
wandb:         train/acc 0.51833
wandb:        train/loss 0.69255
wandb: training_time_sec 846.5237
wandb:           val/acc 0.49
wandb:          val/loss 0.69428
wandb: 
wandb: ğŸš€ View run Trial4_[CV-Variation]_L1_H4_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rtovkt5h
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_221438-rtovkt5h/logs
[I 2026-02-01 22:28:47,115] Trial 4 finished with value: 0.49 and parameters: {'nhead': 4, 'num_layers': 1, 'd_model': 32, 'batch_size': 64, 'use_conv1d': True, 'lr': 0.00010950578932951961, 'dropout': 0.2742917045148085}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run f30prchj
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_222847-f30prchj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial5_[CV-Variation]_L1_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f30prchj
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–â–â–‚â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–â–â–â–‚â–ƒâ–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–â–„â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.935
wandb:             epoch 42
wandb:         grad/norm 1.0
wandb:                lr 4e-05
wandb:         train/acc 0.97067
wandb:        train/loss 0.09232
wandb: training_time_sec 4535.71627
wandb:           val/acc 0.93
wandb:          val/loss 0.21945
wandb: 
wandb: ğŸš€ View run Trial5_[CV-Variation]_L1_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f30prchj
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_222847-f30prchj/logs
[I 2026-02-01 23:44:24,881] Trial 5 finished with value: 0.935 and parameters: {'nhead': 8, 'num_layers': 1, 'd_model': 64, 'batch_size': 128, 'use_conv1d': True, 'lr': 0.002417015403192399, 'dropout': 0.4030496746175367}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run vgn6khmc
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260201_234425-vgn6khmc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial6_[CV-Variation]_L1_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vgn6khmc

No improvement (2/15).
Epoch [28/100] | train_loss 0.5241 | train_acc 0.7640 | val_loss 0.6009 | val_acc 0.7317
No improvement (3/15).
Epoch [29/100] | train_loss 0.5239 | train_acc 0.7637 | val_loss 0.6009 | val_acc 0.7267
No improvement (4/15).
Epoch [30/100] | train_loss 0.5238 | train_acc 0.7643 | val_loss 0.6008 | val_acc 0.7267
No improvement (5/15).
Epoch [31/100] | train_loss 0.5234 | train_acc 0.7640 | val_loss 0.6008 | val_acc 0.7267
No improvement (6/15).
Epoch [32/100] | train_loss 0.5231 | train_acc 0.7647 | val_loss 0.6008 | val_acc 0.7267
No improvement (7/15).
Epoch [33/100] | train_loss 0.5230 | train_acc 0.7647 | val_loss 0.6009 | val_acc 0.7267
No improvement (8/15).
Epoch [34/100] | train_loss 0.5230 | train_acc 0.7640 | val_loss 0.6008 | val_acc 0.7267
No improvement (9/15).
Epoch [35/100] | train_loss 0.5228 | train_acc 0.7640 | val_loss 0.6008 | val_acc 0.7267
No improvement (10/15).
Epoch [36/100] | train_loss 0.5228 | train_acc 0.7653 | val_loss 0.6008 | val_acc 0.7267
No improvement (11/15).
Epoch [37/100] | train_loss 0.5227 | train_acc 0.7653 | val_loss 0.6008 | val_acc 0.7267
No improvement (12/15).
Epoch [38/100] | train_loss 0.5225 | train_acc 0.7653 | val_loss 0.6008 | val_acc 0.7267
No improvement (13/15).
Epoch [39/100] | train_loss 0.5225 | train_acc 0.7663 | val_loss 0.6008 | val_acc 0.7267
No improvement (14/15).
Epoch [40/100] | train_loss 0.5224 | train_acc 0.7647 | val_loss 0.6008 | val_acc 0.7267
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 3 Finished. Best Val Acc: 73.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7427 | train_acc 0.5183 | val_loss 0.6982 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6930 | train_acc 0.5183 | val_loss 0.6938 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6929 | train_acc 0.5183 | val_loss 0.6941 | val_acc 0.4900
No improvement (3/15).
Epoch [4/100] | train_loss 0.6929 | train_acc 0.5183 | val_loss 0.6940 | val_acc 0.4900
No improvement (4/15).
Epoch [5/100] | train_loss 0.6930 | train_acc 0.5183 | val_loss 0.6940 | val_acc 0.4900
No improvement (5/15).
Epoch [6/100] | train_loss 0.6930 | train_acc 0.5183 | val_loss 0.6939 | val_acc 0.4900
No improvement (6/15).
Epoch [7/100] | train_loss 0.6930 | train_acc 0.5183 | val_loss 0.6939 | val_acc 0.4900
No improvement (7/15).
Epoch [8/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6942 | val_acc 0.4900
No improvement (8/15).
Epoch [9/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6942 | val_acc 0.4900
No improvement (9/15).
Epoch [10/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6942 | val_acc 0.4900
No improvement (10/15).
Epoch [11/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6942 | val_acc 0.4900
No improvement (11/15).
Epoch [12/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6942 | val_acc 0.4900
No improvement (12/15).
Epoch [13/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6942 | val_acc 0.4900
No improvement (13/15).
Epoch [14/100] | train_loss 0.6926 | train_acc 0.5183 | val_loss 0.6943 | val_acc 0.4900
No improvement (14/15).
Epoch [15/100] | train_loss 0.6926 | train_acc 0.5183 | val_loss 0.6943 | val_acc 0.4900
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 4 Finished. Best Val Acc: 49.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7073 | train_acc 0.5000 | val_loss 0.6953 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6931 | train_acc 0.5073 | val_loss 0.6944 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6928 | train_acc 0.5143 | val_loss 0.6937 | val_acc 0.4900
No improvement (3/15).
Epoch [4/100] | train_loss 0.6925 | train_acc 0.5250 | val_loss 0.6926 | val_acc 0.4900
Epoch [5/100] | train_loss 0.6831 | train_acc 0.5437 | val_loss 0.6503 | val_acc 0.6533
No improvement (1/15).
Epoch [6/100] | train_loss 0.6246 | train_acc 0.6227 | val_loss 0.6054 | val_acc 0.5850
Epoch [7/100] | train_loss 0.4733 | train_acc 0.7467 | val_loss 0.3978 | val_acc 0.8417
Epoch [8/100] | train_loss 0.2462 | train_acc 0.9030 | val_loss 0.3216 | val_acc 0.8783
Epoch [9/100] | train_loss 0.2178 | train_acc 0.9157 | val_loss 0.2849 | val_acc 0.8883
Epoch [10/100] | train_loss 0.1812 | train_acc 0.9327 | val_loss 0.2625 | val_acc 0.8933
Epoch [11/100] | train_loss 0.1653 | train_acc 0.9383 | val_loss 0.2602 | val_acc 0.8967
Epoch [12/100] | train_loss 0.1480 | train_acc 0.9470 | val_loss 0.2847 | val_acc 0.9067
Epoch [13/100] | train_loss 0.1367 | train_acc 0.9507 | val_loss 0.2475 | val_acc 0.9183
Epoch [14/100] | train_loss 0.1299 | train_acc 0.9523 | val_loss 0.2238 | val_acc 0.9200
Epoch [15/100] | train_loss 0.1230 | train_acc 0.9573 | val_loss 0.2208 | val_acc 0.9233
Epoch [16/100] | train_loss 0.1205 | train_acc 0.9583 | val_loss 0.2246 | val_acc 0.9283
No improvement (1/15).
Epoch [17/100] | train_loss 0.1159 | train_acc 0.9603 | val_loss 0.2192 | val_acc 0.9283
No improvement (2/15).
Epoch [18/100] | train_loss 0.1141 | train_acc 0.9610 | val_loss 0.2117 | val_acc 0.9283
Epoch [19/100] | train_loss 0.1112 | train_acc 0.9633 | val_loss 0.2096 | val_acc 0.9317
No improvement (1/15).
Epoch [20/100] | train_loss 0.1117 | train_acc 0.9603 | val_loss 0.2113 | val_acc 0.9317
No improvement (2/15).
Epoch [21/100] | train_loss 0.1049 | train_acc 0.9640 | val_loss 0.2085 | val_acc 0.9317
No improvement (3/15).
Epoch [22/100] | train_loss 0.1031 | train_acc 0.9657 | val_loss 0.2082 | val_acc 0.9300
No improvement (4/15).
Epoch [23/100] | train_loss 0.1020 | train_acc 0.9663 | val_loss 0.2105 | val_acc 0.9317
No improvement (5/15).
Epoch [24/100] | train_loss 0.1012 | train_acc 0.9667 | val_loss 0.2118 | val_acc 0.9317
No improvement (6/15).
Epoch [25/100] | train_loss 0.0999 | train_acc 0.9653 | val_loss 0.2143 | val_acc 0.9300
No improvement (7/15).
Epoch [26/100] | train_loss 0.1040 | train_acc 0.9613 | val_loss 0.2165 | val_acc 0.9317
Epoch [27/100] | train_loss 0.1042 | train_acc 0.9607 | val_loss 0.2140 | val_acc 0.9333
Epoch [28/100] | train_loss 0.1035 | train_acc 0.9603 | val_loss 0.2140 | val_acc 0.9350
No improvement (1/15).
Epoch [29/100] | train_loss 0.1024 | train_acc 0.9610 | val_loss 0.2139 | val_acc 0.9350
No improvement (2/15).
Epoch [30/100] | train_loss 0.1021 | train_acc 0.9617 | val_loss 0.2142 | val_acc 0.9333
No improvement (3/15).
Epoch [31/100] | train_loss 0.1011 | train_acc 0.9620 | val_loss 0.2144 | val_acc 0.9333
No improvement (4/15).
Epoch [32/100] | train_loss 0.0978 | train_acc 0.9660 | val_loss 0.2096 | val_acc 0.9300
No improvement (5/15).
Epoch [33/100] | train_loss 0.0946 | train_acc 0.9683 | val_loss 0.2116 | val_acc 0.9300
No improvement (6/15).
Epoch [34/100] | train_loss 0.0943 | train_acc 0.9687 | val_loss 0.2128 | val_acc 0.9300
No improvement (7/15).
Epoch [35/100] | train_loss 0.0938 | train_acc 0.9690 | val_loss 0.2137 | val_acc 0.9300
No improvement (8/15).
Epoch [36/100] | train_loss 0.0937 | train_acc 0.9693 | val_loss 0.2153 | val_acc 0.9300
No improvement (9/15).
Epoch [37/100] | train_loss 0.0935 | train_acc 0.9697 | val_loss 0.2158 | val_acc 0.9300
No improvement (10/15).
Epoch [38/100] | train_loss 0.0930 | train_acc 0.9690 | val_loss 0.2174 | val_acc 0.9283
No improvement (11/15).
Epoch [39/100] | train_loss 0.0929 | train_acc 0.9697 | val_loss 0.2188 | val_acc 0.9283
No improvement (12/15).
Epoch [40/100] | train_loss 0.0924 | train_acc 0.9707 | val_loss 0.2190 | val_acc 0.9283
No improvement (13/15).
Epoch [41/100] | train_loss 0.0925 | train_acc 0.9703 | val_loss 0.2194 | val_acc 0.9283
No improvement (14/15).
Epoch [42/100] | train_loss 0.0923 | train_acc 0.9707 | val_loss 0.2194 | val_acc 0.9300
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 5 Finished. Best Val Acc: 93.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6921 | train_acc 0.5417 | val_loss 0.6789 | val_acc 0.5817
Epoch [2/100] | train_loss 0.6123 | train_acc 0.6380 | val_loss 0.5736 | val_acc 0.6267
Epoch [3/100] | train_loss 0.4651 | train_acc 0.7560 | val_loss 0.4920 | val_acc 0.8050
Epoch [4/100] | train_loss 0.2856 | train_acc 0.8823 | val_loss 0.3042 | val_acc 0.8750wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–†
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.93667
wandb:             epoch 45
wandb:         grad/norm 0.78682
wandb:                lr 1e-05
wandb:         train/acc 0.96967
wandb:        train/loss 0.09134
wandb: training_time_sec 2599.6472
wandb:           val/acc 0.93667
wandb:          val/loss 0.18006
wandb: 
wandb: ğŸš€ View run Trial6_[CV-Variation]_L1_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vgn6khmc
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260201_234425-vgn6khmc/logs
[I 2026-02-02 00:27:46,349] Trial 6 finished with value: 0.9366666666666666 and parameters: {'nhead': 4, 'num_layers': 1, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0013802073382387214, 'dropout': 0.35880609658806817}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run 86icg8b4
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_002746-86icg8b4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial7_[CV-Variation]_L1_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/86icg8b4

No improvement (1/15).
Epoch [5/100] | train_loss 0.2331 | train_acc 0.9063 | val_loss 0.3620 | val_acc 0.8683
Epoch [6/100] | train_loss 0.2238 | train_acc 0.9070 | val_loss 0.2888 | val_acc 0.8883
Epoch [7/100] | train_loss 0.2026 | train_acc 0.9153 | val_loss 0.2497 | val_acc 0.8967
No improvement (1/15).
Epoch [8/100] | train_loss 0.1643 | train_acc 0.9343 | val_loss 0.2863 | val_acc 0.8900
No improvement (2/15).
Epoch [9/100] | train_loss 0.1544 | train_acc 0.9413 | val_loss 0.2860 | val_acc 0.8883
Epoch [10/100] | train_loss 0.1480 | train_acc 0.9437 | val_loss 0.2512 | val_acc 0.9050
No improvement (1/15).
Epoch [11/100] | train_loss 0.1378 | train_acc 0.9477 | val_loss 0.2569 | val_acc 0.9000
Epoch [12/100] | train_loss 0.1368 | train_acc 0.9490 | val_loss 0.2381 | val_acc 0.9100
Epoch [13/100] | train_loss 0.1295 | train_acc 0.9543 | val_loss 0.2337 | val_acc 0.9117
Epoch [14/100] | train_loss 0.1588 | train_acc 0.9370 | val_loss 0.2062 | val_acc 0.9167
No improvement (1/15).
Epoch [15/100] | train_loss 0.1299 | train_acc 0.9547 | val_loss 0.2127 | val_acc 0.9167
No improvement (2/15).
Epoch [16/100] | train_loss 0.1292 | train_acc 0.9527 | val_loss 0.2138 | val_acc 0.9150
No improvement (3/15).
Epoch [17/100] | train_loss 0.1439 | train_acc 0.9437 | val_loss 0.1981 | val_acc 0.9167
Epoch [18/100] | train_loss 0.1232 | train_acc 0.9567 | val_loss 0.2081 | val_acc 0.9183
Epoch [19/100] | train_loss 0.1407 | train_acc 0.9487 | val_loss 0.1985 | val_acc 0.9250
No improvement (1/15).
Epoch [20/100] | train_loss 0.1066 | train_acc 0.9613 | val_loss 0.2016 | val_acc 0.9217
No improvement (2/15).
Epoch [21/100] | train_loss 0.1095 | train_acc 0.9610 | val_loss 0.1995 | val_acc 0.9217
No improvement (3/15).
Epoch [22/100] | train_loss 0.1088 | train_acc 0.9620 | val_loss 0.1997 | val_acc 0.9233
No improvement (4/15).
Epoch [23/100] | train_loss 0.1071 | train_acc 0.9623 | val_loss 0.1989 | val_acc 0.9217
No improvement (5/15).
Epoch [24/100] | train_loss 0.1056 | train_acc 0.9627 | val_loss 0.1974 | val_acc 0.9250
Epoch [25/100] | train_loss 0.1060 | train_acc 0.9640 | val_loss 0.1971 | val_acc 0.9267
Epoch [26/100] | train_loss 0.0999 | train_acc 0.9660 | val_loss 0.1840 | val_acc 0.9333
No improvement (1/15).
Epoch [27/100] | train_loss 0.1014 | train_acc 0.9657 | val_loss 0.1817 | val_acc 0.9333
Epoch [28/100] | train_loss 0.1007 | train_acc 0.9663 | val_loss 0.1809 | val_acc 0.9350
No improvement (1/15).
Epoch [29/100] | train_loss 0.1000 | train_acc 0.9663 | val_loss 0.1808 | val_acc 0.9350
No improvement (2/15).
Epoch [30/100] | train_loss 0.1001 | train_acc 0.9663 | val_loss 0.1797 | val_acc 0.9350
Epoch [31/100] | train_loss 0.0995 | train_acc 0.9667 | val_loss 0.1800 | val_acc 0.9367
No improvement (1/15).
Epoch [32/100] | train_loss 0.0962 | train_acc 0.9687 | val_loss 0.1803 | val_acc 0.9350
No improvement (2/15).
Epoch [33/100] | train_loss 0.0967 | train_acc 0.9677 | val_loss 0.1808 | val_acc 0.9350
No improvement (3/15).
Epoch [34/100] | train_loss 0.0964 | train_acc 0.9680 | val_loss 0.1811 | val_acc 0.9367
No improvement (4/15).
Epoch [35/100] | train_loss 0.0961 | train_acc 0.9677 | val_loss 0.1809 | val_acc 0.9367
No improvement (5/15).
Epoch [36/100] | train_loss 0.0960 | train_acc 0.9680 | val_loss 0.1808 | val_acc 0.9367
No improvement (6/15).
Epoch [37/100] | train_loss 0.0957 | train_acc 0.9677 | val_loss 0.1815 | val_acc 0.9367
No improvement (7/15).
Epoch [38/100] | train_loss 0.0930 | train_acc 0.9683 | val_loss 0.1833 | val_acc 0.9333
No improvement (8/15).
Epoch [39/100] | train_loss 0.0924 | train_acc 0.9683 | val_loss 0.1821 | val_acc 0.9333
No improvement (9/15).
Epoch [40/100] | train_loss 0.0927 | train_acc 0.9680 | val_loss 0.1821 | val_acc 0.9333
No improvement (10/15).
Epoch [41/100] | train_loss 0.0925 | train_acc 0.9680 | val_loss 0.1820 | val_acc 0.9317
No improvement (11/15).
Epoch [42/100] | train_loss 0.0925 | train_acc 0.9690 | val_loss 0.1824 | val_acc 0.9333
No improvement (12/15).
Epoch [43/100] | train_loss 0.0922 | train_acc 0.9697 | val_loss 0.1824 | val_acc 0.9333
No improvement (13/15).
Epoch [44/100] | train_loss 0.0914 | train_acc 0.9687 | val_loss 0.1800 | val_acc 0.9367
No improvement (14/15).
Epoch [45/100] | train_loss 0.0913 | train_acc 0.9697 | val_loss 0.1801 | val_acc 0.9367
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 6 Finished. Best Val Acc: 93.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6916 | train_acc 0.5253 | val_loss 0.6904 | val_acc 0.5417
Epoch [2/100] | train_loss 0.6805 | train_acc 0.5837 | val_loss 0.6744 | val_acc 0.5933
Epoch [3/100] | train_loss 0.6541 | train_acc 0.6183 | val_loss 0.6422 | val_acc 0.6150
No improvement (1/15).
Epoch [4/100] | train_loss 0.6127 | train_acc 0.6423 | val_loss 0.6034 | val_acc 0.6083
Epoch [5/100] | train_loss 0.5693 | train_acc 0.6747 | val_loss 0.5668 | val_acc 0.6650
Epoch [6/100] | train_loss 0.5291 | train_acc 0.7107 | val_loss 0.5331 | val_acc 0.7217
No improvement (1/15).
Epoch [7/100] | train_loss 0.4948 | train_acc 0.7520 | val_loss 0.5554 | val_acc 0.6867
Epoch [8/100] | train_loss 0.4675 | train_acc 0.7713 | val_loss 0.4997 | val_acc 0.7433
Epoch [9/100] | train_loss 0.4454 | train_acc 0.7903 | val_loss 0.4956 | val_acc 0.7617
No improvement (1/15).
Epoch [10/100] | train_loss 0.4314 | train_acc 0.8007 | val_loss 0.4916 | val_acc 0.7617
Epoch [11/100] | train_loss 0.4195 | train_acc 0.8107 | val_loss 0.4851 | val_acc 0.7650
Epoch [12/100] | train_loss 0.4084 | train_acc 0.8130 | val_loss 0.4784 | val_acc 0.7717
No improvement (1/15).
Epoch [13/100] | train_loss 0.3974 | train_acc 0.8197 | val_loss 0.4717 | val_acc 0.7717
No improvement (2/15).
Epoch [14/100] | train_loss 0.3878 | train_acc 0.8247 | val_loss 0.4775 | val_acc 0.7567
No improvement (3/15).
Epoch [15/100] | train_loss 0.3825 | train_acc 0.8267 | val_loss 0.4737 | val_acc 0.7633
No improvement (4/15).
Epoch [16/100] | train_loss 0.3778 | train_acc 0.8277 | val_loss 0.4684 | val_acc 0.7667
Epoch [17/100] | train_loss 0.3736 | train_acc 0.8313 | val_loss 0.4651 | val_acc 0.7750
Epoch [18/100] | train_loss 0.3695 | train_acc 0.8333 | val_loss 0.4614 | val_acc 0.7767
Epoch [19/100] | train_loss 0.3649 | train_acc 0.8360 | val_loss 0.4574 | val_acc 0.7833
Epoch [20/100] | train_loss 0.3600 | train_acc 0.8430 | val_loss 0.4475 | val_acc 0.7883
No improvement (1/15).
Epoch [21/100] | train_loss 0.3592 | train_acc 0.8420 | val_loss 0.4437 | val_acc 0.7850
No improvement (2/15).
Epoch [22/100] | train_loss 0.3564 | train_acc 0.8427 | val_loss 0.4434 | val_acc 0.7833
No improvement (3/15).
Epoch [23/100] | train_loss 0.3549 | train_acc 0.8440 | val_loss 0.4417 | val_acc 0.7833
No improvement (4/15).
Epoch [24/100] | train_loss 0.3528 | train_acc 0.8457 | val_loss 0.4410 | val_acc 0.7867
No improvement (5/15).
Epoch [25/100] | train_loss 0.3509 | train_acc 0.8457 | val_loss 0.4393 | val_acc 0.7883
No improvement (6/15).
Epoch [26/100] | train_loss 0.3475 | train_acc 0.8470 | val_loss 0.4491 | val_acc 0.7800
No improvement (7/15).
Epoch [27/100] | train_loss 0.3473 | train_acc 0.8463 | val_loss 0.4475 | val_acc 0.7833
No improvement (8/15).
Epoch [28/100] | train_loss 0.3462 | train_acc 0.8470 | val_loss 0.4474 | val_acc 0.7850
No improvement (9/15).
Epoch [29/100] | train_loss 0.3452 | train_acc 0.8480 | val_loss 0.4470 | val_acc 0.7850
No improvement (10/15).
Epoch [30/100] | train_loss 0.3442 | train_acc 0.8500 | val_loss 0.4466 | val_acc 0.7833
No improvement (11/15).
Epoch [31/100] | train_loss 0.3427 | train_acc 0.8490 | val_loss 0.4453 | val_acc 0.7850
Epoch [32/100] | train_loss 0.3428 | train_acc 0.8477 | val_loss 0.4390 | val_acc 0.7917
No improvement (1/15).
Epoch [33/100] | train_loss 0.3428 | train_acc 0.8477 | val_loss 0.4370 | val_acc 0.7917
No improvement (2/15).
Epoch [34/100] | train_loss 0.3420 | train_acc 0.8473 | val_loss 0.4368 | val_acc 0.7917
No improvement (3/15).
Epoch [35/100] | train_loss 0.3413 | train_acc 0.8483 | val_loss 0.4364 | val_acc 0.7883
No improvement (4/15).
Epoch [36/100] | train_loss 0.3405 | train_acc 0.8470 | val_loss 0.4360 | val_acc 0.7883
No improvement (5/15).
Epoch [37/100] | train_loss 0.3400 | train_acc 0.8487 | val_loss 0.4359 | val_acc 0.7883wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 90-91
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–‚â–‚â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–ƒâ–„â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–†â–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–ƒâ–ƒâ–„â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.795
wandb:             epoch 52
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.854
wandb:        train/loss 0.33505
wandb: training_time_sec 5591.42621
wandb:           val/acc 0.79333
wandb:          val/loss 0.42924
wandb: 
wandb: ğŸš€ View run Trial7_[CV-Variation]_L1_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/86icg8b4
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_002746-86icg8b4/logs
[I 2026-02-02 02:01:00,189] Trial 7 finished with value: 0.795 and parameters: {'nhead': 8, 'num_layers': 1, 'd_model': 64, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.0003424926555787203, 'dropout': 0.2935223631232897}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run fqc8h4tg
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_020100-fqc8h4tg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial8_[CV-Variation]_L3_H8_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fqc8h4tg
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: ğŸš€ View run Trial8_[CV-Variation]_L3_H8_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fqc8h4tg
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_020100-fqc8h4tg/logs
[I 2026-02-02 02:06:02,216] Trial 8 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 32, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0017769965501024305, 'dropout': 0.20125771066434583}. Best is trial 0 with value: 0.9566666666666667.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_020602-vb7ntgpn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial9_[CV-Variation]_L2_H2_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vb7ntgpn
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 55-56
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–„â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–„â–…â–†â–‡â–‡â–†â–…â–†â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–†â–…â–‚â–ƒâ–„â–…â–„â–…â–â–„â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.91167
wandb:             epoch 33
wandb:         grad/norm 1.0
wandb:                lr 0.00023
wandb:         train/acc 0.953
wandb:        train/loss 0.12278
wandb: training_time_sec 1789.27911
wandb:           val/acc 0.905
wandb:          val/loss 0.27164
wandb: 
wandb: ğŸš€ View run Trial9_[CV-Variation]_L2_H2_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vb7ntgpn
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_020602-vb7ntgpn/logs
[I 2026-02-02 02:35:53,648] Trial 9 finished with value: 0.9116666666666666 and parameters: {'nhead': 2, 'num_layers': 2, 'd_model': 16, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.007404123918099649, 'dropout': 0.41990330373646734}. Best is trial 0 with value: 0.9566666666666667.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_023553-3d505d0h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial10_[CV-Variation]_L4_H8_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3d505d0h
wandb: updating run metadata; uploading console lines 0-0
wandb: uploading config.yaml
wandb: ğŸš€ View run Trial10_[CV-Variation]_L4_H8_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3d505d0h
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_023553-3d505d0h/logs
[I 2026-02-02 02:35:56,145] Trial 10 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 128, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.006044685467029982, 'dropout': 0.04967010830080701}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run glt4j11y
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_023556-glt4j11y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial11_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/glt4j11y

Epoch [38/100] | train_loss 0.3396 | train_acc 0.8470 | val_loss 0.4310 | val_acc 0.7950
No improvement (1/15).
Epoch [39/100] | train_loss 0.3383 | train_acc 0.8517 | val_loss 0.4314 | val_acc 0.7917
No improvement (2/15).
Epoch [40/100] | train_loss 0.3377 | train_acc 0.8517 | val_loss 0.4310 | val_acc 0.7933
No improvement (3/15).
Epoch [41/100] | train_loss 0.3377 | train_acc 0.8500 | val_loss 0.4309 | val_acc 0.7933
No improvement (4/15).
Epoch [42/100] | train_loss 0.3374 | train_acc 0.8517 | val_loss 0.4307 | val_acc 0.7933
No improvement (5/15).
Epoch [43/100] | train_loss 0.3372 | train_acc 0.8510 | val_loss 0.4306 | val_acc 0.7933
No improvement (6/15).
Epoch [44/100] | train_loss 0.3365 | train_acc 0.8530 | val_loss 0.4301 | val_acc 0.7933
No improvement (7/15).
Epoch [45/100] | train_loss 0.3363 | train_acc 0.8530 | val_loss 0.4298 | val_acc 0.7950
No improvement (8/15).
Epoch [46/100] | train_loss 0.3359 | train_acc 0.8540 | val_loss 0.4298 | val_acc 0.7950
No improvement (9/15).
Epoch [47/100] | train_loss 0.3360 | train_acc 0.8520 | val_loss 0.4297 | val_acc 0.7950
No improvement (10/15).
Epoch [48/100] | train_loss 0.3357 | train_acc 0.8530 | val_loss 0.4296 | val_acc 0.7950
No improvement (11/15).
Epoch [49/100] | train_loss 0.3356 | train_acc 0.8530 | val_loss 0.4295 | val_acc 0.7950
No improvement (12/15).
Epoch [50/100] | train_loss 0.3351 | train_acc 0.8537 | val_loss 0.4294 | val_acc 0.7950
No improvement (13/15).
Epoch [51/100] | train_loss 0.3350 | train_acc 0.8530 | val_loss 0.4293 | val_acc 0.7933
No improvement (14/15).
Epoch [52/100] | train_loss 0.3350 | train_acc 0.8540 | val_loss 0.4292 | val_acc 0.7933
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 7 Finished. Best Val Acc: 79.50%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 8 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.22 GiB is free. Including non-PyTorch memory, this process has 378.00 MiB memory in use. Of the allocated memory 219.00 MiB is allocated by PyTorch, and 3.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6922 | train_acc 0.5300 | val_loss 0.6814 | val_acc 0.6067
Epoch [2/100] | train_loss 0.6455 | train_acc 0.6233 | val_loss 0.6238 | val_acc 0.6367
Epoch [3/100] | train_loss 0.5454 | train_acc 0.6957 | val_loss 0.5046 | val_acc 0.7200
Epoch [4/100] | train_loss 0.4128 | train_acc 0.8140 | val_loss 0.5833 | val_acc 0.7867
Epoch [5/100] | train_loss 0.3291 | train_acc 0.8573 | val_loss 0.4772 | val_acc 0.8117
Epoch [6/100] | train_loss 0.3110 | train_acc 0.8630 | val_loss 0.3494 | val_acc 0.8467
Epoch [7/100] | train_loss 0.2714 | train_acc 0.8843 | val_loss 0.3831 | val_acc 0.8583
No improvement (1/15).
Epoch [8/100] | train_loss 0.2412 | train_acc 0.9070 | val_loss 0.4409 | val_acc 0.8133
No improvement (2/15).
Epoch [9/100] | train_loss 0.2295 | train_acc 0.9137 | val_loss 0.4757 | val_acc 0.8017
No improvement (3/15).
Epoch [10/100] | train_loss 0.2132 | train_acc 0.9160 | val_loss 0.4466 | val_acc 0.8217
No improvement (4/15).
Epoch [11/100] | train_loss 0.2221 | train_acc 0.9220 | val_loss 0.4864 | val_acc 0.8433
Epoch [12/100] | train_loss 0.2072 | train_acc 0.9110 | val_loss 0.2825 | val_acc 0.8900
No improvement (1/15).
Epoch [13/100] | train_loss 0.1969 | train_acc 0.9243 | val_loss 0.4360 | val_acc 0.8617
Epoch [14/100] | train_loss 0.1810 | train_acc 0.9267 | val_loss 0.2760 | val_acc 0.9033
Epoch [15/100] | train_loss 0.1631 | train_acc 0.9337 | val_loss 0.2955 | val_acc 0.9050
Epoch [16/100] | train_loss 0.1619 | train_acc 0.9347 | val_loss 0.2785 | val_acc 0.9100
No improvement (1/15).
Epoch [17/100] | train_loss 0.1597 | train_acc 0.9367 | val_loss 0.2918 | val_acc 0.9050
No improvement (2/15).
Epoch [18/100] | train_loss 0.1567 | train_acc 0.9363 | val_loss 0.2931 | val_acc 0.9050
Epoch [19/100] | train_loss 0.1516 | train_acc 0.9380 | val_loss 0.2862 | val_acc 0.9117
No improvement (1/15).
Epoch [20/100] | train_loss 0.1412 | train_acc 0.9413 | val_loss 0.3798 | val_acc 0.8733
No improvement (2/15).
Epoch [21/100] | train_loss 0.1423 | train_acc 0.9447 | val_loss 0.3655 | val_acc 0.8800
No improvement (3/15).
Epoch [22/100] | train_loss 0.1436 | train_acc 0.9413 | val_loss 0.3640 | val_acc 0.8867
No improvement (4/15).
Epoch [23/100] | train_loss 0.1447 | train_acc 0.9403 | val_loss 0.3453 | val_acc 0.8833
No improvement (5/15).
Epoch [24/100] | train_loss 0.1441 | train_acc 0.9423 | val_loss 0.3360 | val_acc 0.8917
No improvement (6/15).
Epoch [25/100] | train_loss 0.1392 | train_acc 0.9437 | val_loss 0.3303 | val_acc 0.8983
No improvement (7/15).
Epoch [26/100] | train_loss 0.1282 | train_acc 0.9500 | val_loss 0.2819 | val_acc 0.9033
No improvement (8/15).
Epoch [27/100] | train_loss 0.1284 | train_acc 0.9497 | val_loss 0.2984 | val_acc 0.9017
No improvement (9/15).
Epoch [28/100] | train_loss 0.1278 | train_acc 0.9483 | val_loss 0.2899 | val_acc 0.9083
No improvement (10/15).
Epoch [29/100] | train_loss 0.1274 | train_acc 0.9493 | val_loss 0.2891 | val_acc 0.9067
No improvement (11/15).
Epoch [30/100] | train_loss 0.1264 | train_acc 0.9507 | val_loss 0.2915 | val_acc 0.9083
No improvement (12/15).
Epoch [31/100] | train_loss 0.1273 | train_acc 0.9493 | val_loss 0.2890 | val_acc 0.9083
No improvement (13/15).
Epoch [32/100] | train_loss 0.1285 | train_acc 0.9490 | val_loss 0.2670 | val_acc 0.9067
No improvement (14/15).
Epoch [33/100] | train_loss 0.1228 | train_acc 0.9530 | val_loss 0.2716 | val_acc 0.9050
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 9 Finished. Best Val Acc: 91.17%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 10 Failed: CUDA out of memory. Tried to allocate 626.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 205.81 MiB is free. Including non-PyTorch memory, this process has 7.39 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 101.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6960 | train_acc 0.5453 | val_loss 0.6775 | val_acc 0.6017
No improvement (1/15).
Epoch [2/100] | train_loss 0.6260 | train_acc 0.6337 | val_loss 0.7328 | val_acc 0.5900
Epoch [3/100] | train_loss 0.4117 | train_acc 0.7977 | val_loss 0.6186 | val_acc 0.8083
Epoch [4/100] | train_loss 0.2735 | train_acc 0.8857 | val_loss 0.2734 | val_acc 0.8800
Epoch [5/100] | train_loss 0.1867 | train_acc 0.9323 | val_loss 0.2037 | val_acc 0.9283
Epoch [6/100] | train_loss 0.1543 | train_acc 0.9467 | val_loss 0.2029 | val_acc 0.9300
No improvement (1/15).
Epoch [7/100] | train_loss 0.1556 | train_acc 0.9420 | val_loss 0.4008 | val_acc 0.8750
No improvement (2/15).
Epoch [8/100] | train_loss 0.2100 | train_acc 0.9243 | val_loss 0.3225 | val_acc 0.9017
Epoch [9/100] | train_loss 0.1053 | train_acc 0.9630 | val_loss 0.2013 | val_acc 0.9433
Epoch [10/100] | train_loss 0.1265 | train_acc 0.9517 | val_loss 0.1620 | val_acc 0.9500
No improvement (1/15).
Epoch [11/100] | train_loss 0.1081 | train_acc 0.9607 | val_loss 0.2421 | val_acc 0.9433
Epoch [12/100] | train_loss 0.0937 | train_acc 0.9633 | val_loss 0.1999 | val_acc 0.9517
No improvement (1/15).
Epoch [13/100] | train_loss 0.1071 | train_acc 0.9593 | val_loss 0.2572 | val_acc 0.9183
No improvement (2/15).
Epoch [14/100] | train_loss 0.0900 | train_acc 0.9640 | val_loss 0.3135 | val_acc 0.8983
Epoch [15/100] | train_loss 0.0969 | train_acc 0.9610 | val_loss 0.1857 | val_acc 0.9533
Epoch [16/100] | train_loss 0.0803 | train_acc 0.9697 | val_loss 0.1711 | val_acc 0.9567
No improvement (1/15).
Epoch [17/100] | train_loss 0.0845 | train_acc 0.9667 | val_loss 0.1782 | val_acc 0.9533wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 51-52
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–…â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–…â–‡â–‡â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–‡â–ˆâ–‡â–‚â–‚â–‚â–„â–ƒâ–â–â–‚â–â–‚â–ƒâ–â–â–â–â–â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95667
wandb:             epoch 30
wandb:         grad/norm 0.91947
wandb:                lr 6e-05
wandb:         train/acc 0.97567
wandb:        train/loss 0.0677
wandb: training_time_sec 7003.74554
wandb:           val/acc 0.95
wandb:          val/loss 0.21554
wandb: 
wandb: ğŸš€ View run Trial11_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/glt4j11y
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_023556-glt4j11y/logs
[I 2026-02-02 04:32:42,178] Trial 11 finished with value: 0.9566666666666667 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0009776406966436517, 'dropout': 0.4983173769722363}. Best is trial 0 with value: 0.9566666666666667.
wandb: setting up run 19claatm
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_043242-19claatm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial12_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/19claatm
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–„â–…â–…â–…
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–‡â–…â–…â–ƒâ–ƒâ–‚â–‚â–â–ƒâ–â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 29
wandb:         grad/norm 0.56209
wandb:                lr 4e-05
wandb:         train/acc 0.97
wandb:        train/loss 0.08352
wandb: training_time_sec 12627.87771
wandb:           val/acc 0.95
wandb:          val/loss 0.19065
wandb: 
wandb: ğŸš€ View run Trial12_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/19claatm
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_043242-19claatm/logs
[I 2026-02-02 08:03:12,039] Trial 12 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.000638542482663887, 'dropout': 0.4893107308510497}. Best is trial 12 with value: 0.96.
wandb: setting up run m3897kro
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_080312-m3897kro
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial13_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/m3897kro

No improvement (2/15).
Epoch [18/100] | train_loss 0.0835 | train_acc 0.9693 | val_loss 0.1938 | val_acc 0.9467
No improvement (3/15).
Epoch [19/100] | train_loss 0.0918 | train_acc 0.9657 | val_loss 0.1938 | val_acc 0.9417
No improvement (4/15).
Epoch [20/100] | train_loss 0.1272 | train_acc 0.9473 | val_loss 0.2055 | val_acc 0.9383
No improvement (5/15).
Epoch [21/100] | train_loss 0.0709 | train_acc 0.9730 | val_loss 0.2864 | val_acc 0.9183
No improvement (6/15).
Epoch [22/100] | train_loss 0.0869 | train_acc 0.9683 | val_loss 0.2963 | val_acc 0.9167
No improvement (7/15).
Epoch [23/100] | train_loss 0.0837 | train_acc 0.9667 | val_loss 0.2493 | val_acc 0.9333
No improvement (8/15).
Epoch [24/100] | train_loss 0.0850 | train_acc 0.9673 | val_loss 0.2369 | val_acc 0.9400
No improvement (9/15).
Epoch [25/100] | train_loss 0.0843 | train_acc 0.9667 | val_loss 0.2485 | val_acc 0.9383
No improvement (10/15).
Epoch [26/100] | train_loss 0.0818 | train_acc 0.9667 | val_loss 0.2454 | val_acc 0.9383
No improvement (11/15).
Epoch [27/100] | train_loss 0.0664 | train_acc 0.9737 | val_loss 0.1986 | val_acc 0.9517
No improvement (12/15).
Epoch [28/100] | train_loss 0.0708 | train_acc 0.9763 | val_loss 0.1893 | val_acc 0.9533
No improvement (13/15).
Epoch [29/100] | train_loss 0.0677 | train_acc 0.9753 | val_loss 0.2051 | val_acc 0.9500
No improvement (14/15).
Epoch [30/100] | train_loss 0.0677 | train_acc 0.9757 | val_loss 0.2155 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 11 Finished. Best Val Acc: 95.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7080 | train_acc 0.5190 | val_loss 0.7016 | val_acc 0.5450
Epoch [2/100] | train_loss 0.6631 | train_acc 0.6190 | val_loss 0.6738 | val_acc 0.6017
Epoch [3/100] | train_loss 0.5484 | train_acc 0.6947 | val_loss 0.5976 | val_acc 0.7433
Epoch [4/100] | train_loss 0.3505 | train_acc 0.8393 | val_loss 0.4452 | val_acc 0.8500
Epoch [5/100] | train_loss 0.2209 | train_acc 0.9123 | val_loss 0.4543 | val_acc 0.8817
Epoch [6/100] | train_loss 0.1991 | train_acc 0.9290 | val_loss 0.2817 | val_acc 0.9033
No improvement (1/15).
Epoch [7/100] | train_loss 0.1625 | train_acc 0.9383 | val_loss 0.2968 | val_acc 0.9017
Epoch [8/100] | train_loss 0.1459 | train_acc 0.9423 | val_loss 0.2350 | val_acc 0.9350
Epoch [9/100] | train_loss 0.1318 | train_acc 0.9480 | val_loss 0.2147 | val_acc 0.9433
No improvement (1/15).
Epoch [10/100] | train_loss 0.1281 | train_acc 0.9527 | val_loss 0.1893 | val_acc 0.9417
No improvement (2/15).
Epoch [11/100] | train_loss 0.1304 | train_acc 0.9507 | val_loss 0.2874 | val_acc 0.9117
Epoch [12/100] | train_loss 0.1090 | train_acc 0.9590 | val_loss 0.1561 | val_acc 0.9583
No improvement (1/15).
Epoch [13/100] | train_loss 0.1145 | train_acc 0.9557 | val_loss 0.1521 | val_acc 0.9467
No improvement (2/15).
Epoch [14/100] | train_loss 0.1089 | train_acc 0.9573 | val_loss 0.1676 | val_acc 0.9500
Epoch [15/100] | train_loss 0.1010 | train_acc 0.9613 | val_loss 0.1741 | val_acc 0.9600
No improvement (1/15).
Epoch [16/100] | train_loss 0.1013 | train_acc 0.9617 | val_loss 0.1873 | val_acc 0.9483
No improvement (2/15).
Epoch [17/100] | train_loss 0.0919 | train_acc 0.9673 | val_loss 0.1968 | val_acc 0.9483
No improvement (3/15).
Epoch [18/100] | train_loss 0.0969 | train_acc 0.9653 | val_loss 0.1751 | val_acc 0.9550
No improvement (4/15).
Epoch [19/100] | train_loss 0.1049 | train_acc 0.9613 | val_loss 0.1804 | val_acc 0.9483
No improvement (5/15).
Epoch [20/100] | train_loss 0.0938 | train_acc 0.9663 | val_loss 0.2580 | val_acc 0.9317
No improvement (6/15).
Epoch [21/100] | train_loss 0.1172 | train_acc 0.9527 | val_loss 0.1958 | val_acc 0.9483
No improvement (7/15).
Epoch [22/100] | train_loss 0.1037 | train_acc 0.9593 | val_loss 0.2310 | val_acc 0.9383
No improvement (8/15).
Epoch [23/100] | train_loss 0.1042 | train_acc 0.9597 | val_loss 0.2597 | val_acc 0.9350
No improvement (9/15).
Epoch [24/100] | train_loss 0.1099 | train_acc 0.9580 | val_loss 0.1949 | val_acc 0.9500
No improvement (10/15).
Epoch [25/100] | train_loss 0.0990 | train_acc 0.9620 | val_loss 0.2695 | val_acc 0.9350
No improvement (11/15).
Epoch [26/100] | train_loss 0.0947 | train_acc 0.9650 | val_loss 0.1874 | val_acc 0.9500
No improvement (12/15).
Epoch [27/100] | train_loss 0.0831 | train_acc 0.9693 | val_loss 0.1822 | val_acc 0.9517
No improvement (13/15).
Epoch [28/100] | train_loss 0.0842 | train_acc 0.9693 | val_loss 0.1884 | val_acc 0.9500
No improvement (14/15).
Epoch [29/100] | train_loss 0.0835 | train_acc 0.9700 | val_loss 0.1906 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 12 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6895 | train_acc 0.5673 | val_loss 0.7035 | val_acc 0.5450
Epoch [2/100] | train_loss 0.6653 | train_acc 0.6077 | val_loss 0.6620 | val_acc 0.6267
Epoch [3/100] | train_loss 0.6305 | train_acc 0.6463 | val_loss 0.5920 | val_acc 0.6700
Epoch [4/100] | train_loss 0.5131 | train_acc 0.7417 | val_loss 0.4910 | val_acc 0.7717
Epoch [5/100] | train_loss 0.3014 | train_acc 0.8707 | val_loss 0.6909 | val_acc 0.7900
Epoch [6/100] | train_loss 0.2327 | train_acc 0.9093 | val_loss 0.3868 | val_acc 0.8867
No improvement (1/15).
Epoch [7/100] | train_loss 0.1853 | train_acc 0.9290 | val_loss 0.4435 | val_acc 0.8750
Epoch [8/100] | train_loss 0.1662 | train_acc 0.9360 | val_loss 0.2231 | val_acc 0.9333
Epoch [9/100] | train_loss 0.1342 | train_acc 0.9510 | val_loss 0.2184 | val_acc 0.9350
Epoch [10/100] | train_loss 0.1318 | train_acc 0.9517 | val_loss 0.2152 | val_acc 0.9400
Epoch [11/100] | train_loss 0.1415 | train_acc 0.9437 | val_loss 0.1862 | val_acc 0.9417
Epoch [12/100] | train_loss 0.1130 | train_acc 0.9587 | val_loss 0.1918 | val_acc 0.9467
No improvement (1/15).
Epoch [13/100] | train_loss 0.1232 | train_acc 0.9543 | val_loss 0.1870 | val_acc 0.9450
Epoch [14/100] | train_loss 0.1093 | train_acc 0.9557 | val_loss 0.1866 | val_acc 0.9500
No improvement (1/15).
Epoch [15/100] | train_loss 0.1116 | train_acc 0.9580 | val_loss 0.2294 | val_acc 0.9333
No improvement (2/15).
Epoch [16/100] | train_loss 0.1017 | train_acc 0.9633 | val_loss 0.2379 | val_acc 0.9367
No improvement (3/15).
Epoch [17/100] | train_loss 0.1013 | train_acc 0.9663 | val_loss 0.2429 | val_acc 0.9350
No improvement (4/15).
Epoch [18/100] | train_loss 0.1006 | train_acc 0.9653 | val_loss 0.1964 | val_acc 0.9433
Epoch [19/100] | train_loss 0.1044 | train_acc 0.9607 | val_loss 0.1883 | val_acc 0.9517
No improvement (1/15).
Epoch [20/100] | train_loss 0.0942 | train_acc 0.9653 | val_loss 0.2065 | val_acc 0.9433
No improvement (2/15).
Epoch [21/100] | train_loss 0.1017 | train_acc 0.9597 | val_loss 0.2123 | val_acc 0.9400
No improvement (3/15).
Epoch [22/100] | train_loss 0.1011 | train_acc 0.9637 | val_loss 0.2003 | val_acc 0.9433
No improvement (4/15).
Epoch [23/100] | train_loss 0.1007 | train_acc 0.9623 | val_loss 0.2120 | val_acc 0.9400
No improvement (5/15).
Epoch [24/100] | train_loss 0.1011 | train_acc 0.9627 | val_loss 0.1971 | val_acc 0.9450
No improvement (6/15).
Epoch [25/100] | train_loss 0.0974 | train_acc 0.9620 | val_loss 0.2209 | val_acc 0.9367
No improvement (7/15).
Epoch [26/100] | train_loss 0.0964 | train_acc 0.9627 | val_loss 0.1827 | val_acc 0.9500
No improvement (8/15).
Epoch [27/100] | train_loss 0.0893 | train_acc 0.9683 | val_loss 0.1831 | val_acc 0.9483
No improvement (9/15).
Epoch [28/100] | train_loss 0.0899 | train_acc 0.9650 | val_loss 0.1829 | val_acc 0.9517
Epoch [29/100] | train_loss 0.0898 | train_acc 0.9667 | val_loss 0.1807 | val_acc 0.9550
No improvement (1/15).
Epoch [30/100] | train_loss 0.0887 | train_acc 0.9670 | val_loss 0.1807 | val_acc 0.9550
No improvement (2/15).
Epoch [31/100] | train_loss 0.0889 | train_acc 0.9673 | val_loss 0.1808 | val_acc 0.9550
No improvement (3/15).
Epoch [32/100] | train_loss 0.0879 | train_acc 0.9680 | val_loss 0.1867 | val_acc 0.9517
No improvement (4/15).
Epoch [33/100] | train_loss 0.0877 | train_acc 0.9690 | val_loss 0.1859 | val_acc 0.9517
No improvement (5/15).
Epoch [34/100] | train_loss 0.0876 | train_acc 0.9677 | val_loss 0.1857 | val_acc 0.9517wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–‚â–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–ƒâ–…â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–‡â–…â–ˆâ–„â–…â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.955
wandb:             epoch 43
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.968
wandb:        train/loss 0.08566
wandb: training_time_sec 18498.91331
wandb:           val/acc 0.95167
wandb:          val/loss 0.18468
wandb: 
wandb: ğŸš€ View run Trial13_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/m3897kro
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_080312-m3897kro/logs
[I 2026-02-02 13:11:32,994] Trial 13 finished with value: 0.955 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.000402978551112272, 'dropout': 0.49070221227678645}. Best is trial 12 with value: 0.96.
wandb: setting up run 7tovvtrl
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_131133-7tovvtrl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial14_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7tovvtrl
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–†â–ƒâ–„â–†â–‡â–‚â–„â–„â–‚â–â–„â–‚â–‚â–â–‚â–â–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ƒâ–…â–ƒâ–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.955
wandb:             epoch 37
wandb:         grad/norm 0.33429
wandb:                lr 4e-05
wandb:         train/acc 0.98933
wandb:        train/loss 0.03551
wandb: training_time_sec 15996.20854
wandb:           val/acc 0.95
wandb:          val/loss 0.26751
wandb: 
wandb: ğŸš€ View run Trial14_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7tovvtrl
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_131133-7tovvtrl/logs
[I 2026-02-02 17:38:11,169] Trial 14 finished with value: 0.955 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0024934881604754366, 'dropout': 0.42849469001803464}. Best is trial 12 with value: 0.96.
wandb: setting up run hxemdwoi
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_173811-hxemdwoi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial15_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hxemdwoi

No improvement (6/15).
Epoch [35/100] | train_loss 0.0876 | train_acc 0.9680 | val_loss 0.1861 | val_acc 0.9517
No improvement (7/15).
Epoch [36/100] | train_loss 0.0875 | train_acc 0.9677 | val_loss 0.1870 | val_acc 0.9517
No improvement (8/15).
Epoch [37/100] | train_loss 0.0871 | train_acc 0.9677 | val_loss 0.1857 | val_acc 0.9500
No improvement (9/15).
Epoch [38/100] | train_loss 0.0862 | train_acc 0.9693 | val_loss 0.1844 | val_acc 0.9533
No improvement (10/15).
Epoch [39/100] | train_loss 0.0868 | train_acc 0.9687 | val_loss 0.1844 | val_acc 0.9517
No improvement (11/15).
Epoch [40/100] | train_loss 0.0855 | train_acc 0.9683 | val_loss 0.1847 | val_acc 0.9500
No improvement (12/15).
Epoch [41/100] | train_loss 0.0852 | train_acc 0.9693 | val_loss 0.1845 | val_acc 0.9517
No improvement (13/15).
Epoch [42/100] | train_loss 0.0864 | train_acc 0.9680 | val_loss 0.1847 | val_acc 0.9517
No improvement (14/15).
Epoch [43/100] | train_loss 0.0857 | train_acc 0.9680 | val_loss 0.1847 | val_acc 0.9517
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 13 Finished. Best Val Acc: 95.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7996 | train_acc 0.5310 | val_loss 0.6447 | val_acc 0.6017
Epoch [2/100] | train_loss 0.4823 | train_acc 0.7500 | val_loss 0.3046 | val_acc 0.8733
No improvement (1/15).
Epoch [3/100] | train_loss 0.2741 | train_acc 0.8913 | val_loss 0.4260 | val_acc 0.8550
Epoch [4/100] | train_loss 0.2326 | train_acc 0.8960 | val_loss 0.2716 | val_acc 0.9000
Epoch [5/100] | train_loss 0.2036 | train_acc 0.9147 | val_loss 0.1805 | val_acc 0.9317
Epoch [6/100] | train_loss 0.1781 | train_acc 0.9297 | val_loss 0.1893 | val_acc 0.9417
No improvement (1/15).
Epoch [7/100] | train_loss 0.1382 | train_acc 0.9510 | val_loss 0.2564 | val_acc 0.8967
No improvement (2/15).
Epoch [8/100] | train_loss 0.1296 | train_acc 0.9513 | val_loss 0.1615 | val_acc 0.9400
No improvement (3/15).
Epoch [9/100] | train_loss 0.1192 | train_acc 0.9530 | val_loss 0.2152 | val_acc 0.9217
No improvement (4/15).
Epoch [10/100] | train_loss 0.0989 | train_acc 0.9623 | val_loss 0.1633 | val_acc 0.9400
Epoch [11/100] | train_loss 0.1016 | train_acc 0.9637 | val_loss 0.1727 | val_acc 0.9433
No improvement (1/15).
Epoch [12/100] | train_loss 0.0891 | train_acc 0.9663 | val_loss 0.2306 | val_acc 0.9250
No improvement (2/15).
Epoch [13/100] | train_loss 0.0917 | train_acc 0.9640 | val_loss 0.2076 | val_acc 0.9367
No improvement (3/15).
Epoch [14/100] | train_loss 0.0846 | train_acc 0.9697 | val_loss 0.2633 | val_acc 0.9200
No improvement (4/15).
Epoch [15/100] | train_loss 0.0815 | train_acc 0.9693 | val_loss 0.2398 | val_acc 0.9267
No improvement (5/15).
Epoch [16/100] | train_loss 0.0724 | train_acc 0.9737 | val_loss 0.2212 | val_acc 0.9333
No improvement (6/15).
Epoch [17/100] | train_loss 0.0906 | train_acc 0.9667 | val_loss 0.2812 | val_acc 0.9200
No improvement (7/15).
Epoch [18/100] | train_loss 0.0926 | train_acc 0.9637 | val_loss 0.2343 | val_acc 0.9250
No improvement (8/15).
Epoch [19/100] | train_loss 0.0741 | train_acc 0.9753 | val_loss 0.2290 | val_acc 0.9283
Epoch [20/100] | train_loss 0.0592 | train_acc 0.9813 | val_loss 0.1885 | val_acc 0.9500
No improvement (1/15).
Epoch [21/100] | train_loss 0.0545 | train_acc 0.9850 | val_loss 0.2057 | val_acc 0.9500
Epoch [22/100] | train_loss 0.0552 | train_acc 0.9833 | val_loss 0.1882 | val_acc 0.9517
Epoch [23/100] | train_loss 0.0502 | train_acc 0.9847 | val_loss 0.2009 | val_acc 0.9550
No improvement (1/15).
Epoch [24/100] | train_loss 0.0490 | train_acc 0.9843 | val_loss 0.2087 | val_acc 0.9533
No improvement (2/15).
Epoch [25/100] | train_loss 0.0484 | train_acc 0.9847 | val_loss 0.2087 | val_acc 0.9517
No improvement (3/15).
Epoch [26/100] | train_loss 0.0446 | train_acc 0.9867 | val_loss 0.2353 | val_acc 0.9450
No improvement (4/15).
Epoch [27/100] | train_loss 0.0557 | train_acc 0.9827 | val_loss 0.2427 | val_acc 0.9400
No improvement (5/15).
Epoch [28/100] | train_loss 0.0564 | train_acc 0.9813 | val_loss 0.2191 | val_acc 0.9450
No improvement (6/15).
Epoch [29/100] | train_loss 0.0468 | train_acc 0.9843 | val_loss 0.2372 | val_acc 0.9433
No improvement (7/15).
Epoch [30/100] | train_loss 0.0499 | train_acc 0.9820 | val_loss 0.2272 | val_acc 0.9450
No improvement (8/15).
Epoch [31/100] | train_loss 0.0431 | train_acc 0.9853 | val_loss 0.2646 | val_acc 0.9433
No improvement (9/15).
Epoch [32/100] | train_loss 0.0464 | train_acc 0.9853 | val_loss 0.2259 | val_acc 0.9517
No improvement (10/15).
Epoch [33/100] | train_loss 0.0364 | train_acc 0.9887 | val_loss 0.2371 | val_acc 0.9517
No improvement (11/15).
Epoch [34/100] | train_loss 0.0381 | train_acc 0.9883 | val_loss 0.2472 | val_acc 0.9517
No improvement (12/15).
Epoch [35/100] | train_loss 0.0338 | train_acc 0.9903 | val_loss 0.2573 | val_acc 0.9533
No improvement (13/15).
Epoch [36/100] | train_loss 0.0334 | train_acc 0.9900 | val_loss 0.2664 | val_acc 0.9500
No improvement (14/15).
Epoch [37/100] | train_loss 0.0355 | train_acc 0.9893 | val_loss 0.2675 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 14 Finished. Best Val Acc: 95.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6918 | train_acc 0.5560 | val_loss 0.6876 | val_acc 0.5850
Epoch [2/100] | train_loss 0.6536 | train_acc 0.6283 | val_loss 0.6531 | val_acc 0.6300
Epoch [3/100] | train_loss 0.6294 | train_acc 0.6503 | val_loss 0.6006 | val_acc 0.6633
Epoch [4/100] | train_loss 0.5244 | train_acc 0.7407 | val_loss 0.5406 | val_acc 0.7717
Epoch [5/100] | train_loss 0.4058 | train_acc 0.8200 | val_loss 0.4315 | val_acc 0.7917
Epoch [6/100] | train_loss 0.3609 | train_acc 0.8417 | val_loss 0.4564 | val_acc 0.7967
Epoch [7/100] | train_loss 0.3155 | train_acc 0.8670 | val_loss 0.4054 | val_acc 0.8150
Epoch [8/100] | train_loss 0.2648 | train_acc 0.8883 | val_loss 0.3398 | val_acc 0.8717
Epoch [9/100] | train_loss 0.2218 | train_acc 0.9157 | val_loss 0.3210 | val_acc 0.8950
Epoch [10/100] | train_loss 0.1957 | train_acc 0.9220 | val_loss 0.3070 | val_acc 0.9017
Epoch [11/100] | train_loss 0.1696 | train_acc 0.9377 | val_loss 0.3376 | val_acc 0.9033
No improvement (1/15).
Epoch [12/100] | train_loss 0.1666 | train_acc 0.9433 | val_loss 0.3951 | val_acc 0.9033
Epoch [13/100] | train_loss 0.1569 | train_acc 0.9470 | val_loss 0.3164 | val_acc 0.9233
No improvement (1/15).
Epoch [14/100] | train_loss 0.1350 | train_acc 0.9520 | val_loss 0.4341 | val_acc 0.9000
No improvement (2/15).
Epoch [15/100] | train_loss 0.1279 | train_acc 0.9570 | val_loss 0.4231 | val_acc 0.9083
No improvement (3/15).
Epoch [16/100] | train_loss 0.1313 | train_acc 0.9520 | val_loss 0.3896 | val_acc 0.9133
No improvement (4/15).
Epoch [17/100] | train_loss 0.1229 | train_acc 0.9567 | val_loss 0.4089 | val_acc 0.9133
No improvement (5/15).
Epoch [18/100] | train_loss 0.1304 | train_acc 0.9537 | val_loss 0.3918 | val_acc 0.9133
No improvement (6/15).
Epoch [19/100] | train_loss 0.1291 | train_acc 0.9560 | val_loss 0.3624 | val_acc 0.9217
Epoch [20/100] | train_loss 0.1194 | train_acc 0.9563 | val_loss 0.3764 | val_acc 0.9250
Epoch [21/100] | train_loss 0.1123 | train_acc 0.9623 | val_loss 0.3908 | val_acc 0.9267
Epoch [22/100] | train_loss 0.1086 | train_acc 0.9620 | val_loss 0.3533 | val_acc 0.9300
No improvement (1/15).
Epoch [23/100] | train_loss 0.1092 | train_acc 0.9630 | val_loss 0.3904 | val_acc 0.9250
No improvement (2/15).
Epoch [24/100] | train_loss 0.1116 | train_acc 0.9627 | val_loss 0.3956 | val_acc 0.9250
No improvement (3/15).
Epoch [25/100] | train_loss 0.1079 | train_acc 0.9610 | val_loss 0.3822 | val_acc 0.9267
No improvement (4/15).
Epoch [26/100] | train_loss 0.1024 | train_acc 0.9637 | val_loss 0.4098 | val_acc 0.9283
No improvement (5/15).
Epoch [27/100] | train_loss 0.1021 | train_acc 0.9653 | val_loss 0.4105 | val_acc 0.9250
No improvement (6/15).
Epoch [28/100] | train_loss 0.1005 | train_acc 0.9670 | val_loss 0.4119 | val_acc 0.9267
No improvement (7/15).
Epoch [29/100] | train_loss 0.1006 | train_acc 0.9657 | val_loss 0.4073 | val_acc 0.9267
No improvement (8/15).
Epoch [30/100] | train_loss 0.0997 | train_acc 0.9653 | val_loss 0.4097 | val_acc 0.9267wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–„â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–†â–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–ƒâ–…â–…â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–†â–…â–ƒâ–„â–ƒâ–‚â–â–â–‚â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.93
wandb:             epoch 36
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.96967
wandb:        train/loss 0.0954
wandb: training_time_sec 11651.38263
wandb:           val/acc 0.92667
wandb:          val/loss 0.42419
wandb: 
wandb: ğŸš€ View run Trial15_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hxemdwoi
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_173811-hxemdwoi/logs
[I 2026-02-02 20:52:24,582] Trial 15 finished with value: 0.93 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0004534707964034812, 'dropout': 0.4572725864780352}. Best is trial 12 with value: 0.96.
wandb: setting up run bn1usror
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_205224-bn1usror
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial16_[CV-Variation]_L4_H8_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/bn1usror
wandb: updating run metadata; uploading console lines 0-0
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 1-1
wandb: ğŸš€ View run Trial16_[CV-Variation]_L4_H8_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/bn1usror
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_205224-bn1usror/logs
[I 2026-02-02 20:52:27,038] Trial 16 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 128, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0037787344149597846, 'dropout': 0.09804151562344823}. Best is trial 12 with value: 0.96.
wandb: setting up run 8g8sym6i
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_205227-8g8sym6i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial17_[CV-Variation]_L4_H2_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8g8sym6i
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–†â–…â–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–„â–‡â–‡â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94333
wandb:             epoch 60
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.97133
wandb:        train/loss 0.08709
wandb: training_time_sec 6894.22427
wandb:           val/acc 0.94333
wandb:          val/loss 0.18314
wandb: 
wandb: ğŸš€ View run Trial17_[CV-Variation]_L4_H2_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8g8sym6i
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_205227-8g8sym6i/logs
[I 2026-02-02 22:47:23,213] Trial 17 finished with value: 0.9433333333333334 and parameters: {'nhead': 2, 'num_layers': 4, 'd_model': 32, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0009377711874937875, 'dropout': 0.3910302889641689}. Best is trial 12 with value: 0.96.
wandb: setting up run zclcvsod
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_224723-zclcvsod
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial18_[CV-Variation]_L4_H8_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zclcvsod

No improvement (9/15).
Epoch [31/100] | train_loss 0.0993 | train_acc 0.9660 | val_loss 0.4131 | val_acc 0.9267
No improvement (10/15).
Epoch [32/100] | train_loss 0.1002 | train_acc 0.9647 | val_loss 0.4304 | val_acc 0.9250
No improvement (11/15).
Epoch [33/100] | train_loss 0.0983 | train_acc 0.9680 | val_loss 0.4219 | val_acc 0.9283
No improvement (12/15).
Epoch [34/100] | train_loss 0.0987 | train_acc 0.9660 | val_loss 0.4252 | val_acc 0.9283
No improvement (13/15).
Epoch [35/100] | train_loss 0.0977 | train_acc 0.9693 | val_loss 0.4312 | val_acc 0.9267
No improvement (14/15).
Epoch [36/100] | train_loss 0.0954 | train_acc 0.9697 | val_loss 0.4242 | val_acc 0.9267
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 15 Finished. Best Val Acc: 93.00%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 16 Failed: CUDA out of memory. Tried to allocate 626.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 205.81 MiB is free. Including non-PyTorch memory, this process has 7.39 GiB memory in use. Of the allocated memory 7.14 GiB is allocated by PyTorch, and 101.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6946 | train_acc 0.5133 | val_loss 0.6998 | val_acc 0.5567
Epoch [2/100] | train_loss 0.6625 | train_acc 0.5997 | val_loss 0.6524 | val_acc 0.5933
Epoch [3/100] | train_loss 0.6047 | train_acc 0.6410 | val_loss 0.5920 | val_acc 0.6567
Epoch [4/100] | train_loss 0.4086 | train_acc 0.7933 | val_loss 0.5162 | val_acc 0.7967
Epoch [5/100] | train_loss 0.3254 | train_acc 0.8590 | val_loss 0.3624 | val_acc 0.8667
No improvement (1/15).
Epoch [6/100] | train_loss 0.2618 | train_acc 0.8893 | val_loss 0.5700 | val_acc 0.8050
No improvement (2/15).
Epoch [7/100] | train_loss 0.2478 | train_acc 0.8983 | val_loss 0.5655 | val_acc 0.8033
Epoch [8/100] | train_loss 0.1941 | train_acc 0.9227 | val_loss 0.4306 | val_acc 0.8683
Epoch [9/100] | train_loss 0.1649 | train_acc 0.9377 | val_loss 0.4028 | val_acc 0.8800
Epoch [10/100] | train_loss 0.1571 | train_acc 0.9410 | val_loss 0.2937 | val_acc 0.8950
Epoch [11/100] | train_loss 0.1392 | train_acc 0.9507 | val_loss 0.2764 | val_acc 0.9050
Epoch [12/100] | train_loss 0.1369 | train_acc 0.9483 | val_loss 0.2876 | val_acc 0.9083
No improvement (1/15).
Epoch [13/100] | train_loss 0.1308 | train_acc 0.9533 | val_loss 0.2427 | val_acc 0.9083
No improvement (2/15).
Epoch [14/100] | train_loss 0.1254 | train_acc 0.9547 | val_loss 0.2895 | val_acc 0.9067
Epoch [15/100] | train_loss 0.1189 | train_acc 0.9563 | val_loss 0.2592 | val_acc 0.9100
Epoch [16/100] | train_loss 0.1105 | train_acc 0.9593 | val_loss 0.2794 | val_acc 0.9117
Epoch [17/100] | train_loss 0.1128 | train_acc 0.9600 | val_loss 0.2629 | val_acc 0.9133
Epoch [18/100] | train_loss 0.1146 | train_acc 0.9593 | val_loss 0.2222 | val_acc 0.9183
No improvement (1/15).
Epoch [19/100] | train_loss 0.1062 | train_acc 0.9587 | val_loss 0.2743 | val_acc 0.9083
No improvement (2/15).
Epoch [20/100] | train_loss 0.1087 | train_acc 0.9593 | val_loss 0.2789 | val_acc 0.9100
No improvement (3/15).
Epoch [21/100] | train_loss 0.1060 | train_acc 0.9593 | val_loss 0.2642 | val_acc 0.9133
No improvement (4/15).
Epoch [22/100] | train_loss 0.1031 | train_acc 0.9630 | val_loss 0.2734 | val_acc 0.9117
No improvement (5/15).
Epoch [23/100] | train_loss 0.1037 | train_acc 0.9627 | val_loss 0.2409 | val_acc 0.9183
No improvement (6/15).
Epoch [24/100] | train_loss 0.1006 | train_acc 0.9613 | val_loss 0.2487 | val_acc 0.9183
Epoch [25/100] | train_loss 0.1016 | train_acc 0.9637 | val_loss 0.2489 | val_acc 0.9217
Epoch [26/100] | train_loss 0.0992 | train_acc 0.9620 | val_loss 0.2166 | val_acc 0.9300
Epoch [27/100] | train_loss 0.0986 | train_acc 0.9623 | val_loss 0.2100 | val_acc 0.9317
No improvement (1/15).
Epoch [28/100] | train_loss 0.0941 | train_acc 0.9653 | val_loss 0.2142 | val_acc 0.9300
No improvement (2/15).
Epoch [29/100] | train_loss 0.0979 | train_acc 0.9653 | val_loss 0.2157 | val_acc 0.9250
Epoch [30/100] | train_loss 0.0961 | train_acc 0.9663 | val_loss 0.2138 | val_acc 0.9333
No improvement (1/15).
Epoch [31/100] | train_loss 0.0977 | train_acc 0.9650 | val_loss 0.2086 | val_acc 0.9317
Epoch [32/100] | train_loss 0.0922 | train_acc 0.9670 | val_loss 0.1977 | val_acc 0.9350
Epoch [33/100] | train_loss 0.0938 | train_acc 0.9680 | val_loss 0.1935 | val_acc 0.9367
Epoch [34/100] | train_loss 0.0915 | train_acc 0.9677 | val_loss 0.1886 | val_acc 0.9400
No improvement (1/15).
Epoch [35/100] | train_loss 0.0909 | train_acc 0.9660 | val_loss 0.1944 | val_acc 0.9300
No improvement (2/15).
Epoch [36/100] | train_loss 0.0917 | train_acc 0.9657 | val_loss 0.1962 | val_acc 0.9300
No improvement (3/15).
Epoch [37/100] | train_loss 0.0922 | train_acc 0.9670 | val_loss 0.1937 | val_acc 0.9333
No improvement (4/15).
Epoch [38/100] | train_loss 0.0918 | train_acc 0.9653 | val_loss 0.1865 | val_acc 0.9400
No improvement (5/15).
Epoch [39/100] | train_loss 0.0888 | train_acc 0.9680 | val_loss 0.1874 | val_acc 0.9400
No improvement (6/15).
Epoch [40/100] | train_loss 0.0919 | train_acc 0.9653 | val_loss 0.1896 | val_acc 0.9383
No improvement (7/15).
Epoch [41/100] | train_loss 0.0953 | train_acc 0.9670 | val_loss 0.1885 | val_acc 0.9367
No improvement (8/15).
Epoch [42/100] | train_loss 0.0901 | train_acc 0.9670 | val_loss 0.1890 | val_acc 0.9350
No improvement (9/15).
Epoch [43/100] | train_loss 0.0904 | train_acc 0.9657 | val_loss 0.1849 | val_acc 0.9400
No improvement (10/15).
Epoch [44/100] | train_loss 0.0906 | train_acc 0.9663 | val_loss 0.1852 | val_acc 0.9400
No improvement (11/15).
Epoch [45/100] | train_loss 0.0897 | train_acc 0.9677 | val_loss 0.1844 | val_acc 0.9400
Epoch [46/100] | train_loss 0.0867 | train_acc 0.9683 | val_loss 0.1825 | val_acc 0.9433
No improvement (1/15).
Epoch [47/100] | train_loss 0.0886 | train_acc 0.9687 | val_loss 0.1847 | val_acc 0.9400
No improvement (2/15).
Epoch [48/100] | train_loss 0.0902 | train_acc 0.9653 | val_loss 0.1846 | val_acc 0.9417
No improvement (3/15).
Epoch [49/100] | train_loss 0.0890 | train_acc 0.9697 | val_loss 0.1847 | val_acc 0.9417
No improvement (4/15).
Epoch [50/100] | train_loss 0.0887 | train_acc 0.9663 | val_loss 0.1843 | val_acc 0.9417
No improvement (5/15).
Epoch [51/100] | train_loss 0.0915 | train_acc 0.9650 | val_loss 0.1852 | val_acc 0.9417
No improvement (6/15).
Epoch [52/100] | train_loss 0.0876 | train_acc 0.9687 | val_loss 0.1838 | val_acc 0.9417
No improvement (7/15).
Epoch [53/100] | train_loss 0.0883 | train_acc 0.9680 | val_loss 0.1829 | val_acc 0.9433
No improvement (8/15).
Epoch [54/100] | train_loss 0.0879 | train_acc 0.9677 | val_loss 0.1825 | val_acc 0.9433
No improvement (9/15).
Epoch [55/100] | train_loss 0.0923 | train_acc 0.9673 | val_loss 0.1831 | val_acc 0.9433
No improvement (10/15).
Epoch [56/100] | train_loss 0.0892 | train_acc 0.9660 | val_loss 0.1832 | val_acc 0.9433
No improvement (11/15).
Epoch [57/100] | train_loss 0.0894 | train_acc 0.9673 | val_loss 0.1834 | val_acc 0.9433
No improvement (12/15).
Epoch [58/100] | train_loss 0.0884 | train_acc 0.9687 | val_loss 0.1829 | val_acc 0.9433
No improvement (13/15).
Epoch [59/100] | train_loss 0.0876 | train_acc 0.9687 | val_loss 0.1835 | val_acc 0.9433
No improvement (14/15).
Epoch [60/100] | train_loss 0.0871 | train_acc 0.9713 | val_loss 0.1831 | val_acc 0.9433
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 17 Finished. Best Val Acc: 94.33%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 18 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.32 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.96 MiB is allocated by PyTorch, and 3.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 1-1
wandb: ğŸš€ View run Trial18_[CV-Variation]_L4_H8_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zclcvsod
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_224723-zclcvsod/logs
[I 2026-02-02 22:54:00,934] Trial 18 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 16, 'batch_size': 64, 'use_conv1d': True, 'lr': 0.00022130860255494358, 'dropout': 0.21765694620996426}. Best is trial 12 with value: 0.96.
wandb: setting up run z8zir3lp
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260202_225401-z8zir3lp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial19_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z8zir3lp
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 96-97
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–ƒâ–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–‚â–‚â–…â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–†â–†â–ƒâ–†â–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.90333
wandb:             epoch 57
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.95333
wandb:        train/loss 0.1325
wandb: training_time_sec 18306.83011
wandb:           val/acc 0.90333
wandb:          val/loss 0.39443
wandb: 
wandb: ğŸš€ View run Trial19_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z8zir3lp
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260202_225401-z8zir3lp/logs
[I 2026-02-03 03:59:09,927] Trial 19 finished with value: 0.9033333333333333 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0002337905473103732, 'dropout': 0.45665825152831624}. Best is trial 12 with value: 0.96.
wandb: setting up run l9un8af5
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_035910-l9un8af5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial20_[CV-Variation]_L2_H2_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/l9un8af5

ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6877 | train_acc 0.5703 | val_loss 0.6806 | val_acc 0.6050
No improvement (1/15).
Epoch [2/100] | train_loss 0.6637 | train_acc 0.6323 | val_loss 0.6811 | val_acc 0.5850
Epoch [3/100] | train_loss 0.6462 | train_acc 0.6567 | val_loss 0.6460 | val_acc 0.6233
Epoch [4/100] | train_loss 0.6219 | train_acc 0.6770 | val_loss 0.6158 | val_acc 0.6517
Epoch [5/100] | train_loss 0.5852 | train_acc 0.6937 | val_loss 0.5820 | val_acc 0.6667
Epoch [6/100] | train_loss 0.5107 | train_acc 0.7513 | val_loss 0.4753 | val_acc 0.7783
Epoch [7/100] | train_loss 0.4228 | train_acc 0.8050 | val_loss 0.4840 | val_acc 0.8050
Epoch [8/100] | train_loss 0.3147 | train_acc 0.8673 | val_loss 0.4439 | val_acc 0.8600
No improvement (1/15).
Epoch [9/100] | train_loss 0.2459 | train_acc 0.9013 | val_loss 0.5527 | val_acc 0.8383
No improvement (2/15).
Epoch [10/100] | train_loss 0.2322 | train_acc 0.9017 | val_loss 0.5796 | val_acc 0.8417
Epoch [11/100] | train_loss 0.2161 | train_acc 0.9127 | val_loss 0.5189 | val_acc 0.8650
Epoch [12/100] | train_loss 0.2029 | train_acc 0.9217 | val_loss 0.5306 | val_acc 0.8700
No improvement (1/15).
Epoch [13/100] | train_loss 0.2021 | train_acc 0.9200 | val_loss 0.4769 | val_acc 0.8683
Epoch [14/100] | train_loss 0.1844 | train_acc 0.9293 | val_loss 0.4923 | val_acc 0.8750
No improvement (1/15).
Epoch [15/100] | train_loss 0.1769 | train_acc 0.9360 | val_loss 0.5135 | val_acc 0.8750
No improvement (2/15).
Epoch [16/100] | train_loss 0.1781 | train_acc 0.9297 | val_loss 0.5061 | val_acc 0.8717
No improvement (3/15).
Epoch [17/100] | train_loss 0.1754 | train_acc 0.9313 | val_loss 0.4842 | val_acc 0.8733
No improvement (4/15).
Epoch [18/100] | train_loss 0.1725 | train_acc 0.9353 | val_loss 0.5107 | val_acc 0.8700
Epoch [19/100] | train_loss 0.1718 | train_acc 0.9353 | val_loss 0.4939 | val_acc 0.8767
Epoch [20/100] | train_loss 0.1702 | train_acc 0.9367 | val_loss 0.4815 | val_acc 0.8883
No improvement (1/15).
Epoch [21/100] | train_loss 0.1603 | train_acc 0.9400 | val_loss 0.4483 | val_acc 0.8867
No improvement (2/15).
Epoch [22/100] | train_loss 0.1511 | train_acc 0.9460 | val_loss 0.4593 | val_acc 0.8867
No improvement (3/15).
Epoch [23/100] | train_loss 0.1519 | train_acc 0.9433 | val_loss 0.4514 | val_acc 0.8883
No improvement (4/15).
Epoch [24/100] | train_loss 0.1506 | train_acc 0.9460 | val_loss 0.4429 | val_acc 0.8883
Epoch [25/100] | train_loss 0.1483 | train_acc 0.9450 | val_loss 0.4444 | val_acc 0.8917
No improvement (1/15).
Epoch [26/100] | train_loss 0.1445 | train_acc 0.9483 | val_loss 0.4492 | val_acc 0.8917
No improvement (2/15).
Epoch [27/100] | train_loss 0.1455 | train_acc 0.9480 | val_loss 0.4277 | val_acc 0.8917
No improvement (3/15).
Epoch [28/100] | train_loss 0.1438 | train_acc 0.9517 | val_loss 0.4333 | val_acc 0.8917
No improvement (4/15).
Epoch [29/100] | train_loss 0.1421 | train_acc 0.9520 | val_loss 0.4287 | val_acc 0.8917
Epoch [30/100] | train_loss 0.1423 | train_acc 0.9493 | val_loss 0.4365 | val_acc 0.8933
Epoch [31/100] | train_loss 0.1436 | train_acc 0.9513 | val_loss 0.4220 | val_acc 0.8950
Epoch [32/100] | train_loss 0.1422 | train_acc 0.9507 | val_loss 0.4240 | val_acc 0.8967
No improvement (1/15).
Epoch [33/100] | train_loss 0.1394 | train_acc 0.9510 | val_loss 0.4113 | val_acc 0.8950
No improvement (2/15).
Epoch [34/100] | train_loss 0.1383 | train_acc 0.9507 | val_loss 0.4058 | val_acc 0.8967
No improvement (3/15).
Epoch [35/100] | train_loss 0.1381 | train_acc 0.9503 | val_loss 0.4084 | val_acc 0.8967
No improvement (4/15).
Epoch [36/100] | train_loss 0.1379 | train_acc 0.9520 | val_loss 0.4092 | val_acc 0.8967
No improvement (5/15).
Epoch [37/100] | train_loss 0.1358 | train_acc 0.9540 | val_loss 0.4125 | val_acc 0.8967
Epoch [38/100] | train_loss 0.1373 | train_acc 0.9503 | val_loss 0.4105 | val_acc 0.8983
Epoch [39/100] | train_loss 0.1352 | train_acc 0.9513 | val_loss 0.4001 | val_acc 0.9000
No improvement (1/15).
Epoch [40/100] | train_loss 0.1326 | train_acc 0.9537 | val_loss 0.3972 | val_acc 0.9000
No improvement (2/15).
Epoch [41/100] | train_loss 0.1336 | train_acc 0.9543 | val_loss 0.4012 | val_acc 0.9000
No improvement (3/15).
Epoch [42/100] | train_loss 0.1316 | train_acc 0.9560 | val_loss 0.4058 | val_acc 0.8983
Epoch [43/100] | train_loss 0.1356 | train_acc 0.9533 | val_loss 0.3969 | val_acc 0.9033
No improvement (1/15).
Epoch [44/100] | train_loss 0.1346 | train_acc 0.9547 | val_loss 0.3991 | val_acc 0.9017
No improvement (2/15).
Epoch [45/100] | train_loss 0.1343 | train_acc 0.9533 | val_loss 0.3942 | val_acc 0.9033
No improvement (3/15).
Epoch [46/100] | train_loss 0.1331 | train_acc 0.9527 | val_loss 0.3957 | val_acc 0.9017
No improvement (4/15).
Epoch [47/100] | train_loss 0.1338 | train_acc 0.9523 | val_loss 0.3971 | val_acc 0.9017
No improvement (5/15).
Epoch [48/100] | train_loss 0.1332 | train_acc 0.9550 | val_loss 0.3957 | val_acc 0.9017
No improvement (6/15).
Epoch [49/100] | train_loss 0.1331 | train_acc 0.9533 | val_loss 0.3966 | val_acc 0.9017
No improvement (7/15).
Epoch [50/100] | train_loss 0.1319 | train_acc 0.9543 | val_loss 0.3962 | val_acc 0.9033
No improvement (8/15).
Epoch [51/100] | train_loss 0.1330 | train_acc 0.9547 | val_loss 0.3965 | val_acc 0.9033
No improvement (9/15).
Epoch [52/100] | train_loss 0.1329 | train_acc 0.9547 | val_loss 0.3954 | val_acc 0.9033
No improvement (10/15).
Epoch [53/100] | train_loss 0.1325 | train_acc 0.9547 | val_loss 0.3951 | val_acc 0.9033
No improvement (11/15).
Epoch [54/100] | train_loss 0.1312 | train_acc 0.9547 | val_loss 0.3963 | val_acc 0.9033
No improvement (12/15).
Epoch [55/100] | train_loss 0.1318 | train_acc 0.9537 | val_loss 0.3942 | val_acc 0.9033
No improvement (13/15).
Epoch [56/100] | train_loss 0.1329 | train_acc 0.9533 | val_loss 0.3948 | val_acc 0.9033
No improvement (14/15).
Epoch [57/100] | train_loss 0.1325 | train_acc 0.9533 | val_loss 0.3944 | val_acc 0.9033
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 19 Finished. Best Val Acc: 90.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6883 | train_acc 0.5437 | val_loss 0.6836 | val_acc 0.5800
Epoch [2/100] | train_loss 0.6485 | train_acc 0.6353 | val_loss 0.6439 | val_acc 0.6200
Epoch [3/100] | train_loss 0.5687 | train_acc 0.6777 | val_loss 0.4763 | val_acc 0.7700
Epoch [4/100] | train_loss 0.2590 | train_acc 0.9053 | val_loss 0.3709 | val_acc 0.8717
No improvement (1/15).
Epoch [5/100] | train_loss 0.2253 | train_acc 0.9180 | val_loss 0.3790 | val_acc 0.8633
Epoch [6/100] | train_loss 0.2136 | train_acc 0.9170 | val_loss 0.2030 | val_acc 0.9100
No improvement (1/15).
Epoch [7/100] | train_loss 0.1923 | train_acc 0.9347 | val_loss 0.3268 | val_acc 0.8900
Epoch [8/100] | train_loss 0.1476 | train_acc 0.9447 | val_loss 0.1766 | val_acc 0.9383
No improvement (1/15).
Epoch [9/100] | train_loss 0.1378 | train_acc 0.9520 | val_loss 0.1778 | val_acc 0.9300
No improvement (2/15).
Epoch [10/100] | train_loss 0.1261 | train_acc 0.9573 | val_loss 0.1848 | val_acc 0.9350
No improvement (3/15).
Epoch [11/100] | train_loss 0.1282 | train_acc 0.9540 | val_loss 0.2098 | val_acc 0.9300
No improvement (4/15).
Epoch [12/100] | train_loss 0.1174 | train_acc 0.9593 | val_loss 0.1941 | val_acc 0.9350
No improvement (5/15).
Epoch [13/100] | train_loss 0.1268 | train_acc 0.9553 | val_loss 0.3099 | val_acc 0.8967
No improvement (6/15).
Epoch [14/100] | train_loss 0.1308 | train_acc 0.9520 | val_loss 0.1874 | val_acc 0.9350
No improvement (7/15).
Epoch [15/100] | train_loss 0.1081 | train_acc 0.9627 | val_loss 0.1846 | val_acc 0.9367
No improvement (8/15).
Epoch [16/100] | train_loss 0.1131 | train_acc 0.9610 | val_loss 0.1871 | val_acc 0.9383
No improvement (9/15).
Epoch [17/100] | train_loss 0.1092 | train_acc 0.9620 | val_loss 0.2047 | val_acc 0.9250
No improvement (10/15).
Epoch [18/100] | train_loss 0.1042 | train_acc 0.9660 | val_loss 0.1937 | val_acc 0.9317
No improvement (11/15).
Epoch [19/100] | train_loss 0.1045 | train_acc 0.9643 | val_loss 0.2122 | val_acc 0.9233
Epoch [20/100] | train_loss 0.1038 | train_acc 0.9600 | val_loss 0.1917 | val_acc 0.9400wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–ƒâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–„â–„â–â–ƒâ–â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94667
wandb:             epoch 35
wandb:         grad/norm 1.0
wandb:                lr 2e-05
wandb:         train/acc 0.97133
wandb:        train/loss 0.08992
wandb: training_time_sec 2315.14139
wandb:           val/acc 0.94333
wandb:          val/loss 0.19653
wandb: 
wandb: ğŸš€ View run Trial20_[CV-Variation]_L2_H2_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/l9un8af5
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_035910-l9un8af5/logs
[I 2026-02-03 04:37:47,189] Trial 20 finished with value: 0.9466666666666667 and parameters: {'nhead': 2, 'num_layers': 2, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0006261110273746602, 'dropout': 0.3160566254814919}. Best is trial 12 with value: 0.96.
wandb: setting up run rp815fxm
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_043747-rp815fxm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial21_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rp815fxm
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 36-37
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–†â–ƒâ–„â–‡â–ˆâ–ˆâ–ˆâ–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–
wandb:  train/acc â–â–‚â–†â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–‡â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆ
wandb:   val/loss â–†â–„â–„â–ˆâ–â–â–‚â–‚â–â–â–‚â–…â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94833
wandb:             epoch 20
wandb:         grad/norm 0.14132
wandb:                lr 0.00028
wandb:         train/acc 0.97767
wandb:        train/loss 0.07255
wandb: training_time_sec 4725.65011
wandb:           val/acc 0.92667
wandb:          val/loss 0.26693
wandb: 
wandb: ğŸš€ View run Trial21_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rp815fxm
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_043747-rp815fxm/logs
[I 2026-02-03 05:56:35,078] Trial 21 finished with value: 0.9483333333333334 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.001129982809359665, 'dropout': 0.48499180514040247}. Best is trial 12 with value: 0.96.
wandb: setting up run xk398syn
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_055635-xk398syn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial22_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xk398syn
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 51-52
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–„â–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–„â–…â–‡â–†â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–‡â–„â–„â–„â–‚â–â–ƒâ–‚â–‚â–„â–ƒâ–ƒâ–â–„â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 29
wandb:         grad/norm 1.0
wandb:                lr 5e-05
wandb:         train/acc 0.973
wandb:        train/loss 0.07263
wandb: training_time_sec 6783.65429
wandb:           val/acc 0.955
wandb:          val/loss 0.16051
wandb: 
wandb: ğŸš€ View run Trial22_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xk398syn
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_055635-xk398syn/logs
[I 2026-02-03 07:49:40,953] Trial 22 finished with value: 0.96 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0008405267211782929, 'dropout': 0.4771474758068308}. Best is trial 12 with value: 0.96.
wandb: setting up run so3yz81a
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_074941-so3yz81a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial23_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/so3yz81a

Epoch [21/100] | train_loss 0.1143 | train_acc 0.9557 | val_loss 0.1935 | val_acc 0.9467
No improvement (1/15).
Epoch [22/100] | train_loss 0.1038 | train_acc 0.9623 | val_loss 0.1910 | val_acc 0.9400
No improvement (2/15).
Epoch [23/100] | train_loss 0.1107 | train_acc 0.9580 | val_loss 0.1915 | val_acc 0.9400
No improvement (3/15).
Epoch [24/100] | train_loss 0.1057 | train_acc 0.9597 | val_loss 0.1971 | val_acc 0.9417
No improvement (4/15).
Epoch [25/100] | train_loss 0.1113 | train_acc 0.9587 | val_loss 0.1975 | val_acc 0.9417
No improvement (5/15).
Epoch [26/100] | train_loss 0.0958 | train_acc 0.9663 | val_loss 0.2007 | val_acc 0.9400
No improvement (6/15).
Epoch [27/100] | train_loss 0.0930 | train_acc 0.9673 | val_loss 0.1967 | val_acc 0.9417
No improvement (7/15).
Epoch [28/100] | train_loss 0.0930 | train_acc 0.9670 | val_loss 0.1999 | val_acc 0.9383
No improvement (8/15).
Epoch [29/100] | train_loss 0.0908 | train_acc 0.9700 | val_loss 0.1960 | val_acc 0.9417
No improvement (9/15).
Epoch [30/100] | train_loss 0.0908 | train_acc 0.9683 | val_loss 0.1979 | val_acc 0.9433
No improvement (10/15).
Epoch [31/100] | train_loss 0.0904 | train_acc 0.9690 | val_loss 0.2012 | val_acc 0.9417
No improvement (11/15).
Epoch [32/100] | train_loss 0.0911 | train_acc 0.9687 | val_loss 0.1964 | val_acc 0.9433
No improvement (12/15).
Epoch [33/100] | train_loss 0.0916 | train_acc 0.9687 | val_loss 0.1972 | val_acc 0.9417
No improvement (13/15).
Epoch [34/100] | train_loss 0.0909 | train_acc 0.9703 | val_loss 0.1962 | val_acc 0.9417
No improvement (14/15).
Epoch [35/100] | train_loss 0.0899 | train_acc 0.9713 | val_loss 0.1965 | val_acc 0.9433
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 20 Finished. Best Val Acc: 94.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7264 | train_acc 0.5487 | val_loss 0.6662 | val_acc 0.6000
Epoch [2/100] | train_loss 0.6180 | train_acc 0.6353 | val_loss 0.4665 | val_acc 0.7883
Epoch [3/100] | train_loss 0.2984 | train_acc 0.8653 | val_loss 0.4166 | val_acc 0.8750
No improvement (1/15).
Epoch [4/100] | train_loss 0.2475 | train_acc 0.9080 | val_loss 0.8158 | val_acc 0.8117
Epoch [5/100] | train_loss 0.2410 | train_acc 0.9130 | val_loss 0.1972 | val_acc 0.9350
Epoch [6/100] | train_loss 0.1393 | train_acc 0.9473 | val_loss 0.1778 | val_acc 0.9483
No improvement (1/15).
Epoch [7/100] | train_loss 0.1707 | train_acc 0.9347 | val_loss 0.2382 | val_acc 0.9250
No improvement (2/15).
Epoch [8/100] | train_loss 0.1779 | train_acc 0.9317 | val_loss 0.2265 | val_acc 0.9333
No improvement (3/15).
Epoch [9/100] | train_loss 0.1231 | train_acc 0.9553 | val_loss 0.2071 | val_acc 0.9317
No improvement (4/15).
Epoch [10/100] | train_loss 0.1546 | train_acc 0.9413 | val_loss 0.2053 | val_acc 0.9383
No improvement (5/15).
Epoch [11/100] | train_loss 0.1090 | train_acc 0.9617 | val_loss 0.2771 | val_acc 0.9300
No improvement (6/15).
Epoch [12/100] | train_loss 0.0891 | train_acc 0.9707 | val_loss 0.5824 | val_acc 0.8667
No improvement (7/15).
Epoch [13/100] | train_loss 0.0979 | train_acc 0.9647 | val_loss 0.3045 | val_acc 0.9233
No improvement (8/15).
Epoch [14/100] | train_loss 0.0866 | train_acc 0.9717 | val_loss 0.3089 | val_acc 0.9283
No improvement (9/15).
Epoch [15/100] | train_loss 0.0948 | train_acc 0.9643 | val_loss 0.3015 | val_acc 0.9267
No improvement (10/15).
Epoch [16/100] | train_loss 0.1018 | train_acc 0.9670 | val_loss 0.3437 | val_acc 0.9133
No improvement (11/15).
Epoch [17/100] | train_loss 0.0796 | train_acc 0.9737 | val_loss 0.3547 | val_acc 0.9083
No improvement (12/15).
Epoch [18/100] | train_loss 0.0802 | train_acc 0.9707 | val_loss 0.3907 | val_acc 0.8983
No improvement (13/15).
Epoch [19/100] | train_loss 0.0767 | train_acc 0.9750 | val_loss 0.3608 | val_acc 0.9083
No improvement (14/15).
Epoch [20/100] | train_loss 0.0725 | train_acc 0.9777 | val_loss 0.2669 | val_acc 0.9267
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 21 Finished. Best Val Acc: 94.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6871 | train_acc 0.5737 | val_loss 0.6732 | val_acc 0.5733
Epoch [2/100] | train_loss 0.6409 | train_acc 0.6350 | val_loss 0.6015 | val_acc 0.6867
Epoch [3/100] | train_loss 0.5042 | train_acc 0.7573 | val_loss 0.5913 | val_acc 0.7400
Epoch [4/100] | train_loss 0.3952 | train_acc 0.8177 | val_loss 0.4030 | val_acc 0.8200
Epoch [5/100] | train_loss 0.2803 | train_acc 0.8900 | val_loss 0.3483 | val_acc 0.8950
No improvement (1/15).
Epoch [6/100] | train_loss 0.1892 | train_acc 0.9307 | val_loss 0.3628 | val_acc 0.8717
Epoch [7/100] | train_loss 0.1489 | train_acc 0.9427 | val_loss 0.2129 | val_acc 0.9300
Epoch [8/100] | train_loss 0.1261 | train_acc 0.9540 | val_loss 0.1830 | val_acc 0.9467
No improvement (1/15).
Epoch [9/100] | train_loss 0.1216 | train_acc 0.9577 | val_loss 0.3135 | val_acc 0.9000
No improvement (2/15).
Epoch [10/100] | train_loss 0.1143 | train_acc 0.9593 | val_loss 0.2477 | val_acc 0.9283
No improvement (3/15).
Epoch [11/100] | train_loss 0.1060 | train_acc 0.9617 | val_loss 0.2004 | val_acc 0.9433
No improvement (4/15).
Epoch [12/100] | train_loss 0.1080 | train_acc 0.9603 | val_loss 0.3597 | val_acc 0.8900
No improvement (5/15).
Epoch [13/100] | train_loss 0.1155 | train_acc 0.9563 | val_loss 0.3159 | val_acc 0.9017
No improvement (6/15).
Epoch [14/100] | train_loss 0.1021 | train_acc 0.9640 | val_loss 0.2894 | val_acc 0.9100
Epoch [15/100] | train_loss 0.1167 | train_acc 0.9573 | val_loss 0.1508 | val_acc 0.9600
No improvement (1/15).
Epoch [16/100] | train_loss 0.0952 | train_acc 0.9670 | val_loss 0.3495 | val_acc 0.9017
No improvement (2/15).
Epoch [17/100] | train_loss 0.1008 | train_acc 0.9650 | val_loss 0.2313 | val_acc 0.9250
No improvement (3/15).
Epoch [18/100] | train_loss 0.0844 | train_acc 0.9693 | val_loss 0.3236 | val_acc 0.9117
No improvement (4/15).
Epoch [19/100] | train_loss 0.0968 | train_acc 0.9660 | val_loss 0.2544 | val_acc 0.9233
No improvement (5/15).
Epoch [20/100] | train_loss 0.0813 | train_acc 0.9700 | val_loss 0.1472 | val_acc 0.9583
No improvement (6/15).
Epoch [21/100] | train_loss 0.0866 | train_acc 0.9693 | val_loss 0.1478 | val_acc 0.9600
No improvement (7/15).
Epoch [22/100] | train_loss 0.0834 | train_acc 0.9717 | val_loss 0.1485 | val_acc 0.9600
No improvement (8/15).
Epoch [23/100] | train_loss 0.0847 | train_acc 0.9693 | val_loss 0.1521 | val_acc 0.9600
No improvement (9/15).
Epoch [24/100] | train_loss 0.0831 | train_acc 0.9710 | val_loss 0.1499 | val_acc 0.9583
No improvement (10/15).
Epoch [25/100] | train_loss 0.0838 | train_acc 0.9693 | val_loss 0.1559 | val_acc 0.9583
No improvement (11/15).
Epoch [26/100] | train_loss 0.0800 | train_acc 0.9713 | val_loss 0.1817 | val_acc 0.9550
No improvement (12/15).
Epoch [27/100] | train_loss 0.0742 | train_acc 0.9717 | val_loss 0.1604 | val_acc 0.9567
No improvement (13/15).
Epoch [28/100] | train_loss 0.0733 | train_acc 0.9733 | val_loss 0.1594 | val_acc 0.9550
No improvement (14/15).
Epoch [29/100] | train_loss 0.0726 | train_acc 0.9730 | val_loss 0.1605 | val_acc 0.9550
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 22 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6845 | train_acc 0.5647 | val_loss 0.6827 | val_acc 0.6150
Epoch [2/100] | train_loss 0.6561 | train_acc 0.6240 | val_loss 0.6359 | val_acc 0.6333
Epoch [3/100] | train_loss 0.5270 | train_acc 0.7180 | val_loss 0.6952 | val_acc 0.7033
Epoch [4/100] | train_loss 0.2930 | train_acc 0.8727 | val_loss 0.4307 | val_acc 0.8400
Epoch [5/100] | train_loss 0.2150 | train_acc 0.9150 | val_loss 0.2406 | val_acc 0.9133
Epoch [6/100] | train_loss 0.1908 | train_acc 0.9267 | val_loss 0.2146 | val_acc 0.9300
No improvement (1/15).
Epoch [7/100] | train_loss 0.1587 | train_acc 0.9403 | val_loss 0.2802 | val_acc 0.9217
No improvement (2/15).
Epoch [8/100] | train_loss 0.1799 | train_acc 0.9313 | val_loss 0.4137 | val_acc 0.8617
Epoch [9/100] | train_loss 0.1684 | train_acc 0.9360 | val_loss 0.2283 | val_acc 0.9400
No improvement (1/15).
Epoch [10/100] | train_loss 0.1234 | train_acc 0.9533 | val_loss 0.2155 | val_acc 0.9317wandb: uploading data; updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–…â–‚â–ˆâ–â–‡â–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–„â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–ƒâ–†â–‡â–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–ˆâ–„â–‚â–‚â–‚â–„â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95333
wandb:             epoch 34
wandb:         grad/norm 0.47081
wandb:                lr 2e-05
wandb:         train/acc 0.97433
wandb:        train/loss 0.06947
wandb: training_time_sec 7875.21806
wandb:           val/acc 0.95167
wandb:          val/loss 0.18445
wandb: 
wandb: ğŸš€ View run Trial23_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/so3yz81a
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_074941-so3yz81a/logs
[I 2026-02-03 10:00:58,176] Trial 23 finished with value: 0.9533333333333334 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0006622541398687889, 'dropout': 0.4441454059277331}. Best is trial 12 with value: 0.96.
wandb: setting up run nf3jumlu
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_100058-nf3jumlu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial24_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nf3jumlu
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–„â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–ˆâ–ˆâ–…â–ˆâ–…â–„â–„â–‚â–ƒâ–‚â–†â–†â–…â–â–„â–„â–‚â–ƒâ–ˆâ–ƒâ–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–„â–‡â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–‡â–ƒâ–â–â–ƒâ–â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.955
wandb:             epoch 42
wandb:         grad/norm 0.39744
wandb:                lr 3e-05
wandb:         train/acc 0.99533
wandb:        train/loss 0.01811
wandb: training_time_sec 18091.02109
wandb:           val/acc 0.94833
wandb:          val/loss 0.3271
wandb: 
wandb: ğŸš€ View run Trial24_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nf3jumlu
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_100058-nf3jumlu/logs
[I 2026-02-03 15:02:31,252] Trial 24 finished with value: 0.955 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0020234159331171636, 'dropout': 0.38008506780526957}. Best is trial 12 with value: 0.96.
wandb: setting up run xayafy4r
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_150231-xayafy4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial25_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xayafy4r

Epoch [11/100] | train_loss 0.1079 | train_acc 0.9590 | val_loss 0.1780 | val_acc 0.9417
No improvement (1/15).
Epoch [12/100] | train_loss 0.1057 | train_acc 0.9593 | val_loss 0.1849 | val_acc 0.9400
Epoch [13/100] | train_loss 0.1050 | train_acc 0.9617 | val_loss 0.1764 | val_acc 0.9517
No improvement (1/15).
Epoch [14/100] | train_loss 0.1053 | train_acc 0.9603 | val_loss 0.1691 | val_acc 0.9450
No improvement (2/15).
Epoch [15/100] | train_loss 0.1132 | train_acc 0.9570 | val_loss 0.2344 | val_acc 0.9267
No improvement (3/15).
Epoch [16/100] | train_loss 0.0825 | train_acc 0.9717 | val_loss 0.2161 | val_acc 0.9333
No improvement (4/15).
Epoch [17/100] | train_loss 0.1015 | train_acc 0.9623 | val_loss 0.1690 | val_acc 0.9450
No improvement (5/15).
Epoch [18/100] | train_loss 0.0926 | train_acc 0.9653 | val_loss 0.2058 | val_acc 0.9383
No improvement (6/15).
Epoch [19/100] | train_loss 0.0935 | train_acc 0.9660 | val_loss 0.1978 | val_acc 0.9417
Epoch [20/100] | train_loss 0.0876 | train_acc 0.9683 | val_loss 0.1892 | val_acc 0.9533
No improvement (1/15).
Epoch [21/100] | train_loss 0.0992 | train_acc 0.9640 | val_loss 0.1950 | val_acc 0.9450
No improvement (2/15).
Epoch [22/100] | train_loss 0.0741 | train_acc 0.9770 | val_loss 0.1789 | val_acc 0.9450
No improvement (3/15).
Epoch [23/100] | train_loss 0.0784 | train_acc 0.9723 | val_loss 0.2318 | val_acc 0.9350
No improvement (4/15).
Epoch [24/100] | train_loss 0.0857 | train_acc 0.9693 | val_loss 0.2372 | val_acc 0.9333
No improvement (5/15).
Epoch [25/100] | train_loss 0.0878 | train_acc 0.9680 | val_loss 0.2343 | val_acc 0.9317
No improvement (6/15).
Epoch [26/100] | train_loss 0.0861 | train_acc 0.9680 | val_loss 0.2366 | val_acc 0.9300
No improvement (7/15).
Epoch [27/100] | train_loss 0.0863 | train_acc 0.9687 | val_loss 0.2315 | val_acc 0.9317
No improvement (8/15).
Epoch [28/100] | train_loss 0.0742 | train_acc 0.9703 | val_loss 0.1871 | val_acc 0.9450
No improvement (9/15).
Epoch [29/100] | train_loss 0.0736 | train_acc 0.9723 | val_loss 0.1932 | val_acc 0.9450
No improvement (10/15).
Epoch [30/100] | train_loss 0.0728 | train_acc 0.9733 | val_loss 0.1883 | val_acc 0.9467
No improvement (11/15).
Epoch [31/100] | train_loss 0.0712 | train_acc 0.9737 | val_loss 0.2029 | val_acc 0.9433
No improvement (12/15).
Epoch [32/100] | train_loss 0.0731 | train_acc 0.9717 | val_loss 0.1841 | val_acc 0.9483
No improvement (13/15).
Epoch [33/100] | train_loss 0.0704 | train_acc 0.9750 | val_loss 0.2025 | val_acc 0.9417
No improvement (14/15).
Epoch [34/100] | train_loss 0.0695 | train_acc 0.9743 | val_loss 0.1844 | val_acc 0.9517
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 23 Finished. Best Val Acc: 95.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7560 | train_acc 0.5453 | val_loss 0.6741 | val_acc 0.5983
Epoch [2/100] | train_loss 0.5453 | train_acc 0.7020 | val_loss 0.4221 | val_acc 0.8183
No improvement (1/15).
Epoch [3/100] | train_loss 0.3347 | train_acc 0.8557 | val_loss 0.6307 | val_acc 0.7533
Epoch [4/100] | train_loss 0.2487 | train_acc 0.8967 | val_loss 0.3096 | val_acc 0.8917
Epoch [5/100] | train_loss 0.1772 | train_acc 0.9357 | val_loss 0.1588 | val_acc 0.9433
No improvement (1/15).
Epoch [6/100] | train_loss 0.1846 | train_acc 0.9280 | val_loss 0.1690 | val_acc 0.9367
No improvement (2/15).
Epoch [7/100] | train_loss 0.1534 | train_acc 0.9437 | val_loss 0.3109 | val_acc 0.8733
Epoch [8/100] | train_loss 0.1163 | train_acc 0.9543 | val_loss 0.1405 | val_acc 0.9467
Epoch [9/100] | train_loss 0.1021 | train_acc 0.9630 | val_loss 0.1383 | val_acc 0.9483
Epoch [10/100] | train_loss 0.0918 | train_acc 0.9650 | val_loss 0.1422 | val_acc 0.9517
No improvement (1/15).
Epoch [11/100] | train_loss 0.0952 | train_acc 0.9667 | val_loss 0.2433 | val_acc 0.9367
No improvement (2/15).
Epoch [12/100] | train_loss 0.0864 | train_acc 0.9683 | val_loss 0.1881 | val_acc 0.9450
No improvement (3/15).
Epoch [13/100] | train_loss 0.0753 | train_acc 0.9697 | val_loss 0.1479 | val_acc 0.9500
No improvement (4/15).
Epoch [14/100] | train_loss 0.0816 | train_acc 0.9710 | val_loss 0.1555 | val_acc 0.9500
No improvement (5/15).
Epoch [15/100] | train_loss 0.0874 | train_acc 0.9683 | val_loss 0.1866 | val_acc 0.9467
No improvement (6/15).
Epoch [16/100] | train_loss 0.0703 | train_acc 0.9730 | val_loss 0.1830 | val_acc 0.9483
Epoch [17/100] | train_loss 0.0652 | train_acc 0.9783 | val_loss 0.1784 | val_acc 0.9533
No improvement (1/15).
Epoch [18/100] | train_loss 0.0570 | train_acc 0.9807 | val_loss 0.1899 | val_acc 0.9483
No improvement (2/15).
Epoch [19/100] | train_loss 0.0557 | train_acc 0.9807 | val_loss 0.2131 | val_acc 0.9450
No improvement (3/15).
Epoch [20/100] | train_loss 0.0506 | train_acc 0.9817 | val_loss 0.2579 | val_acc 0.9400
No improvement (4/15).
Epoch [21/100] | train_loss 0.0496 | train_acc 0.9840 | val_loss 0.2333 | val_acc 0.9450
No improvement (5/15).
Epoch [22/100] | train_loss 0.0463 | train_acc 0.9823 | val_loss 0.2914 | val_acc 0.9350
No improvement (6/15).
Epoch [23/100] | train_loss 0.0386 | train_acc 0.9867 | val_loss 0.2894 | val_acc 0.9417
No improvement (7/15).
Epoch [24/100] | train_loss 0.0376 | train_acc 0.9863 | val_loss 0.3367 | val_acc 0.9300
No improvement (8/15).
Epoch [25/100] | train_loss 0.0416 | train_acc 0.9850 | val_loss 0.3385 | val_acc 0.9283
No improvement (9/15).
Epoch [26/100] | train_loss 0.0415 | train_acc 0.9840 | val_loss 0.2533 | val_acc 0.9517
No improvement (10/15).
Epoch [27/100] | train_loss 0.0374 | train_acc 0.9860 | val_loss 0.2426 | val_acc 0.9533
Epoch [28/100] | train_loss 0.0329 | train_acc 0.9877 | val_loss 0.2581 | val_acc 0.9550
No improvement (1/15).
Epoch [29/100] | train_loss 0.0294 | train_acc 0.9897 | val_loss 0.2740 | val_acc 0.9517
No improvement (2/15).
Epoch [30/100] | train_loss 0.0307 | train_acc 0.9893 | val_loss 0.2834 | val_acc 0.9533
No improvement (3/15).
Epoch [31/100] | train_loss 0.0307 | train_acc 0.9893 | val_loss 0.2843 | val_acc 0.9500
No improvement (4/15).
Epoch [32/100] | train_loss 0.0281 | train_acc 0.9923 | val_loss 0.2880 | val_acc 0.9450
No improvement (5/15).
Epoch [33/100] | train_loss 0.0246 | train_acc 0.9940 | val_loss 0.2859 | val_acc 0.9483
No improvement (6/15).
Epoch [34/100] | train_loss 0.0205 | train_acc 0.9950 | val_loss 0.2921 | val_acc 0.9483
No improvement (7/15).
Epoch [35/100] | train_loss 0.0204 | train_acc 0.9950 | val_loss 0.2987 | val_acc 0.9467
No improvement (8/15).
Epoch [36/100] | train_loss 0.0194 | train_acc 0.9950 | val_loss 0.2961 | val_acc 0.9517
No improvement (9/15).
Epoch [37/100] | train_loss 0.0198 | train_acc 0.9953 | val_loss 0.3142 | val_acc 0.9483
No improvement (10/15).
Epoch [38/100] | train_loss 0.0172 | train_acc 0.9960 | val_loss 0.3118 | val_acc 0.9433
No improvement (11/15).
Epoch [39/100] | train_loss 0.0179 | train_acc 0.9947 | val_loss 0.3192 | val_acc 0.9500
No improvement (12/15).
Epoch [40/100] | train_loss 0.0164 | train_acc 0.9950 | val_loss 0.3217 | val_acc 0.9450
No improvement (13/15).
Epoch [41/100] | train_loss 0.0176 | train_acc 0.9950 | val_loss 0.3255 | val_acc 0.9467
No improvement (14/15).
Epoch [42/100] | train_loss 0.0181 | train_acc 0.9953 | val_loss 0.3271 | val_acc 0.9483
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 24 Finished. Best Val Acc: 95.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7371 | train_acc 0.5667 | val_loss 0.5814 | val_acc 0.6633
Epoch [2/100] | train_loss 0.5401 | train_acc 0.7107 | val_loss 0.4354 | val_acc 0.8267
No improvement (1/15).
Epoch [3/100] | train_loss 0.4637 | train_acc 0.8117 | val_loss 0.4494 | val_acc 0.8017
Epoch [4/100] | train_loss 0.2576 | train_acc 0.8950 | val_loss 0.3426 | val_acc 0.8667
Epoch [5/100] | train_loss 0.1987 | train_acc 0.9240 | val_loss 0.2494 | val_acc 0.9050
Epoch [6/100] | train_loss 0.1967 | train_acc 0.9223 | val_loss 0.2722 | val_acc 0.9100
No improvement (1/15).
Epoch [7/100] | train_loss 0.1718 | train_acc 0.9387 | val_loss 0.3094 | val_acc 0.8900
No improvement (2/15).
Epoch [8/100] | train_loss 0.1296 | train_acc 0.9500 | val_loss 0.2623 | val_acc 0.8900
No improvement (3/15).wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–…â–ˆâ–…â–ƒâ–ˆâ–„â–ˆâ–ˆâ–…â–â–ˆâ–ˆâ–ƒâ–ƒâ–…â–ƒâ–†â–‡â–‚â–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–†â–„â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94167
wandb:             epoch 40
wandb:         grad/norm 1.0
wandb:                lr 7e-05
wandb:         train/acc 0.98067
wandb:        train/loss 0.05283
wandb: training_time_sec 9207.02664
wandb:           val/acc 0.935
wandb:          val/loss 0.36392
wandb: 
wandb: ğŸš€ View run Trial25_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xayafy4r
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_150231-xayafy4r/logs
[I 2026-02-03 17:36:00,456] Trial 25 finished with value: 0.9416666666666667 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.004358820788222383, 'dropout': 0.4455493032681241}. Best is trial 12 with value: 0.96.
wandb: setting up run 63mzbwta
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_173601-63mzbwta
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial26_[CV-Variation]_L4_H8_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/63mzbwta
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: ğŸš€ View run Trial26_[CV-Variation]_L4_H8_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/63mzbwta
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_173601-63mzbwta/logs
[I 2026-02-03 17:42:43,294] Trial 26 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 16, 'batch_size': 64, 'use_conv1d': True, 'lr': 0.001464880984782554, 'dropout': 0.14144210304157911}. Best is trial 12 with value: 0.96.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_174243-qm0yb50i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial27_[CV-Variation]_L4_H8_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qm0yb50i
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: ğŸš€ View run Trial27_[CV-Variation]_L4_H8_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qm0yb50i
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_174243-qm0yb50i/logs
[I 2026-02-03 17:49:30,042] Trial 27 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 32, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.000787582295773752, 'dropout': 0.4748738869943243}. Best is trial 12 with value: 0.96.
wandb: setting up run ecrm7801
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_174930-ecrm7801
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial28_[CV-Variation]_L2_H2_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ecrm7801

Epoch [9/100] | train_loss 0.1339 | train_acc 0.9517 | val_loss 0.2550 | val_acc 0.8967
No improvement (4/15).
Epoch [10/100] | train_loss 0.1308 | train_acc 0.9520 | val_loss 0.2233 | val_acc 0.9017
Epoch [11/100] | train_loss 0.1215 | train_acc 0.9543 | val_loss 0.2368 | val_acc 0.9133
Epoch [12/100] | train_loss 0.1203 | train_acc 0.9560 | val_loss 0.2351 | val_acc 0.9167
No improvement (1/15).
Epoch [13/100] | train_loss 0.1401 | train_acc 0.9473 | val_loss 0.2147 | val_acc 0.9117
No improvement (2/15).
Epoch [14/100] | train_loss 0.1014 | train_acc 0.9627 | val_loss 0.2544 | val_acc 0.9133
Epoch [15/100] | train_loss 0.0941 | train_acc 0.9663 | val_loss 0.2449 | val_acc 0.9183
No improvement (1/15).
Epoch [16/100] | train_loss 0.0927 | train_acc 0.9663 | val_loss 0.2726 | val_acc 0.9133
Epoch [17/100] | train_loss 0.0875 | train_acc 0.9687 | val_loss 0.2513 | val_acc 0.9233
Epoch [18/100] | train_loss 0.0940 | train_acc 0.9640 | val_loss 0.2281 | val_acc 0.9283
No improvement (1/15).
Epoch [19/100] | train_loss 0.0988 | train_acc 0.9610 | val_loss 0.2850 | val_acc 0.9083
Epoch [20/100] | train_loss 0.0901 | train_acc 0.9623 | val_loss 0.2062 | val_acc 0.9367
No improvement (1/15).
Epoch [21/100] | train_loss 0.0698 | train_acc 0.9777 | val_loss 0.2648 | val_acc 0.9317
No improvement (2/15).
Epoch [22/100] | train_loss 0.0717 | train_acc 0.9740 | val_loss 0.2742 | val_acc 0.9250
No improvement (3/15).
Epoch [23/100] | train_loss 0.0779 | train_acc 0.9733 | val_loss 0.2767 | val_acc 0.9233
No improvement (4/15).
Epoch [24/100] | train_loss 0.0686 | train_acc 0.9753 | val_loss 0.3011 | val_acc 0.9217
No improvement (5/15).
Epoch [25/100] | train_loss 0.0729 | train_acc 0.9750 | val_loss 0.2701 | val_acc 0.9283
Epoch [26/100] | train_loss 0.0644 | train_acc 0.9777 | val_loss 0.2591 | val_acc 0.9417
No improvement (1/15).
Epoch [27/100] | train_loss 0.0584 | train_acc 0.9793 | val_loss 0.2528 | val_acc 0.9317
No improvement (2/15).
Epoch [28/100] | train_loss 0.0545 | train_acc 0.9813 | val_loss 0.2859 | val_acc 0.9283
No improvement (3/15).
Epoch [29/100] | train_loss 0.0636 | train_acc 0.9770 | val_loss 0.2617 | val_acc 0.9367
No improvement (4/15).
Epoch [30/100] | train_loss 0.0579 | train_acc 0.9790 | val_loss 0.2931 | val_acc 0.9333
No improvement (5/15).
Epoch [31/100] | train_loss 0.0599 | train_acc 0.9797 | val_loss 0.2904 | val_acc 0.9317
No improvement (6/15).
Epoch [32/100] | train_loss 0.0571 | train_acc 0.9787 | val_loss 0.3197 | val_acc 0.9317
No improvement (7/15).
Epoch [33/100] | train_loss 0.0557 | train_acc 0.9807 | val_loss 0.3293 | val_acc 0.9250
No improvement (8/15).
Epoch [34/100] | train_loss 0.0608 | train_acc 0.9800 | val_loss 0.3270 | val_acc 0.9283
No improvement (9/15).
Epoch [35/100] | train_loss 0.0595 | train_acc 0.9797 | val_loss 0.3025 | val_acc 0.9400
No improvement (10/15).
Epoch [36/100] | train_loss 0.0614 | train_acc 0.9797 | val_loss 0.3019 | val_acc 0.9333
No improvement (11/15).
Epoch [37/100] | train_loss 0.0628 | train_acc 0.9757 | val_loss 0.2991 | val_acc 0.9350
No improvement (12/15).
Epoch [38/100] | train_loss 0.0664 | train_acc 0.9750 | val_loss 0.3480 | val_acc 0.9367
No improvement (13/15).
Epoch [39/100] | train_loss 0.0567 | train_acc 0.9800 | val_loss 0.3985 | val_acc 0.9333
No improvement (14/15).
Epoch [40/100] | train_loss 0.0528 | train_acc 0.9807 | val_loss 0.3639 | val_acc 0.9350
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 25 Finished. Best Val Acc: 94.17%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 26 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.32 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.96 MiB is allocated by PyTorch, and 3.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 27 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.22 GiB is free. Including non-PyTorch memory, this process has 378.00 MiB memory in use. Of the allocated memory 219.20 MiB is allocated by PyTorch, and 2.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6684 | train_acc 0.6003 | val_loss 0.6718 | val_acc 0.5983
Epoch [2/100] | train_loss 0.5852 | train_acc 0.7043 | val_loss 0.6979 | val_acc 0.6467
Epoch [3/100] | train_loss 0.4770 | train_acc 0.7723 | val_loss 0.5687 | val_acc 0.7583
No improvement (1/15).
Epoch [4/100] | train_loss 0.3519 | train_acc 0.8577 | val_loss 0.6646 | val_acc 0.7550
No improvement (2/15).
Epoch [5/100] | train_loss 0.3230 | train_acc 0.8690 | val_loss 0.6834 | val_acc 0.7567
Epoch [6/100] | train_loss 0.2910 | train_acc 0.8873 | val_loss 0.6381 | val_acc 0.8000
Epoch [7/100] | train_loss 0.2697 | train_acc 0.8960 | val_loss 0.4044 | val_acc 0.8733
Epoch [8/100] | train_loss 0.2215 | train_acc 0.9177 | val_loss 0.4237 | val_acc 0.8850
Epoch [9/100] | train_loss 0.1714 | train_acc 0.9377 | val_loss 0.4327 | val_acc 0.8883
Epoch [10/100] | train_loss 0.1687 | train_acc 0.9400 | val_loss 0.3531 | val_acc 0.9117
No improvement (1/15).
Epoch [11/100] | train_loss 0.1561 | train_acc 0.9433 | val_loss 0.4465 | val_acc 0.9083
Epoch [12/100] | train_loss 0.1364 | train_acc 0.9527 | val_loss 0.4141 | val_acc 0.9150
No improvement (1/15).
Epoch [13/100] | train_loss 0.1347 | train_acc 0.9580 | val_loss 0.4394 | val_acc 0.9117
Epoch [14/100] | train_loss 0.1187 | train_acc 0.9597 | val_loss 0.4051 | val_acc 0.9167
No improvement (1/15).
Epoch [15/100] | train_loss 0.1088 | train_acc 0.9637 | val_loss 0.4326 | val_acc 0.9017
No improvement (2/15).
Epoch [16/100] | train_loss 0.1176 | train_acc 0.9590 | val_loss 0.4145 | val_acc 0.9017
No improvement (3/15).
Epoch [17/100] | train_loss 0.1142 | train_acc 0.9603 | val_loss 0.3905 | val_acc 0.9117
No improvement (4/15).
Epoch [18/100] | train_loss 0.1145 | train_acc 0.9613 | val_loss 0.3528 | val_acc 0.9167
No improvement (5/15).
Epoch [19/100] | train_loss 0.1022 | train_acc 0.9630 | val_loss 0.3771 | val_acc 0.9133
No improvement (6/15).
Epoch [20/100] | train_loss 0.1040 | train_acc 0.9627 | val_loss 0.3911 | val_acc 0.9167
Epoch [21/100] | train_loss 0.0884 | train_acc 0.9700 | val_loss 0.3366 | val_acc 0.9300
Epoch [22/100] | train_loss 0.0894 | train_acc 0.9697 | val_loss 0.3488 | val_acc 0.9317
No improvement (1/15).
Epoch [23/100] | train_loss 0.0896 | train_acc 0.9730 | val_loss 0.3882 | val_acc 0.9267
Epoch [24/100] | train_loss 0.0899 | train_acc 0.9727 | val_loss 0.3591 | val_acc 0.9333
No improvement (1/15).
Epoch [25/100] | train_loss 0.0845 | train_acc 0.9753 | val_loss 0.3725 | val_acc 0.9300
No improvement (2/15).
Epoch [26/100] | train_loss 0.0805 | train_acc 0.9777 | val_loss 0.3615 | val_acc 0.9317
No improvement (3/15).
Epoch [27/100] | train_loss 0.0827 | train_acc 0.9747 | val_loss 0.3596 | val_acc 0.9300
No improvement (4/15).
Epoch [28/100] | train_loss 0.0788 | train_acc 0.9743 | val_loss 0.3654 | val_acc 0.9317
No improvement (5/15).
Epoch [29/100] | train_loss 0.0779 | train_acc 0.9763 | val_loss 0.3672 | val_acc 0.9300
No improvement (6/15).
Epoch [30/100] | train_loss 0.0752 | train_acc 0.9780 | val_loss 0.3573 | val_acc 0.9283
No improvement (7/15).
Epoch [31/100] | train_loss 0.0769 | train_acc 0.9770 | val_loss 0.3632 | val_acc 0.9300
No improvement (8/15).
Epoch [32/100] | train_loss 0.0752 | train_acc 0.9773 | val_loss 0.3585 | val_acc 0.9283
No improvement (9/15).
Epoch [33/100] | train_loss 0.0725 | train_acc 0.9793 | val_loss 0.3788 | val_acc 0.9300
No improvement (10/15).
Epoch [34/100] | train_loss 0.0693 | train_acc 0.9810 | val_loss 0.3896 | val_acc 0.9267wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–„â–ˆâ–„â–ˆâ–â–…â–‚â–â–…â–„â–…â–‡â–ˆâ–†â–ˆâ–…â–…
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–„â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–„â–„â–„â–…â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–‡â–ˆâ–…â–‡â–ˆâ–‡â–‚â–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.93333
wandb:             epoch 38
wandb:         grad/norm 0.76223
wandb:                lr 1e-05
wandb:         train/acc 0.98267
wandb:        train/loss 0.06716
wandb: training_time_sec 3841.92255
wandb:           val/acc 0.92833
wandb:          val/loss 0.40372
wandb: 
wandb: ğŸš€ View run Trial28_[CV-Variation]_L2_H2_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ecrm7801
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_174930-ecrm7801/logs
[I 2026-02-03 18:53:34,292] Trial 28 finished with value: 0.9333333333333333 and parameters: {'nhead': 2, 'num_layers': 2, 'd_model': 128, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0005028304965755059, 'dropout': 0.41429472969251296}. Best is trial 12 with value: 0.96.
wandb: setting up run 01761egm
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_185334-01761egm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial29_[CV-Variation]_L4_H4_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/01761egm
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run Trial29_[CV-Variation]_L4_H4_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/01761egm
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_185334-01761egm/logs
[I 2026-02-03 18:56:56,461] Trial 29 finished with value: 0.0 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 16, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00019078630322622806, 'dropout': 0.2630025381994026}. Best is trial 12 with value: 0.96.
wandb: setting up run l94sb7ha
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_185656-l94sb7ha
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial30_[CV-Variation]_L3_H2_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/l94sb7ha
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–ƒâ–„â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–‚â–‚â–„â–…â–†â–†â–…â–†â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–†â–†â–…â–…â–„â–„â–„â–‚â–‚â–ˆâ–ˆâ–ƒâ–†â–†â–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.90167
wandb:             epoch 64
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.96067
wandb:        train/loss 0.12423
wandb: training_time_sec 6262.48821
wandb:           val/acc 0.89667
wandb:          val/loss 0.48722
wandb: 
wandb: ğŸš€ View run Trial30_[CV-Variation]_L3_H2_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/l94sb7ha
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_185656-l94sb7ha/logs
[I 2026-02-03 20:41:21,439] Trial 30 finished with value: 0.9016666666666666 and parameters: {'nhead': 2, 'num_layers': 3, 'd_model': 64, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.00026906646370956954, 'dropout': 0.34988497099472793}. Best is trial 12 with value: 0.96.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_204121-hes6ef3j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial31_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hes6ef3j

No improvement (11/15).
Epoch [35/100] | train_loss 0.0671 | train_acc 0.9813 | val_loss 0.3931 | val_acc 0.9317
No improvement (12/15).
Epoch [36/100] | train_loss 0.0698 | train_acc 0.9787 | val_loss 0.3966 | val_acc 0.9283
No improvement (13/15).
Epoch [37/100] | train_loss 0.0681 | train_acc 0.9807 | val_loss 0.3928 | val_acc 0.9267
No improvement (14/15).
Epoch [38/100] | train_loss 0.0672 | train_acc 0.9827 | val_loss 0.4037 | val_acc 0.9283
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 28 Finished. Best Val Acc: 93.33%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 29 Failed: CUDA out of memory. Tried to allocate 23.87 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.32 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.96 MiB is allocated by PyTorch, and 3.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6926 | train_acc 0.5310 | val_loss 0.6916 | val_acc 0.5733
Epoch [2/100] | train_loss 0.6722 | train_acc 0.5973 | val_loss 0.6911 | val_acc 0.5917
Epoch [3/100] | train_loss 0.6528 | train_acc 0.6353 | val_loss 0.6701 | val_acc 0.6067
No improvement (1/15).
Epoch [4/100] | train_loss 0.6285 | train_acc 0.6583 | val_loss 0.6516 | val_acc 0.6067
Epoch [5/100] | train_loss 0.5986 | train_acc 0.6870 | val_loss 0.6134 | val_acc 0.6417
Epoch [6/100] | train_loss 0.5585 | train_acc 0.7070 | val_loss 0.5654 | val_acc 0.7117
Epoch [7/100] | train_loss 0.4798 | train_acc 0.7643 | val_loss 0.6113 | val_acc 0.7200
Epoch [8/100] | train_loss 0.3759 | train_acc 0.8380 | val_loss 0.6241 | val_acc 0.7617
Epoch [9/100] | train_loss 0.3451 | train_acc 0.8470 | val_loss 0.6066 | val_acc 0.7767
Epoch [10/100] | train_loss 0.3174 | train_acc 0.8653 | val_loss 0.5331 | val_acc 0.8133
No improvement (1/15).
Epoch [11/100] | train_loss 0.3144 | train_acc 0.8707 | val_loss 0.5354 | val_acc 0.8083
No improvement (2/15).
Epoch [12/100] | train_loss 0.2870 | train_acc 0.8793 | val_loss 0.7832 | val_acc 0.7517
Epoch [13/100] | train_loss 0.2737 | train_acc 0.8883 | val_loss 0.5324 | val_acc 0.8200
No improvement (1/15).
Epoch [14/100] | train_loss 0.2555 | train_acc 0.8983 | val_loss 0.7880 | val_acc 0.7683
Epoch [15/100] | train_loss 0.2433 | train_acc 0.9090 | val_loss 0.5596 | val_acc 0.8367
Epoch [16/100] | train_loss 0.2256 | train_acc 0.9140 | val_loss 0.5183 | val_acc 0.8467
No improvement (1/15).
Epoch [17/100] | train_loss 0.2165 | train_acc 0.9197 | val_loss 0.6286 | val_acc 0.8267
No improvement (2/15).
Epoch [18/100] | train_loss 0.1973 | train_acc 0.9307 | val_loss 0.7200 | val_acc 0.8200
No improvement (3/15).
Epoch [19/100] | train_loss 0.1955 | train_acc 0.9247 | val_loss 0.6889 | val_acc 0.8417
Epoch [20/100] | train_loss 0.1815 | train_acc 0.9343 | val_loss 0.4854 | val_acc 0.8733
No improvement (1/15).
Epoch [21/100] | train_loss 0.1615 | train_acc 0.9447 | val_loss 0.5278 | val_acc 0.8700
Epoch [22/100] | train_loss 0.1550 | train_acc 0.9483 | val_loss 0.5033 | val_acc 0.8750
No improvement (1/15).
Epoch [23/100] | train_loss 0.1529 | train_acc 0.9467 | val_loss 0.5294 | val_acc 0.8750
Epoch [24/100] | train_loss 0.1492 | train_acc 0.9510 | val_loss 0.5167 | val_acc 0.8867
No improvement (1/15).
Epoch [25/100] | train_loss 0.1465 | train_acc 0.9510 | val_loss 0.5428 | val_acc 0.8850
No improvement (2/15).
Epoch [26/100] | train_loss 0.1483 | train_acc 0.9537 | val_loss 0.5694 | val_acc 0.8800
No improvement (3/15).
Epoch [27/100] | train_loss 0.1407 | train_acc 0.9527 | val_loss 0.5731 | val_acc 0.8750
No improvement (4/15).
Epoch [28/100] | train_loss 0.1403 | train_acc 0.9530 | val_loss 0.5699 | val_acc 0.8783
No improvement (5/15).
Epoch [29/100] | train_loss 0.1351 | train_acc 0.9540 | val_loss 0.5669 | val_acc 0.8767
No improvement (6/15).
Epoch [30/100] | train_loss 0.1373 | train_acc 0.9507 | val_loss 0.5617 | val_acc 0.8783
No improvement (7/15).
Epoch [31/100] | train_loss 0.1367 | train_acc 0.9533 | val_loss 0.5677 | val_acc 0.8800
No improvement (8/15).
Epoch [32/100] | train_loss 0.1323 | train_acc 0.9553 | val_loss 0.5815 | val_acc 0.8800
No improvement (9/15).
Epoch [33/100] | train_loss 0.1321 | train_acc 0.9547 | val_loss 0.5511 | val_acc 0.8850
No improvement (10/15).
Epoch [34/100] | train_loss 0.1279 | train_acc 0.9570 | val_loss 0.5499 | val_acc 0.8850
No improvement (11/15).
Epoch [35/100] | train_loss 0.1288 | train_acc 0.9543 | val_loss 0.5453 | val_acc 0.8867
Epoch [36/100] | train_loss 0.1281 | train_acc 0.9560 | val_loss 0.5338 | val_acc 0.8883
No improvement (1/15).
Epoch [37/100] | train_loss 0.1272 | train_acc 0.9587 | val_loss 0.5443 | val_acc 0.8883
No improvement (2/15).
Epoch [38/100] | train_loss 0.1260 | train_acc 0.9573 | val_loss 0.5407 | val_acc 0.8883
Epoch [39/100] | train_loss 0.1271 | train_acc 0.9603 | val_loss 0.5202 | val_acc 0.8917
Epoch [40/100] | train_loss 0.1237 | train_acc 0.9617 | val_loss 0.5164 | val_acc 0.8933
Epoch [41/100] | train_loss 0.1250 | train_acc 0.9593 | val_loss 0.5026 | val_acc 0.8950
No improvement (1/15).
Epoch [42/100] | train_loss 0.1259 | train_acc 0.9570 | val_loss 0.5141 | val_acc 0.8933
No improvement (2/15).
Epoch [43/100] | train_loss 0.1269 | train_acc 0.9577 | val_loss 0.5128 | val_acc 0.8933
No improvement (3/15).
Epoch [44/100] | train_loss 0.1268 | train_acc 0.9553 | val_loss 0.5042 | val_acc 0.8950
Epoch [45/100] | train_loss 0.1253 | train_acc 0.9613 | val_loss 0.4928 | val_acc 0.9000
No improvement (1/15).
Epoch [46/100] | train_loss 0.1236 | train_acc 0.9603 | val_loss 0.4887 | val_acc 0.9000
No improvement (2/15).
Epoch [47/100] | train_loss 0.1236 | train_acc 0.9620 | val_loss 0.4904 | val_acc 0.8983
No improvement (3/15).
Epoch [48/100] | train_loss 0.1234 | train_acc 0.9590 | val_loss 0.5001 | val_acc 0.8967
No improvement (4/15).
Epoch [49/100] | train_loss 0.1244 | train_acc 0.9613 | val_loss 0.4900 | val_acc 0.8967
Epoch [50/100] | train_loss 0.1221 | train_acc 0.9610 | val_loss 0.4916 | val_acc 0.9017
No improvement (1/15).
Epoch [51/100] | train_loss 0.1249 | train_acc 0.9580 | val_loss 0.4870 | val_acc 0.8983
No improvement (2/15).
Epoch [52/100] | train_loss 0.1246 | train_acc 0.9603 | val_loss 0.4915 | val_acc 0.8967
No improvement (3/15).
Epoch [53/100] | train_loss 0.1219 | train_acc 0.9597 | val_loss 0.4891 | val_acc 0.8967
No improvement (4/15).
Epoch [54/100] | train_loss 0.1233 | train_acc 0.9603 | val_loss 0.4866 | val_acc 0.8983
No improvement (5/15).
Epoch [55/100] | train_loss 0.1234 | train_acc 0.9607 | val_loss 0.4930 | val_acc 0.8967
No improvement (6/15).
Epoch [56/100] | train_loss 0.1232 | train_acc 0.9613 | val_loss 0.4912 | val_acc 0.9017
No improvement (7/15).
Epoch [57/100] | train_loss 0.1205 | train_acc 0.9613 | val_loss 0.4899 | val_acc 0.8967
No improvement (8/15).
Epoch [58/100] | train_loss 0.1219 | train_acc 0.9633 | val_loss 0.4882 | val_acc 0.8983
No improvement (9/15).
Epoch [59/100] | train_loss 0.1206 | train_acc 0.9603 | val_loss 0.4865 | val_acc 0.8983
No improvement (10/15).
Epoch [60/100] | train_loss 0.1235 | train_acc 0.9617 | val_loss 0.4878 | val_acc 0.8983
No improvement (11/15).
Epoch [61/100] | train_loss 0.1223 | train_acc 0.9610 | val_loss 0.4890 | val_acc 0.8983
No improvement (12/15).
Epoch [62/100] | train_loss 0.1191 | train_acc 0.9603 | val_loss 0.4857 | val_acc 0.8983
No improvement (13/15).
Epoch [63/100] | train_loss 0.1217 | train_acc 0.9603 | val_loss 0.4860 | val_acc 0.8983
No improvement (14/15).
Epoch [64/100] | train_loss 0.1242 | train_acc 0.9607 | val_loss 0.4872 | val_acc 0.8967
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 30 Finished. Best Val Acc: 90.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7027 | train_acc 0.5323 | val_loss 0.6754 | val_acc 0.6083
No improvement (1/15).
Epoch [2/100] | train_loss 0.6102 | train_acc 0.6483 | val_loss 0.7059 | val_acc 0.5833wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ƒâ–ƒâ–â–ƒâ–â–â–â–â–â–ƒâ–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95
wandb:             epoch 27
wandb:         grad/norm 1.0
wandb:                lr 6e-05
wandb:         train/acc 0.97567
wandb:        train/loss 0.07419
wandb: training_time_sec 6263.01879
wandb:           val/acc 0.93333
wandb:          val/loss 0.27214
wandb: 
wandb: ğŸš€ View run Trial31_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hes6ef3j
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_204121-hes6ef3j/logs
[I 2026-02-03 22:25:46,484] Trial 31 finished with value: 0.95 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0009759095444805757, 'dropout': 0.4969100121325871}. Best is trial 12 with value: 0.96.
wandb: setting up run pnecajtf
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260203_222546-pnecajtf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial32_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pnecajtf
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 47-48
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–†â–ƒâ–†â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–â–„
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–„â–â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95
wandb:             epoch 27
wandb:         grad/norm 0.54962
wandb:                lr 7e-05
wandb:         train/acc 0.983
wandb:        train/loss 0.05011
wandb: training_time_sec 6282.57128
wandb:           val/acc 0.94333
wandb:          val/loss 0.27769
wandb: 
wandb: ğŸš€ View run Trial32_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pnecajtf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260203_222546-pnecajtf/logs
[I 2026-02-04 00:10:31,212] Trial 32 finished with value: 0.95 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0011260773096785444, 'dropout': 0.49966928327266585}. Best is trial 12 with value: 0.96.
wandb: setting up run tpzb46fc
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_001031-tpzb46fc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial33_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tpzb46fc

Epoch [3/100] | train_loss 0.3211 | train_acc 0.8587 | val_loss 0.3539 | val_acc 0.8533
Epoch [4/100] | train_loss 0.2557 | train_acc 0.8987 | val_loss 0.3314 | val_acc 0.8867
Epoch [5/100] | train_loss 0.1997 | train_acc 0.9193 | val_loss 0.1951 | val_acc 0.9433
No improvement (1/15).
Epoch [6/100] | train_loss 0.1589 | train_acc 0.9373 | val_loss 0.3697 | val_acc 0.8817
No improvement (2/15).
Epoch [7/100] | train_loss 0.1560 | train_acc 0.9407 | val_loss 0.2074 | val_acc 0.9350
Epoch [8/100] | train_loss 0.1562 | train_acc 0.9440 | val_loss 0.1925 | val_acc 0.9467
No improvement (1/15).
Epoch [9/100] | train_loss 0.1232 | train_acc 0.9533 | val_loss 0.1974 | val_acc 0.9350
Epoch [10/100] | train_loss 0.0980 | train_acc 0.9660 | val_loss 0.1904 | val_acc 0.9483
No improvement (1/15).
Epoch [11/100] | train_loss 0.1048 | train_acc 0.9617 | val_loss 0.2024 | val_acc 0.9400
No improvement (2/15).
Epoch [12/100] | train_loss 0.1025 | train_acc 0.9640 | val_loss 0.3216 | val_acc 0.9067
Epoch [13/100] | train_loss 0.1166 | train_acc 0.9563 | val_loss 0.1938 | val_acc 0.9500
No improvement (1/15).
Epoch [14/100] | train_loss 0.0910 | train_acc 0.9687 | val_loss 0.2345 | val_acc 0.9383
No improvement (2/15).
Epoch [15/100] | train_loss 0.0878 | train_acc 0.9693 | val_loss 0.1835 | val_acc 0.9450
No improvement (3/15).
Epoch [16/100] | train_loss 0.0765 | train_acc 0.9723 | val_loss 0.2042 | val_acc 0.9467
No improvement (4/15).
Epoch [17/100] | train_loss 0.0817 | train_acc 0.9700 | val_loss 0.2255 | val_acc 0.9417
No improvement (5/15).
Epoch [18/100] | train_loss 0.0736 | train_acc 0.9743 | val_loss 0.2185 | val_acc 0.9367
No improvement (6/15).
Epoch [19/100] | train_loss 0.0791 | train_acc 0.9733 | val_loss 0.2331 | val_acc 0.9367
No improvement (7/15).
Epoch [20/100] | train_loss 0.0809 | train_acc 0.9720 | val_loss 0.2461 | val_acc 0.9383
No improvement (8/15).
Epoch [21/100] | train_loss 0.0802 | train_acc 0.9720 | val_loss 0.2517 | val_acc 0.9400
No improvement (9/15).
Epoch [22/100] | train_loss 0.0728 | train_acc 0.9730 | val_loss 0.2623 | val_acc 0.9367
No improvement (10/15).
Epoch [23/100] | train_loss 0.0789 | train_acc 0.9737 | val_loss 0.2622 | val_acc 0.9383
No improvement (11/15).
Epoch [24/100] | train_loss 0.0774 | train_acc 0.9737 | val_loss 0.2719 | val_acc 0.9383
No improvement (12/15).
Epoch [25/100] | train_loss 0.0766 | train_acc 0.9737 | val_loss 0.2575 | val_acc 0.9367
No improvement (13/15).
Epoch [26/100] | train_loss 0.0844 | train_acc 0.9710 | val_loss 0.2707 | val_acc 0.9400
No improvement (14/15).
Epoch [27/100] | train_loss 0.0742 | train_acc 0.9757 | val_loss 0.2721 | val_acc 0.9333
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 31 Finished. Best Val Acc: 95.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7078 | train_acc 0.5567 | val_loss 0.6771 | val_acc 0.5917
Epoch [2/100] | train_loss 0.6520 | train_acc 0.6190 | val_loss 0.5795 | val_acc 0.6700
Epoch [3/100] | train_loss 0.4113 | train_acc 0.8107 | val_loss 0.3564 | val_acc 0.8717
Epoch [4/100] | train_loss 0.1739 | train_acc 0.9307 | val_loss 0.1600 | val_acc 0.9417
No improvement (1/15).
Epoch [5/100] | train_loss 0.1816 | train_acc 0.9340 | val_loss 0.1657 | val_acc 0.9417
Epoch [6/100] | train_loss 0.1652 | train_acc 0.9437 | val_loss 0.1905 | val_acc 0.9450
No improvement (1/15).
Epoch [7/100] | train_loss 0.1322 | train_acc 0.9527 | val_loss 0.2473 | val_acc 0.9267
No improvement (2/15).
Epoch [8/100] | train_loss 0.1167 | train_acc 0.9557 | val_loss 0.2793 | val_acc 0.9283
Epoch [9/100] | train_loss 0.0954 | train_acc 0.9653 | val_loss 0.2003 | val_acc 0.9467
No improvement (1/15).
Epoch [10/100] | train_loss 0.0931 | train_acc 0.9653 | val_loss 0.2153 | val_acc 0.9433
Epoch [11/100] | train_loss 0.0849 | train_acc 0.9703 | val_loss 0.2110 | val_acc 0.9483
No improvement (1/15).
Epoch [12/100] | train_loss 0.0846 | train_acc 0.9687 | val_loss 0.1993 | val_acc 0.9467
Epoch [13/100] | train_loss 0.0779 | train_acc 0.9723 | val_loss 0.2492 | val_acc 0.9500
No improvement (1/15).
Epoch [14/100] | train_loss 0.0945 | train_acc 0.9663 | val_loss 0.2074 | val_acc 0.9467
No improvement (2/15).
Epoch [15/100] | train_loss 0.0784 | train_acc 0.9720 | val_loss 0.2206 | val_acc 0.9433
No improvement (3/15).
Epoch [16/100] | train_loss 0.0765 | train_acc 0.9720 | val_loss 0.2326 | val_acc 0.9400
No improvement (4/15).
Epoch [17/100] | train_loss 0.0762 | train_acc 0.9747 | val_loss 0.2333 | val_acc 0.9450
No improvement (5/15).
Epoch [18/100] | train_loss 0.0802 | train_acc 0.9720 | val_loss 0.2329 | val_acc 0.9467
No improvement (6/15).
Epoch [19/100] | train_loss 0.0702 | train_acc 0.9760 | val_loss 0.2418 | val_acc 0.9400
No improvement (7/15).
Epoch [20/100] | train_loss 0.0573 | train_acc 0.9840 | val_loss 0.2647 | val_acc 0.9367
No improvement (8/15).
Epoch [21/100] | train_loss 0.0564 | train_acc 0.9827 | val_loss 0.2718 | val_acc 0.9350
No improvement (9/15).
Epoch [22/100] | train_loss 0.0543 | train_acc 0.9823 | val_loss 0.2494 | val_acc 0.9450
No improvement (10/15).
Epoch [23/100] | train_loss 0.0553 | train_acc 0.9833 | val_loss 0.3052 | val_acc 0.9333
No improvement (11/15).
Epoch [24/100] | train_loss 0.0547 | train_acc 0.9830 | val_loss 0.3114 | val_acc 0.9333
No improvement (12/15).
Epoch [25/100] | train_loss 0.0547 | train_acc 0.9827 | val_loss 0.3133 | val_acc 0.9350
No improvement (13/15).
Epoch [26/100] | train_loss 0.0502 | train_acc 0.9847 | val_loss 0.2728 | val_acc 0.9417
No improvement (14/15).
Epoch [27/100] | train_loss 0.0501 | train_acc 0.9830 | val_loss 0.2777 | val_acc 0.9433
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 32 Finished. Best Val Acc: 95.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6887 | train_acc 0.5433 | val_loss 0.7076 | val_acc 0.5667
Epoch [2/100] | train_loss 0.6681 | train_acc 0.5960 | val_loss 0.6706 | val_acc 0.6133
Epoch [3/100] | train_loss 0.5773 | train_acc 0.6897 | val_loss 0.4292 | val_acc 0.8050
Epoch [4/100] | train_loss 0.3217 | train_acc 0.8630 | val_loss 0.4818 | val_acc 0.8100
Epoch [5/100] | train_loss 0.1901 | train_acc 0.9273 | val_loss 0.3365 | val_acc 0.8950
Epoch [6/100] | train_loss 0.1985 | train_acc 0.9220 | val_loss 0.1855 | val_acc 0.9367
Epoch [7/100] | train_loss 0.1844 | train_acc 0.9283 | val_loss 0.1973 | val_acc 0.9383
No improvement (1/15).
Epoch [8/100] | train_loss 0.1216 | train_acc 0.9570 | val_loss 0.2245 | val_acc 0.9350
No improvement (2/15).
Epoch [9/100] | train_loss 0.1146 | train_acc 0.9597 | val_loss 0.3205 | val_acc 0.9150
No improvement (3/15).
Epoch [10/100] | train_loss 0.1106 | train_acc 0.9590 | val_loss 0.2745 | val_acc 0.9233
No improvement (4/15).
Epoch [11/100] | train_loss 0.1149 | train_acc 0.9583 | val_loss 0.2540 | val_acc 0.9267
Epoch [12/100] | train_loss 0.1020 | train_acc 0.9620 | val_loss 0.2487 | val_acc 0.9450
Epoch [13/100] | train_loss 0.1158 | train_acc 0.9553 | val_loss 0.2104 | val_acc 0.9500
No improvement (1/15).
Epoch [14/100] | train_loss 0.0928 | train_acc 0.9637 | val_loss 0.2448 | val_acc 0.9433
No improvement (2/15).
Epoch [15/100] | train_loss 0.0908 | train_acc 0.9680 | val_loss 0.2387 | val_acc 0.9400
No improvement (3/15).
Epoch [16/100] | train_loss 0.0882 | train_acc 0.9677 | val_loss 0.2473 | val_acc 0.9383
No improvement (4/15).
Epoch [17/100] | train_loss 0.0874 | train_acc 0.9683 | val_loss 0.2493 | val_acc 0.9483
No improvement (5/15).
Epoch [18/100] | train_loss 0.0844 | train_acc 0.9687 | val_loss 0.2515 | val_acc 0.9417
No improvement (6/15).
Epoch [19/100] | train_loss 0.0838 | train_acc 0.9690 | val_loss 0.2787 | val_acc 0.9317
No improvement (7/15).
Epoch [20/100] | train_loss 0.0845 | train_acc 0.9710 | val_loss 0.2286 | val_acc 0.9433
No improvement (8/15).
Epoch [21/100] | train_loss 0.0817 | train_acc 0.9710 | val_loss 0.2365 | val_acc 0.9367
No improvement (9/15).
Epoch [22/100] | train_loss 0.0810 | train_acc 0.9710 | val_loss 0.2313 | val_acc 0.9367
No improvement (10/15).
Epoch [23/100] | train_loss 0.0812 | train_acc 0.9713 | val_loss 0.2439 | val_acc 0.9417
No improvement (11/15).
Epoch [24/100] | train_loss 0.0829 | train_acc 0.9713 | val_loss 0.2394 | val_acc 0.9417wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–„â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–„â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–„â–…â–ƒâ–â–â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95
wandb:             epoch 27
wandb:         grad/norm 0.7895
wandb:                lr 4e-05
wandb:         train/acc 0.967
wandb:        train/loss 0.08022
wandb: training_time_sec 6288.2398
wandb:           val/acc 0.94333
wandb:          val/loss 0.25722
wandb: 
wandb: ğŸš€ View run Trial33_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tpzb46fc
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_001031-tpzb46fc/logs
[I 2026-02-04 01:55:21,387] Trial 33 finished with value: 0.95 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0005966220652038791, 'dropout': 0.45746388503785246}. Best is trial 12 with value: 0.96.
wandb: setting up run rk02djcq
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_015521-rk02djcq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial34_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rk02djcq
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 75-76
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:    val/acc â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–â–â–ˆâ–â–‚â–ˆâ–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡
wandb:   val/loss â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–„â–ˆâ–†â–†â–…â–…â–…â–…â–ƒâ–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.57333
wandb:             epoch 39
wandb:         grad/norm 1.0
wandb:                lr 2e-05
wandb:         train/acc 0.631
wandb:        train/loss 0.6453
wandb: training_time_sec 8972.21354
wandb:           val/acc 0.55667
wandb:          val/loss 0.68265
wandb: 
wandb: ğŸš€ View run Trial34_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rk02djcq
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_015521-rk02djcq/logs
[I 2026-02-04 04:24:55,812] Trial 34 finished with value: 0.5733333333333334 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': True, 'lr': 0.0007892011722190755, 'dropout': 0.3867737205572347}. Best is trial 12 with value: 0.96.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_042456-b9bp4zcy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial35_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/b9bp4zcy
wandb: uploading data; updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 52-53
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–â–‚â–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–‡â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–‚â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–†â–†â–ƒâ–…â–„â–‚â–â–ƒâ–„â–‚â–â–â–â–â–ƒâ–â–‚â–â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–…â–„â–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94
wandb:             epoch 30
wandb:         grad/norm 1.0
wandb:                lr 9e-05
wandb:         train/acc 0.97867
wandb:        train/loss 0.06379
wandb: training_time_sec 6975.10585
wandb:           val/acc 0.92333
wandb:          val/loss 0.38135
wandb: 
wandb: ğŸš€ View run Trial35_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/b9bp4zcy
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_042456-b9bp4zcy/logs
[I 2026-02-04 06:21:13,097] Trial 35 finished with value: 0.94 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 32, 'use_conv1d': True, 'lr': 0.001513281104854654, 'dropout': 0.4276020453595391}. Best is trial 12 with value: 0.96.
wandb: setting up run 4w8mnhx3
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_062113-4w8mnhx3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial36_[CV-Variation]_L2_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4w8mnhx3

No improvement (12/15).
Epoch [25/100] | train_loss 0.0800 | train_acc 0.9713 | val_loss 0.2423 | val_acc 0.9367
No improvement (13/15).
Epoch [26/100] | train_loss 0.0876 | train_acc 0.9657 | val_loss 0.2798 | val_acc 0.9367
No improvement (14/15).
Epoch [27/100] | train_loss 0.0802 | train_acc 0.9670 | val_loss 0.2572 | val_acc 0.9433
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 33 Finished. Best Val Acc: 95.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7246 | train_acc 0.4950 | val_loss 0.6936 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6934 | train_acc 0.5143 | val_loss 0.6958 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6937 | train_acc 0.5020 | val_loss 0.6952 | val_acc 0.4900
No improvement (3/15).
Epoch [4/100] | train_loss 0.6937 | train_acc 0.5040 | val_loss 0.6939 | val_acc 0.4900
No improvement (4/15).
Epoch [5/100] | train_loss 0.6933 | train_acc 0.5123 | val_loss 0.6947 | val_acc 0.4900
No improvement (5/15).
Epoch [6/100] | train_loss 0.6934 | train_acc 0.5070 | val_loss 0.6940 | val_acc 0.4900
No improvement (6/15).
Epoch [7/100] | train_loss 0.6932 | train_acc 0.5047 | val_loss 0.6939 | val_acc 0.4900
No improvement (7/15).
Epoch [8/100] | train_loss 0.6931 | train_acc 0.5167 | val_loss 0.6938 | val_acc 0.4900
Epoch [9/100] | train_loss 0.6930 | train_acc 0.5117 | val_loss 0.6930 | val_acc 0.5100
No improvement (1/15).
Epoch [10/100] | train_loss 0.6930 | train_acc 0.5123 | val_loss 0.6931 | val_acc 0.5100
No improvement (2/15).
Epoch [11/100] | train_loss 0.6929 | train_acc 0.5097 | val_loss 0.6931 | val_acc 0.5100
No improvement (3/15).
Epoch [12/100] | train_loss 0.6928 | train_acc 0.5077 | val_loss 0.6931 | val_acc 0.5100
No improvement (4/15).
Epoch [13/100] | train_loss 0.6928 | train_acc 0.5073 | val_loss 0.6930 | val_acc 0.5100
No improvement (5/15).
Epoch [14/100] | train_loss 0.6929 | train_acc 0.5117 | val_loss 0.6931 | val_acc 0.5100
No improvement (6/15).
Epoch [15/100] | train_loss 0.6931 | train_acc 0.5153 | val_loss 0.6937 | val_acc 0.4900
No improvement (7/15).
Epoch [16/100] | train_loss 0.6929 | train_acc 0.5183 | val_loss 0.6941 | val_acc 0.4900
No improvement (8/15).
Epoch [17/100] | train_loss 0.6928 | train_acc 0.5183 | val_loss 0.6939 | val_acc 0.4900
No improvement (9/15).
Epoch [18/100] | train_loss 0.6927 | train_acc 0.5183 | val_loss 0.6938 | val_acc 0.4900
No improvement (10/15).
Epoch [19/100] | train_loss 0.6926 | train_acc 0.5183 | val_loss 0.6933 | val_acc 0.4900
No improvement (11/15).
Epoch [20/100] | train_loss 0.6926 | train_acc 0.5237 | val_loss 0.6930 | val_acc 0.4900
No improvement (12/15).
Epoch [21/100] | train_loss 0.6922 | train_acc 0.5207 | val_loss 0.6934 | val_acc 0.4900
Epoch [22/100] | train_loss 0.6917 | train_acc 0.5200 | val_loss 0.6919 | val_acc 0.5700
No improvement (1/15).
Epoch [23/100] | train_loss 0.6912 | train_acc 0.5430 | val_loss 0.6905 | val_acc 0.4850
No improvement (2/15).
Epoch [24/100] | train_loss 0.6900 | train_acc 0.5300 | val_loss 0.6887 | val_acc 0.4950
Epoch [25/100] | train_loss 0.6844 | train_acc 0.5603 | val_loss 0.6870 | val_acc 0.5733
No improvement (1/15).
Epoch [26/100] | train_loss 0.6757 | train_acc 0.5910 | val_loss 0.6782 | val_acc 0.5583
No improvement (2/15).
Epoch [27/100] | train_loss 0.6657 | train_acc 0.6113 | val_loss 0.6763 | val_acc 0.5600
No improvement (3/15).
Epoch [28/100] | train_loss 0.6592 | train_acc 0.6180 | val_loss 0.7005 | val_acc 0.5517
No improvement (4/15).
Epoch [29/100] | train_loss 0.6580 | train_acc 0.6143 | val_loss 0.7311 | val_acc 0.5517
No improvement (5/15).
Epoch [30/100] | train_loss 0.6579 | train_acc 0.6130 | val_loss 0.7163 | val_acc 0.5483
No improvement (6/15).
Epoch [31/100] | train_loss 0.6555 | train_acc 0.6170 | val_loss 0.7130 | val_acc 0.5500
No improvement (7/15).
Epoch [32/100] | train_loss 0.6542 | train_acc 0.6163 | val_loss 0.7105 | val_acc 0.5517
No improvement (8/15).
Epoch [33/100] | train_loss 0.6525 | train_acc 0.6183 | val_loss 0.7076 | val_acc 0.5517
No improvement (9/15).
Epoch [34/100] | train_loss 0.6511 | train_acc 0.6203 | val_loss 0.7077 | val_acc 0.5517
No improvement (10/15).
Epoch [35/100] | train_loss 0.6498 | train_acc 0.6210 | val_loss 0.7065 | val_acc 0.5550
No improvement (11/15).
Epoch [36/100] | train_loss 0.6473 | train_acc 0.6267 | val_loss 0.6886 | val_acc 0.5583
No improvement (12/15).
Epoch [37/100] | train_loss 0.6462 | train_acc 0.6300 | val_loss 0.6861 | val_acc 0.5583
No improvement (13/15).
Epoch [38/100] | train_loss 0.6460 | train_acc 0.6327 | val_loss 0.6840 | val_acc 0.5567
No improvement (14/15).
Epoch [39/100] | train_loss 0.6453 | train_acc 0.6310 | val_loss 0.6826 | val_acc 0.5567
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 34 Finished. Best Val Acc: 57.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7717 | train_acc 0.5057 | val_loss 0.7041 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6951 | train_acc 0.4960 | val_loss 0.6951 | val_acc 0.4900
Epoch [3/100] | train_loss 0.6711 | train_acc 0.5663 | val_loss 0.6905 | val_acc 0.5717
Epoch [4/100] | train_loss 0.6054 | train_acc 0.6317 | val_loss 0.5456 | val_acc 0.7183
Epoch [5/100] | train_loss 0.3829 | train_acc 0.8250 | val_loss 0.5305 | val_acc 0.8333
Epoch [6/100] | train_loss 0.2917 | train_acc 0.8850 | val_loss 0.3264 | val_acc 0.9033
No improvement (1/15).
Epoch [7/100] | train_loss 0.2439 | train_acc 0.9083 | val_loss 0.4548 | val_acc 0.8650
No improvement (2/15).
Epoch [8/100] | train_loss 0.1570 | train_acc 0.9387 | val_loss 0.4195 | val_acc 0.8817
Epoch [9/100] | train_loss 0.1474 | train_acc 0.9437 | val_loss 0.2731 | val_acc 0.9100
Epoch [10/100] | train_loss 0.1238 | train_acc 0.9513 | val_loss 0.2234 | val_acc 0.9333
No improvement (1/15).
Epoch [11/100] | train_loss 0.1128 | train_acc 0.9607 | val_loss 0.3658 | val_acc 0.9050
No improvement (2/15).
Epoch [12/100] | train_loss 0.1096 | train_acc 0.9607 | val_loss 0.3941 | val_acc 0.8850
No improvement (3/15).
Epoch [13/100] | train_loss 0.1236 | train_acc 0.9563 | val_loss 0.2777 | val_acc 0.9267
No improvement (4/15).
Epoch [14/100] | train_loss 0.1051 | train_acc 0.9630 | val_loss 0.2084 | val_acc 0.9250
Epoch [15/100] | train_loss 0.0911 | train_acc 0.9663 | val_loss 0.1885 | val_acc 0.9383
Epoch [16/100] | train_loss 0.0980 | train_acc 0.9660 | val_loss 0.1975 | val_acc 0.9400
No improvement (1/15).
Epoch [17/100] | train_loss 0.0936 | train_acc 0.9677 | val_loss 0.2135 | val_acc 0.9383
No improvement (2/15).
Epoch [18/100] | train_loss 0.0993 | train_acc 0.9663 | val_loss 0.3434 | val_acc 0.9183
No improvement (3/15).
Epoch [19/100] | train_loss 0.0967 | train_acc 0.9677 | val_loss 0.2145 | val_acc 0.9367
No improvement (4/15).
Epoch [20/100] | train_loss 0.0882 | train_acc 0.9697 | val_loss 0.2824 | val_acc 0.9200
No improvement (5/15).
Epoch [21/100] | train_loss 0.0924 | train_acc 0.9680 | val_loss 0.2177 | val_acc 0.9400
No improvement (6/15).
Epoch [22/100] | train_loss 0.0689 | train_acc 0.9757 | val_loss 0.2017 | val_acc 0.9400
No improvement (7/15).
Epoch [23/100] | train_loss 0.0904 | train_acc 0.9680 | val_loss 0.3003 | val_acc 0.9217
No improvement (8/15).
Epoch [24/100] | train_loss 0.0712 | train_acc 0.9757 | val_loss 0.2689 | val_acc 0.9300
No improvement (9/15).
Epoch [25/100] | train_loss 0.0696 | train_acc 0.9767 | val_loss 0.2332 | val_acc 0.9350
No improvement (10/15).
Epoch [26/100] | train_loss 0.0670 | train_acc 0.9780 | val_loss 0.3564 | val_acc 0.9300
No improvement (11/15).
Epoch [27/100] | train_loss 0.0690 | train_acc 0.9770 | val_loss 0.3436 | val_acc 0.9217
No improvement (12/15).
Epoch [28/100] | train_loss 0.0639 | train_acc 0.9773 | val_loss 0.5028 | val_acc 0.9100
No improvement (13/15).
Epoch [29/100] | train_loss 0.0700 | train_acc 0.9750 | val_loss 0.4273 | val_acc 0.9183
No improvement (14/15).
Epoch [30/100] | train_loss 0.0638 | train_acc 0.9787 | val_loss 0.3814 | val_acc 0.9233
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 35 Finished. Best Val Acc: 94.00%
ğŸ§¹ Memory Cleared
Starting training...wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–„â–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–…â–‚â–ˆâ–‚â–â–‚â–ƒâ–…â–‚â–ˆâ–„â–ˆâ–ˆâ–„
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–†â–†â–†â–…â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–‡
wandb:   val/loss â–ˆâ–„â–ƒâ–ƒâ–…â–†â–‚â–â–â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–…â–†â–†â–†
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94167
wandb:             epoch 30
wandb:         grad/norm 0.53167
wandb:                lr 0.00018
wandb:         train/acc 0.98133
wandb:        train/loss 0.0521
wandb: training_time_sec 3466.70744
wandb:           val/acc 0.89667
wandb:          val/loss 0.48889
wandb: 
wandb: ğŸš€ View run Trial36_[CV-Variation]_L2_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4w8mnhx3
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_062113-4w8mnhx3/logs
[I 2026-02-04 07:19:01,916] Trial 36 finished with value: 0.9416666666666667 and parameters: {'nhead': 4, 'num_layers': 2, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0028690141453396882, 'dropout': 0.47086243188916177}. Best is trial 12 with value: 0.96.
wandb: setting up run w58f7q9y
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_071902-w58f7q9y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial37_[CV-Variation]_L1_H4_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/w58f7q9y

Epoch [1/100] | train_loss 0.6899 | train_acc 0.5687 | val_loss 0.6213 | val_acc 0.6467
Epoch [2/100] | train_loss 0.4786 | train_acc 0.7480 | val_loss 0.3975 | val_acc 0.8333
Epoch [3/100] | train_loss 0.3538 | train_acc 0.8397 | val_loss 0.3241 | val_acc 0.8483
Epoch [4/100] | train_loss 0.2521 | train_acc 0.8967 | val_loss 0.3371 | val_acc 0.8767
No improvement (1/15).
Epoch [5/100] | train_loss 0.2431 | train_acc 0.9003 | val_loss 0.4084 | val_acc 0.8550
No improvement (2/15).
Epoch [6/100] | train_loss 0.2081 | train_acc 0.9187 | val_loss 0.5218 | val_acc 0.7950
Epoch [7/100] | train_loss 0.2504 | train_acc 0.9103 | val_loss 0.2844 | val_acc 0.9067
Epoch [8/100] | train_loss 0.1332 | train_acc 0.9510 | val_loss 0.1968 | val_acc 0.9200
Epoch [9/100] | train_loss 0.1141 | train_acc 0.9583 | val_loss 0.1954 | val_acc 0.9350
No improvement (1/15).
Epoch [10/100] | train_loss 0.1098 | train_acc 0.9637 | val_loss 0.2670 | val_acc 0.9217
No improvement (2/15).
Epoch [11/100] | train_loss 0.1244 | train_acc 0.9553 | val_loss 0.2512 | val_acc 0.9200
No improvement (3/15).
Epoch [12/100] | train_loss 0.1196 | train_acc 0.9580 | val_loss 0.3747 | val_acc 0.8883
No improvement (4/15).
Epoch [13/100] | train_loss 0.1227 | train_acc 0.9547 | val_loss 0.2353 | val_acc 0.9317
No improvement (5/15).
Epoch [14/100] | train_loss 0.0915 | train_acc 0.9643 | val_loss 0.2325 | val_acc 0.9333
No improvement (6/15).
Epoch [15/100] | train_loss 0.0746 | train_acc 0.9773 | val_loss 0.2537 | val_acc 0.9300
Epoch [16/100] | train_loss 0.1043 | train_acc 0.9587 | val_loss 0.2324 | val_acc 0.9417
No improvement (1/15).
Epoch [17/100] | train_loss 0.0827 | train_acc 0.9707 | val_loss 0.2516 | val_acc 0.9333
No improvement (2/15).
Epoch [18/100] | train_loss 0.0772 | train_acc 0.9747 | val_loss 0.2574 | val_acc 0.9367
No improvement (3/15).
Epoch [19/100] | train_loss 0.0879 | train_acc 0.9687 | val_loss 0.2748 | val_acc 0.9300
No improvement (4/15).
Epoch [20/100] | train_loss 0.0637 | train_acc 0.9810 | val_loss 0.2616 | val_acc 0.9383
No improvement (5/15).
Epoch [21/100] | train_loss 0.0644 | train_acc 0.9797 | val_loss 0.2830 | val_acc 0.9283
No improvement (6/15).
Epoch [22/100] | train_loss 0.0574 | train_acc 0.9820 | val_loss 0.2837 | val_acc 0.9367
No improvement (7/15).
Epoch [23/100] | train_loss 0.0568 | train_acc 0.9820 | val_loss 0.2868 | val_acc 0.9350
No improvement (8/15).
Epoch [24/100] | train_loss 0.0570 | train_acc 0.9813 | val_loss 0.3430 | val_acc 0.9233
No improvement (9/15).
Epoch [25/100] | train_loss 0.0528 | train_acc 0.9843 | val_loss 0.3122 | val_acc 0.9350
No improvement (10/15).
Epoch [26/100] | train_loss 0.0531 | train_acc 0.9843 | val_loss 0.4513 | val_acc 0.8917
No improvement (11/15).
Epoch [27/100] | train_loss 0.0489 | train_acc 0.9833 | val_loss 0.4155 | val_acc 0.9000
No improvement (12/15).
Epoch [28/100] | train_loss 0.0513 | train_acc 0.9833 | val_loss 0.5217 | val_acc 0.8750
No improvement (13/15).
Epoch [29/100] | train_loss 0.0508 | train_acc 0.9833 | val_loss 0.5274 | val_acc 0.8817
No improvement (14/15).
Epoch [30/100] | train_loss 0.0521 | train_acc 0.9813 | val_loss 0.4889 | val_acc 0.8967
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 36 Finished. Best Val Acc: 94.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6909 | train_acc 0.5500 | val_loss 0.6820 | val_acc 0.6117
Epoch [2/100] | train_loss 0.6531 | train_acc 0.6437 | val_loss 0.6661 | val_acc 0.6267
No improvement (1/15).
Epoch [3/100] | train_loss 0.6299 | train_acc 0.6490 | val_loss 0.6468 | val_acc 0.6200
Epoch [4/100] | train_loss 0.5885 | train_acc 0.6840 | val_loss 0.5780 | val_acc 0.6933
Epoch [5/100] | train_loss 0.5340 | train_acc 0.7293 | val_loss 0.5683 | val_acc 0.6967
No improvement (1/15).
Epoch [6/100] | train_loss 0.5117 | train_acc 0.7393 | val_loss 0.5798 | val_acc 0.6800
Epoch [7/100] | train_loss 0.4471 | train_acc 0.7893 | val_loss 0.4396 | val_acc 0.7950
Epoch [8/100] | train_loss 0.3809 | train_acc 0.8310 | val_loss 0.4092 | val_acc 0.8083
Epoch [9/100] | train_loss 0.3573 | train_acc 0.8430 | val_loss 0.3856 | val_acc 0.8117
Epoch [10/100] | train_loss 0.3344 | train_acc 0.8513 | val_loss 0.3700 | val_acc 0.8217
Epoch [11/100] | train_loss 0.3139 | train_acc 0.8587 | val_loss 0.3574 | val_acc 0.8333
Epoch [12/100] | train_loss 0.2937 | train_acc 0.8717 | val_loss 0.3527 | val_acc 0.8400
Epoch [13/100] | train_loss 0.2749 | train_acc 0.8843 | val_loss 0.3492 | val_acc 0.8467
No improvement (1/15).
Epoch [14/100] | train_loss 0.2571 | train_acc 0.8933 | val_loss 0.3466 | val_acc 0.8233
No improvement (2/15).
Epoch [15/100] | train_loss 0.2485 | train_acc 0.8947 | val_loss 0.3429 | val_acc 0.8433
No improvement (3/15).
Epoch [16/100] | train_loss 0.2420 | train_acc 0.8973 | val_loss 0.3399 | val_acc 0.8467
No improvement (4/15).
Epoch [17/100] | train_loss 0.2349 | train_acc 0.9007 | val_loss 0.3369 | val_acc 0.8400
No improvement (5/15).
Epoch [18/100] | train_loss 0.2283 | train_acc 0.9053 | val_loss 0.3376 | val_acc 0.8417
No improvement (6/15).
Epoch [19/100] | train_loss 0.2230 | train_acc 0.9080 | val_loss 0.3381 | val_acc 0.8450
Epoch [20/100] | train_loss 0.2140 | train_acc 0.9127 | val_loss 0.3417 | val_acc 0.8483
No improvement (1/15).
Epoch [21/100] | train_loss 0.2113 | train_acc 0.9140 | val_loss 0.3442 | val_acc 0.8483
Epoch [22/100] | train_loss 0.2095 | train_acc 0.9157 | val_loss 0.3462 | val_acc 0.8517
No improvement (1/15).
Epoch [23/100] | train_loss 0.2074 | train_acc 0.9173 | val_loss 0.3463 | val_acc 0.8517
Epoch [24/100] | train_loss 0.2053 | train_acc 0.9173 | val_loss 0.3470 | val_acc 0.8533
Epoch [25/100] | train_loss 0.2031 | train_acc 0.9170 | val_loss 0.3479 | val_acc 0.8550
No improvement (1/15).
Epoch [26/100] | train_loss 0.2017 | train_acc 0.9177 | val_loss 0.3483 | val_acc 0.8550
No improvement (2/15).
Epoch [27/100] | train_loss 0.1990 | train_acc 0.9200 | val_loss 0.3469 | val_acc 0.8550
No improvement (3/15).
Epoch [28/100] | train_loss 0.1988 | train_acc 0.9193 | val_loss 0.3475 | val_acc 0.8533
No improvement (4/15).
Epoch [29/100] | train_loss 0.1978 | train_acc 0.9200 | val_loss 0.3495 | val_acc 0.8533
No improvement (5/15).
Epoch [30/100] | train_loss 0.1967 | train_acc 0.9220 | val_loss 0.3493 | val_acc 0.8533
No improvement (6/15).
Epoch [31/100] | train_loss 0.1967 | train_acc 0.9217 | val_loss 0.3481 | val_acc 0.8533
No improvement (7/15).
Epoch [32/100] | train_loss 0.1961 | train_acc 0.9237 | val_loss 0.3452 | val_acc 0.8550
Epoch [33/100] | train_loss 0.1948 | train_acc 0.9227 | val_loss 0.3451 | val_acc 0.8567
Epoch [34/100] | train_loss 0.1952 | train_acc 0.9203 | val_loss 0.3446 | val_acc 0.8583
No improvement (1/15).
Epoch [35/100] | train_loss 0.1943 | train_acc 0.9237 | val_loss 0.3447 | val_acc 0.8583
No improvement (2/15).
Epoch [36/100] | train_loss 0.1939 | train_acc 0.9247 | val_loss 0.3452 | val_acc 0.8583
No improvement (3/15).
Epoch [37/100] | train_loss 0.1939 | train_acc 0.9243 | val_loss 0.3456 | val_acc 0.8583
Epoch [38/100] | train_loss 0.1943 | train_acc 0.9217 | val_loss 0.3450 | val_acc 0.8633
No improvement (1/15).
Epoch [39/100] | train_loss 0.1933 | train_acc 0.9240 | val_loss 0.3452 | val_acc 0.8633
No improvement (2/15).
Epoch [40/100] | train_loss 0.1928 | train_acc 0.9240 | val_loss 0.3452 | val_acc 0.8633
No improvement (3/15).
Epoch [41/100] | train_loss 0.1935 | train_acc 0.9243 | val_loss 0.3451 | val_acc 0.8633
No improvement (4/15).
Epoch [42/100] | train_loss 0.1924 | train_acc 0.9240 | val_loss 0.3452 | val_acc 0.8617
No improvement (5/15).
Epoch [43/100] | train_loss 0.1932 | train_acc 0.9237 | val_loss 0.3452 | val_acc 0.8633
No improvement (6/15).
Epoch [44/100] | train_loss 0.1922 | train_acc 0.9233 | val_loss 0.3458 | val_acc 0.8600
No improvement (7/15).
Epoch [45/100] | train_loss 0.1916 | train_acc 0.9230 | val_loss 0.3459 | val_acc 0.8583
No improvement (8/15).
Epoch [46/100] | train_loss 0.1918 | train_acc 0.9230 | val_loss 0.3460 | val_acc 0.8583
No improvement (9/15).
Epoch [47/100] | train_loss 0.1921 | train_acc 0.9217 | val_loss 0.3459 | val_acc 0.8583
No improvement (10/15).
Epoch [48/100] | train_loss 0.1915 | train_acc 0.9233 | val_loss 0.3460 | val_acc 0.8583wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 87-88
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–ƒâ–„â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–‡â–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–ƒâ–ƒâ–†â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–‡â–†â–†â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.86333
wandb:             epoch 52
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.92333
wandb:        train/loss 0.191
wandb: training_time_sec 3397.69069
wandb:           val/acc 0.85833
wandb:          val/loss 0.34632
wandb: 
wandb: ğŸš€ View run Trial37_[CV-Variation]_L1_H4_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/w58f7q9y
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_071902-w58f7q9y/logs
[I 2026-02-04 08:15:41,719] Trial 37 finished with value: 0.8633333333333333 and parameters: {'nhead': 4, 'num_layers': 1, 'd_model': 128, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0003439039277105298, 'dropout': 0.3409141688785556}. Best is trial 12 with value: 0.96.
wandb: setting up run ms1rgkhr
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_081542-ms1rgkhr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial38_[CV-Variation]_L4_H8_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ms1rgkhr
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: ğŸš€ View run Trial38_[CV-Variation]_L4_H8_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ms1rgkhr
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_081542-ms1rgkhr/logs
[I 2026-02-04 08:22:22,235] Trial 38 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 32, 'batch_size': 128, 'use_conv1d': True, 'lr': 0.0012147980561788168, 'dropout': 0.3739943731100811}. Best is trial 12 with value: 0.96.
wandb: setting up run sc9pcdcn
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_082222-sc9pcdcn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial39_[CV-Variation]_L1_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sc9pcdcn
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–ƒâ–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–‡â–…â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–‚â–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–‡â–†â–†â–ƒâ–‚â–‚â–‚â–ƒâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.93
wandb:             epoch 30
wandb:         grad/norm 1.0
wandb:                lr 5e-05
wandb:         train/acc 0.96167
wandb:        train/loss 0.10513
wandb: training_time_sec 1728.69512
wandb:           val/acc 0.92667
wandb:          val/loss 0.23808
wandb: 
wandb: ğŸš€ View run Trial39_[CV-Variation]_L1_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sc9pcdcn
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_082222-sc9pcdcn/logs
[I 2026-02-04 08:51:12,914] Trial 39 finished with value: 0.93 and parameters: {'nhead': 4, 'num_layers': 1, 'd_model': 64, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.0008771546198841335, 'dropout': 0.31116455854282254}. Best is trial 12 with value: 0.96.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_085113-6edlso56
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial40_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/6edlso56
wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    val/acc â–â–„â–‡â–‡â–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–ƒâ–ƒâ–‡â–…â–‚â–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 24
wandb:         grad/norm 1.0
wandb:                lr 0.00023
wandb:         train/acc 0.97367
wandb:        train/loss 0.07
wandb: training_time_sec 10453.16283
wandb:           val/acc 0.935
wandb:          val/loss 0.24723
wandb: 
wandb: ğŸš€ View run Trial40_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/6edlso56
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_085113-6edlso56/logs
[I 2026-02-04 11:45:28,197] Trial 40 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0018518774702339696, 'dropout': 0.4050050518663336}. Best is trial 12 with value: 0.96.
wandb: setting up run 3svw1cv7
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_114528-3svw1cv7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial41_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3svw1cv7

No improvement (11/15).
Epoch [49/100] | train_loss 0.1917 | train_acc 0.9247 | val_loss 0.3461 | val_acc 0.8583
No improvement (12/15).
Epoch [50/100] | train_loss 0.1907 | train_acc 0.9240 | val_loss 0.3461 | val_acc 0.8583
No improvement (13/15).
Epoch [51/100] | train_loss 0.1912 | train_acc 0.9227 | val_loss 0.3462 | val_acc 0.8583
No improvement (14/15).
Epoch [52/100] | train_loss 0.1910 | train_acc 0.9233 | val_loss 0.3463 | val_acc 0.8583
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 37 Finished. Best Val Acc: 86.33%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 38 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.22 GiB is free. Including non-PyTorch memory, this process has 378.00 MiB memory in use. Of the allocated memory 219.21 MiB is allocated by PyTorch, and 2.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6881 | train_acc 0.5470 | val_loss 0.6850 | val_acc 0.6000
Epoch [2/100] | train_loss 0.6549 | train_acc 0.6350 | val_loss 0.6497 | val_acc 0.6167
Epoch [3/100] | train_loss 0.6195 | train_acc 0.6720 | val_loss 0.6500 | val_acc 0.6217
Epoch [4/100] | train_loss 0.5769 | train_acc 0.6890 | val_loss 0.5835 | val_acc 0.6600
Epoch [5/100] | train_loss 0.4796 | train_acc 0.7577 | val_loss 0.5749 | val_acc 0.7117
Epoch [6/100] | train_loss 0.3265 | train_acc 0.8517 | val_loss 0.3497 | val_acc 0.8467
Epoch [7/100] | train_loss 0.2251 | train_acc 0.9077 | val_loss 0.2661 | val_acc 0.9017
No improvement (1/15).
Epoch [8/100] | train_loss 0.1814 | train_acc 0.9320 | val_loss 0.3141 | val_acc 0.8867
No improvement (2/15).
Epoch [9/100] | train_loss 0.1652 | train_acc 0.9417 | val_loss 0.2876 | val_acc 0.9000
No improvement (3/15).
Epoch [10/100] | train_loss 0.1614 | train_acc 0.9383 | val_loss 0.3483 | val_acc 0.8850
Epoch [11/100] | train_loss 0.1574 | train_acc 0.9437 | val_loss 0.2472 | val_acc 0.9150
No improvement (1/15).
Epoch [12/100] | train_loss 0.1409 | train_acc 0.9477 | val_loss 0.2824 | val_acc 0.9017
No improvement (2/15).
Epoch [13/100] | train_loss 0.1365 | train_acc 0.9520 | val_loss 0.2718 | val_acc 0.9100
Epoch [14/100] | train_loss 0.1262 | train_acc 0.9573 | val_loss 0.2353 | val_acc 0.9217
No improvement (1/15).
Epoch [15/100] | train_loss 0.1241 | train_acc 0.9557 | val_loss 0.2377 | val_acc 0.9167
Epoch [16/100] | train_loss 0.1193 | train_acc 0.9570 | val_loss 0.2246 | val_acc 0.9300
No improvement (1/15).
Epoch [17/100] | train_loss 0.1241 | train_acc 0.9547 | val_loss 0.2399 | val_acc 0.9233
No improvement (2/15).
Epoch [18/100] | train_loss 0.1122 | train_acc 0.9600 | val_loss 0.2204 | val_acc 0.9283
No improvement (3/15).
Epoch [19/100] | train_loss 0.1195 | train_acc 0.9567 | val_loss 0.2344 | val_acc 0.9217
No improvement (4/15).
Epoch [20/100] | train_loss 0.1112 | train_acc 0.9607 | val_loss 0.2256 | val_acc 0.9267
No improvement (5/15).
Epoch [21/100] | train_loss 0.1090 | train_acc 0.9620 | val_loss 0.2315 | val_acc 0.9200
No improvement (6/15).
Epoch [22/100] | train_loss 0.1061 | train_acc 0.9643 | val_loss 0.2290 | val_acc 0.9217
No improvement (7/15).
Epoch [23/100] | train_loss 0.1061 | train_acc 0.9623 | val_loss 0.2333 | val_acc 0.9217
No improvement (8/15).
Epoch [24/100] | train_loss 0.1047 | train_acc 0.9647 | val_loss 0.2422 | val_acc 0.9183
No improvement (9/15).
Epoch [25/100] | train_loss 0.1054 | train_acc 0.9657 | val_loss 0.2503 | val_acc 0.9117
No improvement (10/15).
Epoch [26/100] | train_loss 0.1059 | train_acc 0.9617 | val_loss 0.2364 | val_acc 0.9283
No improvement (11/15).
Epoch [27/100] | train_loss 0.1056 | train_acc 0.9597 | val_loss 0.2379 | val_acc 0.9283
No improvement (12/15).
Epoch [28/100] | train_loss 0.1051 | train_acc 0.9607 | val_loss 0.2392 | val_acc 0.9267
No improvement (13/15).
Epoch [29/100] | train_loss 0.1051 | train_acc 0.9603 | val_loss 0.2381 | val_acc 0.9283
No improvement (14/15).
Epoch [30/100] | train_loss 0.1051 | train_acc 0.9617 | val_loss 0.2381 | val_acc 0.9267
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 39 Finished. Best Val Acc: 93.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7215 | train_acc 0.5377 | val_loss 0.6665 | val_acc 0.5683
Epoch [2/100] | train_loss 0.5988 | train_acc 0.6587 | val_loss 0.4670 | val_acc 0.7583
Epoch [3/100] | train_loss 0.3259 | train_acc 0.8597 | val_loss 0.2772 | val_acc 0.9067
No improvement (1/15).
Epoch [4/100] | train_loss 0.2489 | train_acc 0.9000 | val_loss 0.2725 | val_acc 0.9067
No improvement (2/15).
Epoch [5/100] | train_loss 0.1551 | train_acc 0.9347 | val_loss 0.5772 | val_acc 0.8317
No improvement (3/15).
Epoch [6/100] | train_loss 0.1713 | train_acc 0.9370 | val_loss 0.4672 | val_acc 0.8567
Epoch [7/100] | train_loss 0.1335 | train_acc 0.9497 | val_loss 0.2046 | val_acc 0.9417
No improvement (1/15).
Epoch [8/100] | train_loss 0.1259 | train_acc 0.9523 | val_loss 0.1955 | val_acc 0.9417
Epoch [9/100] | train_loss 0.1051 | train_acc 0.9563 | val_loss 0.1626 | val_acc 0.9550
Epoch [10/100] | train_loss 0.1071 | train_acc 0.9587 | val_loss 0.1673 | val_acc 0.9600
No improvement (1/15).
Epoch [11/100] | train_loss 0.1016 | train_acc 0.9613 | val_loss 0.1671 | val_acc 0.9517
No improvement (2/15).
Epoch [12/100] | train_loss 0.0894 | train_acc 0.9640 | val_loss 0.1700 | val_acc 0.9550
No improvement (3/15).
Epoch [13/100] | train_loss 0.0836 | train_acc 0.9693 | val_loss 0.1959 | val_acc 0.9517
No improvement (4/15).
Epoch [14/100] | train_loss 0.0740 | train_acc 0.9723 | val_loss 0.1621 | val_acc 0.9600
No improvement (5/15).
Epoch [15/100] | train_loss 0.0778 | train_acc 0.9723 | val_loss 0.1467 | val_acc 0.9550
No improvement (6/15).
Epoch [16/100] | train_loss 0.0940 | train_acc 0.9640 | val_loss 0.1919 | val_acc 0.9333
No improvement (7/15).
Epoch [17/100] | train_loss 0.0964 | train_acc 0.9620 | val_loss 0.1646 | val_acc 0.9467
No improvement (8/15).
Epoch [18/100] | train_loss 0.1331 | train_acc 0.9483 | val_loss 0.1374 | val_acc 0.9600
No improvement (9/15).
Epoch [19/100] | train_loss 0.0855 | train_acc 0.9653 | val_loss 0.1465 | val_acc 0.9517
No improvement (10/15).
Epoch [20/100] | train_loss 0.0659 | train_acc 0.9747 | val_loss 0.2067 | val_acc 0.9450
No improvement (11/15).
Epoch [21/100] | train_loss 0.0707 | train_acc 0.9737 | val_loss 0.1923 | val_acc 0.9533
No improvement (12/15).
Epoch [22/100] | train_loss 0.0654 | train_acc 0.9767 | val_loss 0.2026 | val_acc 0.9550
No improvement (13/15).
Epoch [23/100] | train_loss 0.0685 | train_acc 0.9743 | val_loss 0.1981 | val_acc 0.9533
No improvement (14/15).
Epoch [24/100] | train_loss 0.0700 | train_acc 0.9737 | val_loss 0.2472 | val_acc 0.9350
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 40 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7424 | train_acc 0.5770 | val_loss 0.7146 | val_acc 0.5717
Epoch [2/100] | train_loss 0.5045 | train_acc 0.7470 | val_loss 0.4641 | val_acc 0.8233
Epoch [3/100] | train_loss 0.3553 | train_acc 0.8537 | val_loss 0.4194 | val_acc 0.8683
Epoch [4/100] | train_loss 0.2550 | train_acc 0.8967 | val_loss 0.3736 | val_acc 0.8783
No improvement (1/15).
Epoch [5/100] | train_loss 0.2551 | train_acc 0.8927 | val_loss 0.7121 | val_acc 0.8400
Epoch [6/100] | train_loss 0.2264 | train_acc 0.9060 | val_loss 0.4071 | val_acc 0.8933
Epoch [7/100] | train_loss 0.1674 | train_acc 0.9370 | val_loss 0.1838 | val_acc 0.9417
Epoch [8/100] | train_loss 0.1175 | train_acc 0.9577 | val_loss 0.1568 | val_acc 0.9567
No improvement (1/15).
Epoch [9/100] | train_loss 0.1310 | train_acc 0.9523 | val_loss 0.1322 | val_acc 0.9550
Epoch [10/100] | train_loss 0.1025 | train_acc 0.9617 | val_loss 0.1370 | val_acc 0.9600
No improvement (1/15).
Epoch [11/100] | train_loss 0.1026 | train_acc 0.9623 | val_loss 0.1421 | val_acc 0.9500wandb: updating run metadata
wandb: uploading summary, console lines 41-42; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–†â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–„â–„â–ˆâ–„â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 24
wandb:         grad/norm 0.99371
wandb:                lr 0.00024
wandb:         train/acc 0.973
wandb:        train/loss 0.07078
wandb: training_time_sec 10454.58677
wandb:           val/acc 0.93833
wandb:          val/loss 0.22568
wandb: 
wandb: ğŸš€ View run Trial41_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3svw1cv7
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_114528-3svw1cv7/logs
[I 2026-02-04 14:39:44,887] Trial 41 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0018810624173278216, 'dropout': 0.40533182955329433}. Best is trial 12 with value: 0.96.
wandb: setting up run oti77ven
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_143945-oti77ven
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial42_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/oti77ven
wandb: updating run metadata
wandb: uploading summary, console lines 36-37
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–„â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–ˆâ–‡â–‚â–„â–ˆâ–ˆâ–ˆâ–ˆâ–â–„â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–‡â–‡â–‡â–ˆâ–…â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–„â–ƒâ–‚â–â–…â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.945
wandb:             epoch 20
wandb:         grad/norm 1.0
wandb:                lr 0.00041
wandb:         train/acc 0.97633
wandb:        train/loss 0.06674
wandb: training_time_sec 8793.75403
wandb:           val/acc 0.92833
wandb:          val/loss 0.29429
wandb: 
wandb: ğŸš€ View run Trial42_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/oti77ven
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_143945-oti77ven/logs
[I 2026-02-04 17:06:20,919] Trial 42 finished with value: 0.945 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.003240156195463198, 'dropout': 0.39524423457503277}. Best is trial 12 with value: 0.96.
wandb: setting up run i7ffbrs9
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_170621-i7ffbrs9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial43_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i7ffbrs9
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–†â–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:  train/acc â–â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡
wandb:   val/loss â–ˆâ–‡â–„â–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95167
wandb:             epoch 21
wandb:         grad/norm 1.0
wandb:                lr 0.00024
wandb:         train/acc 0.972
wandb:        train/loss 0.07931
wandb: training_time_sec 9195.60632
wandb:           val/acc 0.91833
wandb:          val/loss 0.25563
wandb: 
wandb: ğŸš€ View run Trial43_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i7ffbrs9
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_170621-i7ffbrs9/logs
[I 2026-02-04 19:39:38,648] Trial 43 finished with value: 0.9516666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0019534821747034985, 'dropout': 0.003642107138502576}. Best is trial 12 with value: 0.96.
wandb: setting up run krknbe0t
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_193938-krknbe0t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial44_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/krknbe0t

No improvement (2/15).
Epoch [12/100] | train_loss 0.1018 | train_acc 0.9633 | val_loss 0.1292 | val_acc 0.9550
No improvement (3/15).
Epoch [13/100] | train_loss 0.0909 | train_acc 0.9693 | val_loss 0.1501 | val_acc 0.9517
No improvement (4/15).
Epoch [14/100] | train_loss 0.0808 | train_acc 0.9707 | val_loss 0.1627 | val_acc 0.9533
No improvement (5/15).
Epoch [15/100] | train_loss 0.0849 | train_acc 0.9667 | val_loss 0.1754 | val_acc 0.9567
No improvement (6/15).
Epoch [16/100] | train_loss 0.0870 | train_acc 0.9663 | val_loss 0.1971 | val_acc 0.9500
No improvement (7/15).
Epoch [17/100] | train_loss 0.0899 | train_acc 0.9660 | val_loss 0.1909 | val_acc 0.9567
No improvement (8/15).
Epoch [18/100] | train_loss 0.0749 | train_acc 0.9733 | val_loss 0.2118 | val_acc 0.9517
No improvement (9/15).
Epoch [19/100] | train_loss 0.0797 | train_acc 0.9707 | val_loss 0.1992 | val_acc 0.9500
No improvement (10/15).
Epoch [20/100] | train_loss 0.0646 | train_acc 0.9763 | val_loss 0.2237 | val_acc 0.9400
No improvement (11/15).
Epoch [21/100] | train_loss 0.0656 | train_acc 0.9777 | val_loss 0.2476 | val_acc 0.9400
No improvement (12/15).
Epoch [22/100] | train_loss 0.0673 | train_acc 0.9773 | val_loss 0.2696 | val_acc 0.9317
No improvement (13/15).
Epoch [23/100] | train_loss 0.0701 | train_acc 0.9743 | val_loss 0.2464 | val_acc 0.9350
No improvement (14/15).
Epoch [24/100] | train_loss 0.0708 | train_acc 0.9730 | val_loss 0.2257 | val_acc 0.9383
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 41 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7943 | train_acc 0.5267 | val_loss 0.6652 | val_acc 0.6050
Epoch [2/100] | train_loss 0.4823 | train_acc 0.7490 | val_loss 0.4370 | val_acc 0.8383
Epoch [3/100] | train_loss 0.2770 | train_acc 0.8807 | val_loss 0.3742 | val_acc 0.8867
Epoch [4/100] | train_loss 0.2424 | train_acc 0.9050 | val_loss 0.2713 | val_acc 0.9183
No improvement (1/15).
Epoch [5/100] | train_loss 0.1702 | train_acc 0.9370 | val_loss 0.2311 | val_acc 0.9183
Epoch [6/100] | train_loss 0.1504 | train_acc 0.9427 | val_loss 0.1626 | val_acc 0.9450
No improvement (1/15).
Epoch [7/100] | train_loss 0.2078 | train_acc 0.9237 | val_loss 0.4774 | val_acc 0.8083
No improvement (2/15).
Epoch [8/100] | train_loss 0.1311 | train_acc 0.9493 | val_loss 0.2223 | val_acc 0.9400
No improvement (3/15).
Epoch [9/100] | train_loss 0.1002 | train_acc 0.9603 | val_loss 0.2150 | val_acc 0.9433
No improvement (4/15).
Epoch [10/100] | train_loss 0.1101 | train_acc 0.9590 | val_loss 0.3312 | val_acc 0.9183
No improvement (5/15).
Epoch [11/100] | train_loss 0.0860 | train_acc 0.9697 | val_loss 0.2695 | val_acc 0.9183
No improvement (6/15).
Epoch [12/100] | train_loss 0.0774 | train_acc 0.9740 | val_loss 0.2788 | val_acc 0.9267
No improvement (7/15).
Epoch [13/100] | train_loss 0.0756 | train_acc 0.9723 | val_loss 0.2815 | val_acc 0.9350
No improvement (8/15).
Epoch [14/100] | train_loss 0.0680 | train_acc 0.9780 | val_loss 0.3416 | val_acc 0.9383
No improvement (9/15).
Epoch [15/100] | train_loss 0.1085 | train_acc 0.9610 | val_loss 0.2465 | val_acc 0.9367
No improvement (10/15).
Epoch [16/100] | train_loss 0.0684 | train_acc 0.9750 | val_loss 0.3238 | val_acc 0.9333
No improvement (11/15).
Epoch [17/100] | train_loss 0.1067 | train_acc 0.9617 | val_loss 0.2270 | val_acc 0.9450
No improvement (12/15).
Epoch [18/100] | train_loss 0.0657 | train_acc 0.9777 | val_loss 0.2878 | val_acc 0.9433
No improvement (13/15).
Epoch [19/100] | train_loss 0.0650 | train_acc 0.9793 | val_loss 0.2530 | val_acc 0.9417
No improvement (14/15).
Epoch [20/100] | train_loss 0.0667 | train_acc 0.9763 | val_loss 0.2943 | val_acc 0.9283
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 42 Finished. Best Val Acc: 94.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7204 | train_acc 0.5693 | val_loss 0.6220 | val_acc 0.6817
Epoch [2/100] | train_loss 0.4493 | train_acc 0.7933 | val_loss 0.5547 | val_acc 0.7050
Epoch [3/100] | train_loss 0.3458 | train_acc 0.8457 | val_loss 0.3264 | val_acc 0.8633
Epoch [4/100] | train_loss 0.2594 | train_acc 0.9013 | val_loss 0.1834 | val_acc 0.9250
Epoch [5/100] | train_loss 0.1485 | train_acc 0.9433 | val_loss 0.1711 | val_acc 0.9467
No improvement (1/15).
Epoch [6/100] | train_loss 0.1930 | train_acc 0.9307 | val_loss 0.2545 | val_acc 0.9067
Epoch [7/100] | train_loss 0.1431 | train_acc 0.9477 | val_loss 0.1330 | val_acc 0.9517
No improvement (1/15).
Epoch [8/100] | train_loss 0.1473 | train_acc 0.9437 | val_loss 0.1708 | val_acc 0.9367
No improvement (2/15).
Epoch [9/100] | train_loss 0.1082 | train_acc 0.9610 | val_loss 0.1729 | val_acc 0.9383
No improvement (3/15).
Epoch [10/100] | train_loss 0.0904 | train_acc 0.9680 | val_loss 0.1391 | val_acc 0.9483
No improvement (4/15).
Epoch [11/100] | train_loss 0.0954 | train_acc 0.9643 | val_loss 0.1748 | val_acc 0.9383
No improvement (5/15).
Epoch [12/100] | train_loss 0.0833 | train_acc 0.9703 | val_loss 0.1682 | val_acc 0.9417
No improvement (6/15).
Epoch [13/100] | train_loss 0.0817 | train_acc 0.9713 | val_loss 0.1415 | val_acc 0.9500
No improvement (7/15).
Epoch [14/100] | train_loss 0.0765 | train_acc 0.9753 | val_loss 0.1743 | val_acc 0.9400
No improvement (8/15).
Epoch [15/100] | train_loss 0.1026 | train_acc 0.9550 | val_loss 0.1808 | val_acc 0.9350
No improvement (9/15).
Epoch [16/100] | train_loss 0.0737 | train_acc 0.9723 | val_loss 0.1681 | val_acc 0.9467
No improvement (10/15).
Epoch [17/100] | train_loss 0.0765 | train_acc 0.9703 | val_loss 0.1673 | val_acc 0.9467
No improvement (11/15).
Epoch [18/100] | train_loss 0.0666 | train_acc 0.9747 | val_loss 0.1802 | val_acc 0.9450
No improvement (12/15).
Epoch [19/100] | train_loss 0.0584 | train_acc 0.9783 | val_loss 0.1993 | val_acc 0.9417
No improvement (13/15).
Epoch [20/100] | train_loss 0.0526 | train_acc 0.9820 | val_loss 0.2732 | val_acc 0.9167
No improvement (14/15).
Epoch [21/100] | train_loss 0.0793 | train_acc 0.9720 | val_loss 0.2556 | val_acc 0.9183
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 43 Finished. Best Val Acc: 95.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7251 | train_acc 0.5370 | val_loss 0.6692 | val_acc 0.6050
Epoch [2/100] | train_loss 0.5762 | train_acc 0.6657 | val_loss 0.3798 | val_acc 0.8483
Epoch [3/100] | train_loss 0.3359 | train_acc 0.8573 | val_loss 0.2783 | val_acc 0.8950
Epoch [4/100] | train_loss 0.2220 | train_acc 0.9087 | val_loss 0.2573 | val_acc 0.9250
Epoch [5/100] | train_loss 0.2480 | train_acc 0.9023 | val_loss 0.2299 | val_acc 0.9433
Epoch [6/100] | train_loss 0.1979 | train_acc 0.9243 | val_loss 0.1599 | val_acc 0.9583
No improvement (1/15).
Epoch [7/100] | train_loss 0.1676 | train_acc 0.9460 | val_loss 0.1918 | val_acc 0.9333
No improvement (2/15).
Epoch [8/100] | train_loss 0.1053 | train_acc 0.9607 | val_loss 0.1739 | val_acc 0.9517
No improvement (3/15).
Epoch [9/100] | train_loss 0.0938 | train_acc 0.9670 | val_loss 0.1610 | val_acc 0.9533
No improvement (4/15).
Epoch [10/100] | train_loss 0.0895 | train_acc 0.9700 | val_loss 0.1831 | val_acc 0.9417
No improvement (5/15).
Epoch [11/100] | train_loss 0.0919 | train_acc 0.9653 | val_loss 0.2246 | val_acc 0.9333
No improvement (6/15).
Epoch [12/100] | train_loss 0.0919 | train_acc 0.9647 | val_loss 0.1711 | val_acc 0.9550
No improvement (7/15).
Epoch [13/100] | train_loss 0.0997 | train_acc 0.9647 | val_loss 0.2067 | val_acc 0.9450
No improvement (8/15).
Epoch [14/100] | train_loss 0.0690 | train_acc 0.9767 | val_loss 0.1971 | val_acc 0.9550
No improvement (9/15).
Epoch [15/100] | train_loss 0.0673 | train_acc 0.9793 | val_loss 0.1961 | val_acc 0.9583
No improvement (10/15).
Epoch [16/100] | train_loss 0.0692 | train_acc 0.9773 | val_loss 0.1852 | val_acc 0.9533
No improvement (11/15).
Epoch [17/100] | train_loss 0.0950 | train_acc 0.9663 | val_loss 0.2171 | val_acc 0.9450
No improvement (12/15).
Epoch [18/100] | train_loss 0.0897 | train_acc 0.9693 | val_loss 0.2330 | val_acc 0.9450
No improvement (13/15).
Epoch [19/100] | train_loss 0.0756 | train_acc 0.9753 | val_loss 0.2133 | val_acc 0.9550wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–‡â–ˆâ–ˆâ–…â–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:   val/loss â–ˆâ–„â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95833
wandb:             epoch 20
wandb:         grad/norm 1.0
wandb:                lr 0.0002
wandb:         train/acc 0.97633
wandb:        train/loss 0.06481
wandb: training_time_sec 8806.88424
wandb:           val/acc 0.925
wandb:          val/loss 0.29156
wandb: 
wandb: ğŸš€ View run Trial44_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/krknbe0t
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_193938-krknbe0t/logs
[I 2026-02-04 22:06:27,481] Trial 44 finished with value: 0.9583333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.001570837923043144, 'dropout': 0.41135462374161286}. Best is trial 12 with value: 0.96.
wandb: setting up run 8j2unqtd
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260204_220627-8j2unqtd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial45_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8j2unqtd
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 48-49
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–„â–ˆâ–ƒâ–‚â–ƒâ–ˆâ–†â–…â–ƒâ–â–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ƒâ–„â–â–‚â–ƒâ–â–â–â–â–‚â–â–ƒâ–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 28
wandb:         grad/norm 0.19288
wandb:                lr 0.0001
wandb:         train/acc 0.98167
wandb:        train/loss 0.05146
wandb: training_time_sec 12161.81796
wandb:           val/acc 0.94
wandb:          val/loss 0.33173
wandb: 
wandb: ğŸš€ View run Trial45_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8j2unqtd
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260204_220627-8j2unqtd/logs
[I 2026-02-05 01:29:11,530] Trial 45 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0016329174917696554, 'dropout': 0.4195658641356695}. Best is trial 12 with value: 0.96.
wandb: setting up run 516mbboh
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_012911-516mbboh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial46_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/516mbboh
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–„â–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–‚â–†â–ˆâ–ƒâ–ƒâ–ˆâ–„â–â–ƒâ–â–„â–‚â–†â–â–â–ƒâ–‚â–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–„â–…â–â–„â–â–†â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–„â–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.955
wandb:             epoch 42
wandb:         grad/norm 0.03625
wandb:                lr 4e-05
wandb:         train/acc 0.99367
wandb:        train/loss 0.02212
wandb: training_time_sec 13484.91593
wandb:           val/acc 0.95333
wandb:          val/loss 0.40615
wandb: 
wandb: ğŸš€ View run Trial46_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/516mbboh
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_012911-516mbboh/logs
[I 2026-02-05 05:13:58,592] Trial 46 finished with value: 0.955 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0023817640392295174, 'dropout': 0.3650332120352136}. Best is trial 12 with value: 0.96.
wandb: setting up run sdibls7s
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_051358-sdibls7s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial47_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sdibls7s

No improvement (14/15).
Epoch [20/100] | train_loss 0.0648 | train_acc 0.9763 | val_loss 0.2916 | val_acc 0.9250
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 44 Finished. Best Val Acc: 95.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7231 | train_acc 0.5450 | val_loss 0.6808 | val_acc 0.5800
Epoch [2/100] | train_loss 0.5150 | train_acc 0.7080 | val_loss 0.3277 | val_acc 0.8767
No improvement (1/15).
Epoch [3/100] | train_loss 0.2756 | train_acc 0.8940 | val_loss 0.4027 | val_acc 0.8417
Epoch [4/100] | train_loss 0.1832 | train_acc 0.9337 | val_loss 0.1861 | val_acc 0.9250
Epoch [5/100] | train_loss 0.1988 | train_acc 0.9223 | val_loss 0.2309 | val_acc 0.9267
No improvement (1/15).
Epoch [6/100] | train_loss 0.1760 | train_acc 0.9323 | val_loss 0.3278 | val_acc 0.8850
Epoch [7/100] | train_loss 0.1668 | train_acc 0.9410 | val_loss 0.1590 | val_acc 0.9450
Epoch [8/100] | train_loss 0.1073 | train_acc 0.9577 | val_loss 0.1545 | val_acc 0.9467
Epoch [9/100] | train_loss 0.1008 | train_acc 0.9620 | val_loss 0.1556 | val_acc 0.9517
No improvement (1/15).
Epoch [10/100] | train_loss 0.0935 | train_acc 0.9660 | val_loss 0.1858 | val_acc 0.9483
No improvement (2/15).
Epoch [11/100] | train_loss 0.0845 | train_acc 0.9673 | val_loss 0.2130 | val_acc 0.9433
Epoch [12/100] | train_loss 0.0779 | train_acc 0.9713 | val_loss 0.1893 | val_acc 0.9533
No improvement (1/15).
Epoch [13/100] | train_loss 0.1000 | train_acc 0.9673 | val_loss 0.2697 | val_acc 0.9283
Epoch [14/100] | train_loss 0.0685 | train_acc 0.9773 | val_loss 0.1636 | val_acc 0.9600
No improvement (1/15).
Epoch [15/100] | train_loss 0.0677 | train_acc 0.9763 | val_loss 0.1798 | val_acc 0.9550
No improvement (2/15).
Epoch [16/100] | train_loss 0.0586 | train_acc 0.9793 | val_loss 0.1818 | val_acc 0.9550
No improvement (3/15).
Epoch [17/100] | train_loss 0.0664 | train_acc 0.9740 | val_loss 0.2034 | val_acc 0.9583
No improvement (4/15).
Epoch [18/100] | train_loss 0.0784 | train_acc 0.9730 | val_loss 0.1984 | val_acc 0.9500
No improvement (5/15).
Epoch [19/100] | train_loss 0.0557 | train_acc 0.9810 | val_loss 0.2123 | val_acc 0.9550
No improvement (6/15).
Epoch [20/100] | train_loss 0.0541 | train_acc 0.9793 | val_loss 0.2522 | val_acc 0.9533
No improvement (7/15).
Epoch [21/100] | train_loss 0.0479 | train_acc 0.9840 | val_loss 0.2447 | val_acc 0.9517
No improvement (8/15).
Epoch [22/100] | train_loss 0.0495 | train_acc 0.9830 | val_loss 0.2682 | val_acc 0.9533
No improvement (9/15).
Epoch [23/100] | train_loss 0.0485 | train_acc 0.9850 | val_loss 0.2831 | val_acc 0.9483
No improvement (10/15).
Epoch [24/100] | train_loss 0.0514 | train_acc 0.9817 | val_loss 0.3071 | val_acc 0.9467
No improvement (11/15).
Epoch [25/100] | train_loss 0.0488 | train_acc 0.9827 | val_loss 0.3251 | val_acc 0.9450
No improvement (12/15).
Epoch [26/100] | train_loss 0.0462 | train_acc 0.9817 | val_loss 0.3153 | val_acc 0.9400
No improvement (13/15).
Epoch [27/100] | train_loss 0.0525 | train_acc 0.9800 | val_loss 0.3193 | val_acc 0.9433
No improvement (14/15).
Epoch [28/100] | train_loss 0.0515 | train_acc 0.9817 | val_loss 0.3317 | val_acc 0.9400
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 45 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6985 | train_acc 0.5523 | val_loss 0.6575 | val_acc 0.5783
Epoch [2/100] | train_loss 0.5196 | train_acc 0.7273 | val_loss 0.4031 | val_acc 0.8767
Epoch [3/100] | train_loss 0.2735 | train_acc 0.8857 | val_loss 0.4298 | val_acc 0.8850
Epoch [4/100] | train_loss 0.2240 | train_acc 0.9163 | val_loss 0.1913 | val_acc 0.9317
No improvement (1/15).
Epoch [5/100] | train_loss 0.1564 | train_acc 0.9437 | val_loss 0.3933 | val_acc 0.8900
Epoch [6/100] | train_loss 0.1433 | train_acc 0.9527 | val_loss 0.1555 | val_acc 0.9500
No improvement (1/15).
Epoch [7/100] | train_loss 0.1557 | train_acc 0.9450 | val_loss 0.4817 | val_acc 0.8950
No improvement (2/15).
Epoch [8/100] | train_loss 0.1032 | train_acc 0.9603 | val_loss 0.2258 | val_acc 0.9400
No improvement (3/15).
Epoch [9/100] | train_loss 0.0918 | train_acc 0.9670 | val_loss 0.3073 | val_acc 0.9300
No improvement (4/15).
Epoch [10/100] | train_loss 0.1206 | train_acc 0.9593 | val_loss 0.1931 | val_acc 0.9383
No improvement (5/15).
Epoch [11/100] | train_loss 0.1081 | train_acc 0.9620 | val_loss 0.2343 | val_acc 0.9433
No improvement (6/15).
Epoch [12/100] | train_loss 0.1112 | train_acc 0.9593 | val_loss 0.1943 | val_acc 0.9467
No improvement (7/15).
Epoch [13/100] | train_loss 0.0872 | train_acc 0.9657 | val_loss 0.2437 | val_acc 0.9500
No improvement (8/15).
Epoch [14/100] | train_loss 0.0777 | train_acc 0.9720 | val_loss 0.2712 | val_acc 0.9450
Epoch [15/100] | train_loss 0.0822 | train_acc 0.9713 | val_loss 0.2600 | val_acc 0.9517
No improvement (1/15).
Epoch [16/100] | train_loss 0.0707 | train_acc 0.9777 | val_loss 0.2471 | val_acc 0.9483
No improvement (2/15).
Epoch [17/100] | train_loss 0.0617 | train_acc 0.9783 | val_loss 0.3262 | val_acc 0.9317
No improvement (3/15).
Epoch [18/100] | train_loss 0.0725 | train_acc 0.9733 | val_loss 0.3147 | val_acc 0.9333
No improvement (4/15).
Epoch [19/100] | train_loss 0.0731 | train_acc 0.9767 | val_loss 0.3637 | val_acc 0.9367
No improvement (5/15).
Epoch [20/100] | train_loss 0.0683 | train_acc 0.9740 | val_loss 0.4003 | val_acc 0.9350
No improvement (6/15).
Epoch [21/100] | train_loss 0.0498 | train_acc 0.9823 | val_loss 0.3122 | val_acc 0.9433
No improvement (7/15).
Epoch [22/100] | train_loss 0.0482 | train_acc 0.9830 | val_loss 0.4388 | val_acc 0.9250
No improvement (8/15).
Epoch [23/100] | train_loss 0.0485 | train_acc 0.9837 | val_loss 0.4542 | val_acc 0.9233
No improvement (9/15).
Epoch [24/100] | train_loss 0.0475 | train_acc 0.9843 | val_loss 0.3034 | val_acc 0.9467
No improvement (10/15).
Epoch [25/100] | train_loss 0.0586 | train_acc 0.9780 | val_loss 0.4004 | val_acc 0.9383
No improvement (11/15).
Epoch [26/100] | train_loss 0.0598 | train_acc 0.9803 | val_loss 0.2813 | val_acc 0.9483
No improvement (12/15).
Epoch [27/100] | train_loss 0.0419 | train_acc 0.9860 | val_loss 0.2841 | val_acc 0.9517
Epoch [28/100] | train_loss 0.0425 | train_acc 0.9843 | val_loss 0.2853 | val_acc 0.9550
No improvement (1/15).
Epoch [29/100] | train_loss 0.0400 | train_acc 0.9860 | val_loss 0.2905 | val_acc 0.9533
No improvement (2/15).
Epoch [30/100] | train_loss 0.0364 | train_acc 0.9887 | val_loss 0.3027 | val_acc 0.9467
No improvement (3/15).
Epoch [31/100] | train_loss 0.0332 | train_acc 0.9890 | val_loss 0.3091 | val_acc 0.9450
No improvement (4/15).
Epoch [32/100] | train_loss 0.0353 | train_acc 0.9893 | val_loss 0.3138 | val_acc 0.9533
No improvement (5/15).
Epoch [33/100] | train_loss 0.0282 | train_acc 0.9907 | val_loss 0.3304 | val_acc 0.9500
No improvement (6/15).
Epoch [34/100] | train_loss 0.0261 | train_acc 0.9917 | val_loss 0.3562 | val_acc 0.9483
No improvement (7/15).
Epoch [35/100] | train_loss 0.0284 | train_acc 0.9913 | val_loss 0.3724 | val_acc 0.9500
No improvement (8/15).
Epoch [36/100] | train_loss 0.0262 | train_acc 0.9920 | val_loss 0.3474 | val_acc 0.9517
No improvement (9/15).
Epoch [37/100] | train_loss 0.0253 | train_acc 0.9923 | val_loss 0.3775 | val_acc 0.9467
No improvement (10/15).
Epoch [38/100] | train_loss 0.0244 | train_acc 0.9933 | val_loss 0.3944 | val_acc 0.9533
No improvement (11/15).
Epoch [39/100] | train_loss 0.0217 | train_acc 0.9950 | val_loss 0.4156 | val_acc 0.9533
No improvement (12/15).
Epoch [40/100] | train_loss 0.0212 | train_acc 0.9943 | val_loss 0.3995 | val_acc 0.9550
No improvement (13/15).
Epoch [41/100] | train_loss 0.0226 | train_acc 0.9933 | val_loss 0.4003 | val_acc 0.9533
No improvement (14/15).
Epoch [42/100] | train_loss 0.0221 | train_acc 0.9937 | val_loss 0.4062 | val_acc 0.9533
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 46 Finished. Best Val Acc: 95.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7424 | train_acc 0.5627 | val_loss 0.6824 | val_acc 0.5417
Epoch [2/100] | train_loss 0.5869 | train_acc 0.6907 | val_loss 0.4858 | val_acc 0.7700wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡
wandb:   val/loss â–ˆâ–…â–ƒâ–„â–‚â–‚â–â–‚â–‚â–â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–„â–‚â–ƒâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96333
wandb:             epoch 21
wandb:         grad/norm 1.0
wandb:                lr 0.00025
wandb:         train/acc 0.98533
wandb:        train/loss 0.04299
wandb: training_time_sec 9211.3164
wandb:           val/acc 0.89167
wandb:          val/loss 0.69499
wandb: 
wandb: ğŸš€ View run Trial47_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sdibls7s
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_051358-sdibls7s/logs
[I 2026-02-05 07:47:31,942] Trial 47 finished with value: 0.9633333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0019873991025308583, 'dropout': 0.4371496000823332}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_074732-j1idcybl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial48_[CV-Variation]_L1_H8_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j1idcybl
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run Trial48_[CV-Variation]_L1_H8_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j1idcybl
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_074732-j1idcybl/logs
[I 2026-02-05 07:49:12,806] Trial 48 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 1, 'd_model': 16, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.00439864830139499, 'dropout': 0.469332276172454}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run d43plv5z
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_074913-d43plv5z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial49_[CV-Variation]_L4_H8_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d43plv5z
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run Trial49_[CV-Variation]_L4_H8_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d43plv5z
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_074913-d43plv5z/logs
[I 2026-02-05 07:55:51,906] Trial 49 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 32, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0022266390858756147, 'dropout': 0.44318856889066166}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run ajywhh2l
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_075552-ajywhh2l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial50_[CV-Variation]_L2_H8_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ajywhh2l
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading summary, console lines 33-34
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–
wandb:  train/acc â–â–â–â–ƒâ–„â–‡â–ˆâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚
wandb: train/loss â–ˆâ–„â–„â–„â–ƒâ–‚â–â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:    val/acc â–†â–†â–ˆâ–ˆâ–…â–†â–â–…â–…â–†â–†â–†â–‡â–†â–†â–†â–†
wandb:   val/loss â–â–â–â–â–‚â–ˆâ–…â–â–â–â–â–‚â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.51
wandb:             epoch 17
wandb:         grad/norm 1.0
wandb:                lr 0.00279
wandb:         train/acc 0.53733
wandb:        train/loss 0.68779
wandb: training_time_sec 3994.21952
wandb:           val/acc 0.49
wandb:          val/loss 0.71293
wandb: 
wandb: ğŸš€ View run Trial50_[CV-Variation]_L2_H8_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ajywhh2l
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_075552-ajywhh2l/logs
[I 2026-02-05 09:02:28,446] Trial 50 finished with value: 0.51 and parameters: {'nhead': 8, 'num_layers': 2, 'd_model': 128, 'batch_size': 128, 'use_conv1d': True, 'lr': 0.005574069835052231, 'dropout': 0.23919350214266924}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run u6mhmhkf
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_090228-u6mhmhkf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial51_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u6mhmhkf

Epoch [3/100] | train_loss 0.4183 | train_acc 0.8200 | val_loss 0.2941 | val_acc 0.8833
Epoch [4/100] | train_loss 0.2305 | train_acc 0.9143 | val_loss 0.3615 | val_acc 0.9083
Epoch [5/100] | train_loss 0.1832 | train_acc 0.9317 | val_loss 0.1856 | val_acc 0.9300
No improvement (1/15).
Epoch [6/100] | train_loss 0.1684 | train_acc 0.9397 | val_loss 0.2551 | val_acc 0.9233
Epoch [7/100] | train_loss 0.1517 | train_acc 0.9463 | val_loss 0.1443 | val_acc 0.9633
No improvement (1/15).
Epoch [8/100] | train_loss 0.1085 | train_acc 0.9617 | val_loss 0.2502 | val_acc 0.9300
No improvement (2/15).
Epoch [9/100] | train_loss 0.0880 | train_acc 0.9657 | val_loss 0.2135 | val_acc 0.9450
No improvement (3/15).
Epoch [10/100] | train_loss 0.1025 | train_acc 0.9617 | val_loss 0.1815 | val_acc 0.9417
No improvement (4/15).
Epoch [11/100] | train_loss 0.0804 | train_acc 0.9740 | val_loss 0.2132 | val_acc 0.9483
No improvement (5/15).
Epoch [12/100] | train_loss 0.0885 | train_acc 0.9647 | val_loss 0.2908 | val_acc 0.9317
No improvement (6/15).
Epoch [13/100] | train_loss 0.0989 | train_acc 0.9597 | val_loss 0.3000 | val_acc 0.9250
No improvement (7/15).
Epoch [14/100] | train_loss 0.0729 | train_acc 0.9750 | val_loss 0.1972 | val_acc 0.9517
No improvement (8/15).
Epoch [15/100] | train_loss 0.0801 | train_acc 0.9680 | val_loss 0.2661 | val_acc 0.9317
No improvement (9/15).
Epoch [16/100] | train_loss 0.0586 | train_acc 0.9793 | val_loss 0.2387 | val_acc 0.9500
No improvement (10/15).
Epoch [17/100] | train_loss 0.0723 | train_acc 0.9723 | val_loss 0.3953 | val_acc 0.9083
No improvement (11/15).
Epoch [18/100] | train_loss 0.0523 | train_acc 0.9840 | val_loss 0.2621 | val_acc 0.9433
No improvement (12/15).
Epoch [19/100] | train_loss 0.0499 | train_acc 0.9823 | val_loss 0.2780 | val_acc 0.9417
No improvement (13/15).
Epoch [20/100] | train_loss 0.0493 | train_acc 0.9847 | val_loss 0.6806 | val_acc 0.8833
No improvement (14/15).
Epoch [21/100] | train_loss 0.0430 | train_acc 0.9853 | val_loss 0.6950 | val_acc 0.8917
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 47 Finished. Best Val Acc: 96.33%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 48 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.32 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.77 MiB is allocated by PyTorch, and 3.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 49 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.22 GiB is free. Including non-PyTorch memory, this process has 378.00 MiB memory in use. Of the allocated memory 219.20 MiB is allocated by PyTorch, and 2.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.9955 | train_acc 0.4950 | val_loss 0.6932 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.7156 | train_acc 0.5083 | val_loss 0.7102 | val_acc 0.4900
Epoch [3/100] | train_loss 0.7098 | train_acc 0.4927 | val_loss 0.6913 | val_acc 0.5100
No improvement (1/15).
Epoch [4/100] | train_loss 0.6829 | train_acc 0.5493 | val_loss 0.6943 | val_acc 0.5083
No improvement (2/15).
Epoch [5/100] | train_loss 0.6326 | train_acc 0.6030 | val_loss 0.7425 | val_acc 0.4783
No improvement (3/15).
Epoch [6/100] | train_loss 0.5526 | train_acc 0.7247 | val_loss 1.3589 | val_acc 0.4883
No improvement (4/15).
Epoch [7/100] | train_loss 0.5040 | train_acc 0.7490 | val_loss 1.0434 | val_acc 0.4250
No improvement (5/15).
Epoch [8/100] | train_loss 0.7322 | train_acc 0.5123 | val_loss 0.6933 | val_acc 0.4767
No improvement (6/15).
Epoch [9/100] | train_loss 0.6886 | train_acc 0.5520 | val_loss 0.6976 | val_acc 0.4733
No improvement (7/15).
Epoch [10/100] | train_loss 0.6832 | train_acc 0.5593 | val_loss 0.7374 | val_acc 0.4900
No improvement (8/15).
Epoch [11/100] | train_loss 0.6846 | train_acc 0.5453 | val_loss 0.7114 | val_acc 0.4833
No improvement (9/15).
Epoch [12/100] | train_loss 0.6851 | train_acc 0.5580 | val_loss 0.7616 | val_acc 0.4900
No improvement (10/15).
Epoch [13/100] | train_loss 0.6881 | train_acc 0.5433 | val_loss 0.7149 | val_acc 0.4950
No improvement (11/15).
Epoch [14/100] | train_loss 0.6865 | train_acc 0.5433 | val_loss 0.7188 | val_acc 0.4900
No improvement (12/15).
Epoch [15/100] | train_loss 0.6864 | train_acc 0.5543 | val_loss 0.7296 | val_acc 0.4900
No improvement (13/15).
Epoch [16/100] | train_loss 0.6889 | train_acc 0.5307 | val_loss 0.7183 | val_acc 0.4900
No improvement (14/15).
Epoch [17/100] | train_loss 0.6878 | train_acc 0.5373 | val_loss 0.7129 | val_acc 0.4900
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 50 Finished. Best Val Acc: 51.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7129 | train_acc 0.5540 | val_loss 0.6561 | val_acc 0.5900
Epoch [2/100] | train_loss 0.5331 | train_acc 0.7257 | val_loss 0.4213 | val_acc 0.8300
No improvement (1/15).
Epoch [3/100] | train_loss 0.3931 | train_acc 0.8233 | val_loss 0.6297 | val_acc 0.7717
Epoch [4/100] | train_loss 0.2365 | train_acc 0.9047 | val_loss 0.4178 | val_acc 0.8750
Epoch [5/100] | train_loss 0.1743 | train_acc 0.9327 | val_loss 0.1767 | val_acc 0.9417
No improvement (1/15).
Epoch [6/100] | train_loss 0.1651 | train_acc 0.9397 | val_loss 0.6847 | val_acc 0.7917
No improvement (2/15).
Epoch [7/100] | train_loss 0.1600 | train_acc 0.9417 | val_loss 0.2001 | val_acc 0.9417
No improvement (3/15).
Epoch [8/100] | train_loss 0.1276 | train_acc 0.9537 | val_loss 0.2864 | val_acc 0.9000
No improvement (4/15).
Epoch [9/100] | train_loss 0.1165 | train_acc 0.9567 | val_loss 0.2129 | val_acc 0.9383
No improvement (5/15).
Epoch [10/100] | train_loss 0.1164 | train_acc 0.9573 | val_loss 0.2698 | val_acc 0.9367
Epoch [11/100] | train_loss 0.1083 | train_acc 0.9633 | val_loss 0.1745 | val_acc 0.9483
No improvement (1/15).
Epoch [12/100] | train_loss 0.0917 | train_acc 0.9687 | val_loss 0.1994 | val_acc 0.9417
Epoch [13/100] | train_loss 0.0790 | train_acc 0.9713 | val_loss 0.1467 | val_acc 0.9583
No improvement (1/15).
Epoch [14/100] | train_loss 0.0772 | train_acc 0.9727 | val_loss 0.1462 | val_acc 0.9533
No improvement (2/15).
Epoch [15/100] | train_loss 0.0729 | train_acc 0.9737 | val_loss 0.1521 | val_acc 0.9567
No improvement (3/15).
Epoch [16/100] | train_loss 0.0626 | train_acc 0.9770 | val_loss 0.1799 | val_acc 0.9517
No improvement (4/15).
Epoch [17/100] | train_loss 0.0722 | train_acc 0.9747 | val_loss 0.1675 | val_acc 0.9550
No improvement (5/15).
Epoch [18/100] | train_loss 0.0891 | train_acc 0.9653 | val_loss 0.1712 | val_acc 0.9583
No improvement (6/15).
Epoch [19/100] | train_loss 0.0662 | train_acc 0.9743 | val_loss 0.2031 | val_acc 0.9517
No improvement (7/15).
Epoch [20/100] | train_loss 0.0504 | train_acc 0.9840 | val_loss 0.2025 | val_acc 0.9500
No improvement (8/15).
Epoch [21/100] | train_loss 0.0547 | train_acc 0.9793 | val_loss 0.1790 | val_acc 0.9567
No improvement (9/15).
Epoch [22/100] | train_loss 0.0525 | train_acc 0.9820 | val_loss 0.2156 | val_acc 0.9583
No improvement (10/15).
Epoch [23/100] | train_loss 0.0442 | train_acc 0.9867 | val_loss 0.2213 | val_acc 0.9533
No improvement (11/15).
Epoch [24/100] | train_loss 0.0520 | train_acc 0.9820 | val_loss 0.2017 | val_acc 0.9517
No improvement (12/15).
Epoch [25/100] | train_loss 0.0470 | train_acc 0.9860 | val_loss 0.2719 | val_acc 0.9500
No improvement (13/15).
Epoch [26/100] | train_loss 0.0517 | train_acc 0.9807 | val_loss 0.2212 | val_acc 0.9550
No improvement (14/15).
Epoch [27/100] | train_loss 0.0473 | train_acc 0.9833 | val_loss 0.2163 | val_acc 0.9550wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–†â–ˆâ–ˆâ–â–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–‡â–„â–„â–ƒâ–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  train/acc â–â–„â–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–„â–†â–ˆâ–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–‡â–…â–â–ˆâ–‚â–ƒâ–‚â–ƒâ–â–‚â–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95833
wandb:             epoch 27
wandb:         grad/norm 0.27926
wandb:                lr 0.00016
wandb:         train/acc 0.98333
wandb:        train/loss 0.04731
wandb: training_time_sec 11782.46002
wandb:           val/acc 0.955
wandb:          val/loss 0.21634
wandb: 
wandb: ğŸš€ View run Trial51_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u6mhmhkf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_090228-u6mhmhkf/logs
[I 2026-02-05 12:18:53,011] Trial 51 finished with value: 0.9583333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.001306268829075687, 'dropout': 0.43273092636151184}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_121853-2kcsbnqe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial52_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2kcsbnqe
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading summary, console lines 46-47
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ƒâ–‡â–ƒâ–â–„â–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–†â–‡â–„â–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–†â–‚â–†â–ƒâ–†â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.955
wandb:             epoch 26
wandb:         grad/norm 0.45497
wandb:                lr 0.00011
wandb:         train/acc 0.97933
wandb:        train/loss 0.06013
wandb: training_time_sec 11317.47724
wandb:           val/acc 0.95167
wandb:          val/loss 0.19844
wandb: 
wandb: ğŸš€ View run Trial52_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2kcsbnqe
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_121853-2kcsbnqe/logs
[I 2026-02-05 15:27:33,254] Trial 52 finished with value: 0.955 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0017198420730757586, 'dropout': 0.4076695132174674}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 8ysf4htx
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_152733-8ysf4htx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial53_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8ysf4htx
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–„â–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–†â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–„â–‚â–â–†â–â–â–ƒâ–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–„â–„â–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95667
wandb:             epoch 22
wandb:         grad/norm 1.0
wandb:                lr 0.00022
wandb:         train/acc 0.984
wandb:        train/loss 0.05217
wandb: training_time_sec 9605.0214
wandb:           val/acc 0.93
wandb:          val/loss 0.38548
wandb: 
wandb: ğŸš€ View run Trial53_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8ysf4htx
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_152733-8ysf4htx/logs
[I 2026-02-05 18:07:41,170] Trial 53 finished with value: 0.9566666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0017437839225625054, 'dropout': 0.46943271847415374}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run anzen97y
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_180741-anzen97y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial54_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/anzen97y
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading summary, console lines 41-42
wandb: uploading data
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  train/acc â–â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡
wandb:   val/loss â–ˆâ–…â–‚â–â–â–â–â–â–â–â–â–‡â–‚â–â–â–‚â–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95167
wandb:             epoch 23
wandb:         grad/norm 0.20205
wandb:                lr 0.00043
wandb:         train/acc 0.99167
wandb:        train/loss 0.03215
wandb: training_time_sec 10079.53845
wandb:           val/acc 0.925
wandb:          val/loss 0.38855
wandb: 
wandb: ğŸš€ View run Trial54_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/anzen97y
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_180741-anzen97y/logs
[I 2026-02-05 20:55:43,003] Trial 54 finished with value: 0.9516666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.0034482053810085474, 'dropout': 0.2878409963224168}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run knsoovxx
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260205_205543-knsoovxx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial55_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/knsoovxx

No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 51 Finished. Best Val Acc: 95.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6981 | train_acc 0.5663 | val_loss 0.6707 | val_acc 0.5950
Epoch [2/100] | train_loss 0.6262 | train_acc 0.6447 | val_loss 0.5973 | val_acc 0.6483
Epoch [3/100] | train_loss 0.3690 | train_acc 0.8487 | val_loss 0.5581 | val_acc 0.8267
Epoch [4/100] | train_loss 0.3209 | train_acc 0.8667 | val_loss 0.2594 | val_acc 0.9083
No improvement (1/15).
Epoch [5/100] | train_loss 0.2215 | train_acc 0.9130 | val_loss 0.5049 | val_acc 0.7367
No improvement (2/15).
Epoch [6/100] | train_loss 0.2327 | train_acc 0.9050 | val_loss 0.3099 | val_acc 0.8983
No improvement (3/15).
Epoch [7/100] | train_loss 0.1494 | train_acc 0.9487 | val_loss 0.5044 | val_acc 0.8367
Epoch [8/100] | train_loss 0.1403 | train_acc 0.9450 | val_loss 0.1957 | val_acc 0.9300
No improvement (1/15).
Epoch [9/100] | train_loss 0.1139 | train_acc 0.9577 | val_loss 0.2582 | val_acc 0.9133
Epoch [10/100] | train_loss 0.1238 | train_acc 0.9520 | val_loss 0.1916 | val_acc 0.9450
No improvement (1/15).
Epoch [11/100] | train_loss 0.1073 | train_acc 0.9580 | val_loss 0.2235 | val_acc 0.9350
Epoch [12/100] | train_loss 0.0999 | train_acc 0.9610 | val_loss 0.1565 | val_acc 0.9550
No improvement (1/15).
Epoch [13/100] | train_loss 0.0997 | train_acc 0.9607 | val_loss 0.1973 | val_acc 0.9467
No improvement (2/15).
Epoch [14/100] | train_loss 0.0766 | train_acc 0.9727 | val_loss 0.2228 | val_acc 0.9483
No improvement (3/15).
Epoch [15/100] | train_loss 0.0826 | train_acc 0.9697 | val_loss 0.2229 | val_acc 0.9483
No improvement (4/15).
Epoch [16/100] | train_loss 0.0755 | train_acc 0.9733 | val_loss 0.1912 | val_acc 0.9533
No improvement (5/15).
Epoch [17/100] | train_loss 0.0805 | train_acc 0.9697 | val_loss 0.1705 | val_acc 0.9533
No improvement (6/15).
Epoch [18/100] | train_loss 0.0723 | train_acc 0.9730 | val_loss 0.1842 | val_acc 0.9533
No improvement (7/15).
Epoch [19/100] | train_loss 0.0681 | train_acc 0.9783 | val_loss 0.2432 | val_acc 0.9400
No improvement (8/15).
Epoch [20/100] | train_loss 0.0907 | train_acc 0.9660 | val_loss 0.1962 | val_acc 0.9450
No improvement (9/15).
Epoch [21/100] | train_loss 0.0535 | train_acc 0.9847 | val_loss 0.1752 | val_acc 0.9550
No improvement (10/15).
Epoch [22/100] | train_loss 0.0660 | train_acc 0.9760 | val_loss 0.2001 | val_acc 0.9483
No improvement (11/15).
Epoch [23/100] | train_loss 0.0521 | train_acc 0.9840 | val_loss 0.1914 | val_acc 0.9517
No improvement (12/15).
Epoch [24/100] | train_loss 0.0595 | train_acc 0.9807 | val_loss 0.2091 | val_acc 0.9533
No improvement (13/15).
Epoch [25/100] | train_loss 0.0510 | train_acc 0.9850 | val_loss 0.1986 | val_acc 0.9550
No improvement (14/15).
Epoch [26/100] | train_loss 0.0601 | train_acc 0.9793 | val_loss 0.1984 | val_acc 0.9517
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 52 Finished. Best Val Acc: 95.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7102 | train_acc 0.5607 | val_loss 0.6567 | val_acc 0.5683
Epoch [2/100] | train_loss 0.5556 | train_acc 0.6933 | val_loss 0.4609 | val_acc 0.8117
Epoch [3/100] | train_loss 0.3084 | train_acc 0.8637 | val_loss 0.3573 | val_acc 0.8583
Epoch [4/100] | train_loss 0.1820 | train_acc 0.9310 | val_loss 0.2038 | val_acc 0.9317
Epoch [5/100] | train_loss 0.1651 | train_acc 0.9373 | val_loss 0.1532 | val_acc 0.9483
No improvement (1/15).
Epoch [6/100] | train_loss 0.1678 | train_acc 0.9337 | val_loss 0.4884 | val_acc 0.8333
Epoch [7/100] | train_loss 0.1579 | train_acc 0.9370 | val_loss 0.1727 | val_acc 0.9500
Epoch [8/100] | train_loss 0.1036 | train_acc 0.9620 | val_loss 0.1626 | val_acc 0.9567
No improvement (1/15).
Epoch [9/100] | train_loss 0.0950 | train_acc 0.9657 | val_loss 0.2617 | val_acc 0.9317
No improvement (2/15).
Epoch [10/100] | train_loss 0.1234 | train_acc 0.9517 | val_loss 0.1718 | val_acc 0.9450
No improvement (3/15).
Epoch [11/100] | train_loss 0.0890 | train_acc 0.9680 | val_loss 0.2425 | val_acc 0.9317
No improvement (4/15).
Epoch [12/100] | train_loss 0.0937 | train_acc 0.9647 | val_loss 0.1607 | val_acc 0.9533
No improvement (5/15).
Epoch [13/100] | train_loss 0.0866 | train_acc 0.9647 | val_loss 0.2097 | val_acc 0.9433
No improvement (6/15).
Epoch [14/100] | train_loss 0.0991 | train_acc 0.9637 | val_loss 0.1822 | val_acc 0.9533
No improvement (7/15).
Epoch [15/100] | train_loss 0.0748 | train_acc 0.9727 | val_loss 0.2148 | val_acc 0.9517
No improvement (8/15).
Epoch [16/100] | train_loss 0.0634 | train_acc 0.9777 | val_loss 0.2115 | val_acc 0.9417
No improvement (9/15).
Epoch [17/100] | train_loss 0.0654 | train_acc 0.9783 | val_loss 0.2415 | val_acc 0.9467
No improvement (10/15).
Epoch [18/100] | train_loss 0.0713 | train_acc 0.9753 | val_loss 0.2195 | val_acc 0.9417
No improvement (11/15).
Epoch [19/100] | train_loss 0.0592 | train_acc 0.9783 | val_loss 0.2588 | val_acc 0.9500
No improvement (12/15).
Epoch [20/100] | train_loss 0.0554 | train_acc 0.9813 | val_loss 0.3816 | val_acc 0.9317
No improvement (13/15).
Epoch [21/100] | train_loss 0.0545 | train_acc 0.9820 | val_loss 0.3458 | val_acc 0.9367
No improvement (14/15).
Epoch [22/100] | train_loss 0.0522 | train_acc 0.9840 | val_loss 0.3855 | val_acc 0.9300
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 53 Finished. Best Val Acc: 95.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7269 | train_acc 0.5883 | val_loss 0.7731 | val_acc 0.6317
Epoch [2/100] | train_loss 0.4396 | train_acc 0.7927 | val_loss 0.4805 | val_acc 0.7967
Epoch [3/100] | train_loss 0.2995 | train_acc 0.8727 | val_loss 0.2424 | val_acc 0.9117
No improvement (1/15).
Epoch [4/100] | train_loss 0.2322 | train_acc 0.9043 | val_loss 0.2087 | val_acc 0.9117
Epoch [5/100] | train_loss 0.2151 | train_acc 0.9150 | val_loss 0.1772 | val_acc 0.9317
No improvement (1/15).
Epoch [6/100] | train_loss 0.1708 | train_acc 0.9333 | val_loss 0.2089 | val_acc 0.9200
No improvement (2/15).
Epoch [7/100] | train_loss 0.1652 | train_acc 0.9370 | val_loss 0.2115 | val_acc 0.9283
Epoch [8/100] | train_loss 0.0992 | train_acc 0.9640 | val_loss 0.1926 | val_acc 0.9333
Epoch [9/100] | train_loss 0.0914 | train_acc 0.9693 | val_loss 0.1719 | val_acc 0.9517
No improvement (1/15).
Epoch [10/100] | train_loss 0.0863 | train_acc 0.9690 | val_loss 0.1733 | val_acc 0.9433
No improvement (2/15).
Epoch [11/100] | train_loss 0.0778 | train_acc 0.9743 | val_loss 0.1934 | val_acc 0.9417
No improvement (3/15).
Epoch [12/100] | train_loss 0.1172 | train_acc 0.9597 | val_loss 0.6689 | val_acc 0.8450
No improvement (4/15).
Epoch [13/100] | train_loss 0.1379 | train_acc 0.9490 | val_loss 0.2668 | val_acc 0.9117
No improvement (5/15).
Epoch [14/100] | train_loss 0.0761 | train_acc 0.9723 | val_loss 0.1712 | val_acc 0.9483
No improvement (6/15).
Epoch [15/100] | train_loss 0.0684 | train_acc 0.9777 | val_loss 0.1797 | val_acc 0.9517
No improvement (7/15).
Epoch [16/100] | train_loss 0.0568 | train_acc 0.9823 | val_loss 0.2285 | val_acc 0.9433
No improvement (8/15).
Epoch [17/100] | train_loss 0.0546 | train_acc 0.9827 | val_loss 0.3355 | val_acc 0.9200
No improvement (9/15).
Epoch [18/100] | train_loss 0.0586 | train_acc 0.9803 | val_loss 0.2043 | val_acc 0.9483
No improvement (10/15).
Epoch [19/100] | train_loss 0.0506 | train_acc 0.9823 | val_loss 0.3103 | val_acc 0.9333
No improvement (11/15).
Epoch [20/100] | train_loss 0.0605 | train_acc 0.9813 | val_loss 0.2878 | val_acc 0.9300
No improvement (12/15).
Epoch [21/100] | train_loss 0.0472 | train_acc 0.9847 | val_loss 0.3002 | val_acc 0.9367
No improvement (13/15).
Epoch [22/100] | train_loss 0.0430 | train_acc 0.9873 | val_loss 0.3619 | val_acc 0.9200
No improvement (14/15).
Epoch [23/100] | train_loss 0.0322 | train_acc 0.9917 | val_loss 0.3885 | val_acc 0.9250
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 54 Finished. Best Val Acc: 95.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7450 | train_acc 0.5523 | val_loss 0.5942 | val_acc 0.7483
No improvement (1/15).
Epoch [2/100] | train_loss 0.5007 | train_acc 0.7590 | val_loss 0.7918 | val_acc 0.7183wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 62-63
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–†â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–„â–†â–…â–…â–ˆâ–†â–‡â–ƒâ–„â–„â–â–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–‚â–â–†â–ˆâ–‡â–ˆâ–‡â–†â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡
wandb:   val/loss â–†â–ˆâ–‚â–â–â–â–â–ƒâ–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–…â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95667
wandb:             epoch 34
wandb:         grad/norm 0.19692
wandb:                lr 8e-05
wandb:         train/acc 0.99267
wandb:        train/loss 0.02657
wandb: training_time_sec 14711.51471
wandb:           val/acc 0.93
wandb:          val/loss 0.46425
wandb: 
wandb: ğŸš€ View run Trial55_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/knsoovxx
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260205_205543-knsoovxx/logs
[I 2026-02-06 01:00:56,758] Trial 55 finished with value: 0.9566666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0025790045515769844, 'dropout': 0.4216878752914491}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run aa22yxks
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_010057-aa22yxks
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial56_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/aa22yxks
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–…â–…â–‡â–…â–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–…â–„â–†â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95833
wandb:             epoch 43
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.97467
wandb:        train/loss 0.07737
wandb: training_time_sec 18490.71891
wandb:           val/acc 0.95
wandb:          val/loss 0.1794
wandb: 
wandb: ğŸš€ View run Trial56_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/aa22yxks
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_010057-aa22yxks/logs

Epoch [3/100] | train_loss 0.3144 | train_acc 0.8743 | val_loss 0.2643 | val_acc 0.8967
Epoch [4/100] | train_loss 0.2070 | train_acc 0.9200 | val_loss 0.1892 | val_acc 0.9433
No improvement (1/15).
Epoch [5/100] | train_loss 0.2076 | train_acc 0.9267 | val_loss 0.1985 | val_acc 0.9300
Epoch [6/100] | train_loss 0.1664 | train_acc 0.9350 | val_loss 0.1921 | val_acc 0.9450
No improvement (1/15).
Epoch [7/100] | train_loss 0.1296 | train_acc 0.9557 | val_loss 0.1900 | val_acc 0.9317
No improvement (2/15).
Epoch [8/100] | train_loss 0.1559 | train_acc 0.9407 | val_loss 0.3262 | val_acc 0.8850
Epoch [9/100] | train_loss 0.1207 | train_acc 0.9557 | val_loss 0.1897 | val_acc 0.9500
Epoch [10/100] | train_loss 0.0952 | train_acc 0.9663 | val_loss 0.1931 | val_acc 0.9550
No improvement (1/15).
Epoch [11/100] | train_loss 0.0842 | train_acc 0.9700 | val_loss 0.2969 | val_acc 0.9250
No improvement (2/15).
Epoch [12/100] | train_loss 0.1118 | train_acc 0.9560 | val_loss 0.2367 | val_acc 0.9533
No improvement (3/15).
Epoch [13/100] | train_loss 0.0856 | train_acc 0.9723 | val_loss 0.2768 | val_acc 0.9383
No improvement (4/15).
Epoch [14/100] | train_loss 0.0779 | train_acc 0.9710 | val_loss 0.2829 | val_acc 0.9467
No improvement (5/15).
Epoch [15/100] | train_loss 0.0753 | train_acc 0.9723 | val_loss 0.2144 | val_acc 0.9517
No improvement (6/15).
Epoch [16/100] | train_loss 0.0925 | train_acc 0.9653 | val_loss 0.1880 | val_acc 0.9533
No improvement (7/15).
Epoch [17/100] | train_loss 0.0759 | train_acc 0.9710 | val_loss 0.2075 | val_acc 0.9483
No improvement (8/15).
Epoch [18/100] | train_loss 0.1016 | train_acc 0.9600 | val_loss 0.1772 | val_acc 0.9517
No improvement (9/15).
Epoch [19/100] | train_loss 0.0813 | train_acc 0.9690 | val_loss 0.1973 | val_acc 0.9467
Epoch [20/100] | train_loss 0.0983 | train_acc 0.9627 | val_loss 0.1830 | val_acc 0.9567
No improvement (1/15).
Epoch [21/100] | train_loss 0.0485 | train_acc 0.9820 | val_loss 0.2458 | val_acc 0.9417
No improvement (2/15).
Epoch [22/100] | train_loss 0.0514 | train_acc 0.9810 | val_loss 0.2736 | val_acc 0.9350
No improvement (3/15).
Epoch [23/100] | train_loss 0.0463 | train_acc 0.9833 | val_loss 0.2757 | val_acc 0.9400
No improvement (4/15).
Epoch [24/100] | train_loss 0.0523 | train_acc 0.9797 | val_loss 0.2658 | val_acc 0.9333
No improvement (5/15).
Epoch [25/100] | train_loss 0.0451 | train_acc 0.9833 | val_loss 0.3149 | val_acc 0.9317
No improvement (6/15).
Epoch [26/100] | train_loss 0.0435 | train_acc 0.9833 | val_loss 0.3765 | val_acc 0.9233
No improvement (7/15).
Epoch [27/100] | train_loss 0.0398 | train_acc 0.9847 | val_loss 0.4186 | val_acc 0.9233
No improvement (8/15).
Epoch [28/100] | train_loss 0.0330 | train_acc 0.9887 | val_loss 0.4098 | val_acc 0.9267
No improvement (9/15).
Epoch [29/100] | train_loss 0.0343 | train_acc 0.9893 | val_loss 0.4354 | val_acc 0.9200
No improvement (10/15).
Epoch [30/100] | train_loss 0.0317 | train_acc 0.9897 | val_loss 0.4367 | val_acc 0.9267
No improvement (11/15).
Epoch [31/100] | train_loss 0.0308 | train_acc 0.9903 | val_loss 0.3953 | val_acc 0.9417
No improvement (12/15).
Epoch [32/100] | train_loss 0.0317 | train_acc 0.9903 | val_loss 0.4963 | val_acc 0.9233
No improvement (13/15).
Epoch [33/100] | train_loss 0.0283 | train_acc 0.9917 | val_loss 0.3940 | val_acc 0.9450
No improvement (14/15).
Epoch [34/100] | train_loss 0.0266 | train_acc 0.9927 | val_loss 0.4642 | val_acc 0.9300
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 55 Finished. Best Val Acc: 95.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6921 | train_acc 0.5320 | val_loss 0.7004 | val_acc 0.5717
Epoch [2/100] | train_loss 0.6422 | train_acc 0.6480 | val_loss 0.5990 | val_acc 0.6950
Epoch [3/100] | train_loss 0.4972 | train_acc 0.7670 | val_loss 0.4715 | val_acc 0.7700
Epoch [4/100] | train_loss 0.3661 | train_acc 0.8420 | val_loss 0.4405 | val_acc 0.8150
Epoch [5/100] | train_loss 0.2560 | train_acc 0.9010 | val_loss 0.3788 | val_acc 0.8767
No improvement (1/15).
Epoch [6/100] | train_loss 0.2400 | train_acc 0.9077 | val_loss 0.5604 | val_acc 0.7867
Epoch [7/100] | train_loss 0.1965 | train_acc 0.9237 | val_loss 0.2641 | val_acc 0.9000
Epoch [8/100] | train_loss 0.1479 | train_acc 0.9497 | val_loss 0.2011 | val_acc 0.9383
No improvement (1/15).
Epoch [9/100] | train_loss 0.1320 | train_acc 0.9547 | val_loss 0.3519 | val_acc 0.8667
No improvement (2/15).
Epoch [10/100] | train_loss 0.1320 | train_acc 0.9530 | val_loss 0.2278 | val_acc 0.9267
No improvement (3/15).
Epoch [11/100] | train_loss 0.1210 | train_acc 0.9557 | val_loss 0.2482 | val_acc 0.9267
Epoch [12/100] | train_loss 0.1165 | train_acc 0.9603 | val_loss 0.1846 | val_acc 0.9467
No improvement (1/15).
Epoch [13/100] | train_loss 0.1158 | train_acc 0.9597 | val_loss 0.2546 | val_acc 0.9267
Epoch [14/100] | train_loss 0.1146 | train_acc 0.9567 | val_loss 0.1742 | val_acc 0.9500
No improvement (1/15).
Epoch [15/100] | train_loss 0.1052 | train_acc 0.9643 | val_loss 0.2029 | val_acc 0.9417
No improvement (2/15).
Epoch [16/100] | train_loss 0.0978 | train_acc 0.9657 | val_loss 0.2130 | val_acc 0.9433
No improvement (3/15).
Epoch [17/100] | train_loss 0.0983 | train_acc 0.9670 | val_loss 0.2165 | val_acc 0.9433
No improvement (4/15).
Epoch [18/100] | train_loss 0.0953 | train_acc 0.9663 | val_loss 0.2082 | val_acc 0.9433
No improvement (5/15).
Epoch [19/100] | train_loss 0.0936 | train_acc 0.9683 | val_loss 0.1869 | val_acc 0.9500
Epoch [20/100] | train_loss 0.0920 | train_acc 0.9683 | val_loss 0.1731 | val_acc 0.9517
Epoch [21/100] | train_loss 0.0936 | train_acc 0.9670 | val_loss 0.1691 | val_acc 0.9533
Epoch [22/100] | train_loss 0.0910 | train_acc 0.9680 | val_loss 0.1669 | val_acc 0.9550
No improvement (1/15).
Epoch [23/100] | train_loss 0.0909 | train_acc 0.9677 | val_loss 0.1694 | val_acc 0.9500
No improvement (2/15).
Epoch [24/100] | train_loss 0.0909 | train_acc 0.9673 | val_loss 0.1664 | val_acc 0.9533
No improvement (3/15).
Epoch [25/100] | train_loss 0.0883 | train_acc 0.9673 | val_loss 0.1715 | val_acc 0.9533
No improvement (4/15).
Epoch [26/100] | train_loss 0.0842 | train_acc 0.9723 | val_loss 0.1658 | val_acc 0.9550
Epoch [27/100] | train_loss 0.0822 | train_acc 0.9727 | val_loss 0.1655 | val_acc 0.9567
No improvement (1/15).
Epoch [28/100] | train_loss 0.0831 | train_acc 0.9733 | val_loss 0.1655 | val_acc 0.9550
Epoch [29/100] | train_loss 0.0815 | train_acc 0.9723 | val_loss 0.1646 | val_acc 0.9583
No improvement (1/15).
Epoch [30/100] | train_loss 0.0825 | train_acc 0.9720 | val_loss 0.1658 | val_acc 0.9567
No improvement (2/15).
Epoch [31/100] | train_loss 0.0810 | train_acc 0.9743 | val_loss 0.1678 | val_acc 0.9550
No improvement (3/15).
Epoch [32/100] | train_loss 0.0803 | train_acc 0.9743 | val_loss 0.1702 | val_acc 0.9517
No improvement (4/15).
Epoch [33/100] | train_loss 0.0795 | train_acc 0.9730 | val_loss 0.1701 | val_acc 0.9533
No improvement (5/15).
Epoch [34/100] | train_loss 0.0795 | train_acc 0.9733 | val_loss 0.1702 | val_acc 0.9533
No improvement (6/15).
Epoch [35/100] | train_loss 0.0795 | train_acc 0.9733 | val_loss 0.1709 | val_acc 0.9533
No improvement (7/15).
Epoch [36/100] | train_loss 0.0792 | train_acc 0.9747 | val_loss 0.1712 | val_acc 0.9533
No improvement (8/15).
Epoch [37/100] | train_loss 0.0781 | train_acc 0.9740 | val_loss 0.1711 | val_acc 0.9533
No improvement (9/15).
Epoch [38/100] | train_loss 0.0793 | train_acc 0.9727 | val_loss 0.1806 | val_acc 0.9500
No improvement (10/15).
Epoch [39/100] | train_loss 0.0768 | train_acc 0.9747 | val_loss 0.1794 | val_acc 0.9500
No improvement (11/15).
Epoch [40/100] | train_loss 0.0770 | train_acc 0.9750 | val_loss 0.1787 | val_acc 0.9500
No improvement (12/15).
Epoch [41/100] | train_loss 0.0767 | train_acc 0.9747 | val_loss 0.1778 | val_acc 0.9483
No improvement (13/15).
Epoch [42/100] | train_loss 0.0780 | train_acc 0.9740 | val_loss 0.1783 | val_acc 0.9500
No improvement (14/15).
Epoch [43/100] | train_loss 0.0774 | train_acc 0.9747 | val_loss 0.1794 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 56 Finished. Best Val Acc: 95.83%
[I 2026-02-06 06:09:09,793] Trial 56 finished with value: 0.9583333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0004979335708967632, 'dropout': 0.33026923583304957}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_060910-ewnysqjl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial57_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ewnysqjl
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 54-55
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–†â–†â–ƒâ–„â–†â–ˆâ–ˆâ–…â–â–…â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–„â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–„â–‚â–ƒâ–‚â–â–â–‚â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–â–â–‚â–„â–„â–ƒâ–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 31
wandb:         grad/norm 1.0
wandb:                lr 4e-05
wandb:         train/acc 0.98333
wandb:        train/loss 0.0475
wandb: training_time_sec 10070.07608
wandb:           val/acc 0.95
wandb:          val/loss 0.24373
wandb: 
wandb: ğŸš€ View run Trial57_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ewnysqjl
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_060910-ewnysqjl/logs
[I 2026-02-06 08:57:02,025] Trial 57 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0011580274465564084, 'dropout': 0.447991648927791}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 2p3zgrrm
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_085702-2p3zgrrm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial58_[CV-Variation]_L4_H2_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2p3zgrrm
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–…â–ˆâ–ˆâ–†â–ˆâ–ˆâ–„â–ˆâ–‡â–†â–‚â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–„â–…â–†â–‡â–…â–‚â–‚â–â–â–â–‚â–„â–‚â–‚â–„â–‚â–â–„â–…â–„â–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–…â–†â–†â–†â–†â–†â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–‚â–…â–†â–…â–‡â–â–…â–†â–†â–ˆâ–ˆâ–†â–…â–„â–†â–„â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:   val/loss â–†â–„â–‚â–„â–‚â–ˆâ–†â–„â–…â–ƒâ–â–„â–…â–†â–…â–‡â–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.86167
wandb:             epoch 40
wandb:         grad/norm 0.45748
wandb:                lr 0.00027
wandb:         train/acc 0.96567
wandb:        train/loss 0.10583
wandb: training_time_sec 5207.82083
wandb:           val/acc 0.85167
wandb:          val/loss 0.65477
wandb: 
wandb: ğŸš€ View run Trial58_[CV-Variation]_L4_H2_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2p3zgrrm
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_085702-2p3zgrrm/logs
[I 2026-02-06 10:23:51,990] Trial 58 finished with value: 0.8616666666666667 and parameters: {'nhead': 2, 'num_layers': 4, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.008731812428186853, 'dropout': 0.4848244808789581}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run wzzk911f
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_102352-wzzk911f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial59_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wzzk911f
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6993 | train_acc 0.5650 | val_loss 0.6607 | val_acc 0.5900
Epoch [2/100] | train_loss 0.5967 | train_acc 0.6763 | val_loss 0.5570 | val_acc 0.7450
Epoch [3/100] | train_loss 0.4327 | train_acc 0.8013 | val_loss 0.4798 | val_acc 0.7800
Epoch [4/100] | train_loss 0.3102 | train_acc 0.8690 | val_loss 0.3610 | val_acc 0.8667
Epoch [5/100] | train_loss 0.2093 | train_acc 0.9163 | val_loss 0.2194 | val_acc 0.9267
No improvement (1/15).
Epoch [6/100] | train_loss 0.1589 | train_acc 0.9423 | val_loss 0.2924 | val_acc 0.9000
Epoch [7/100] | train_loss 0.1788 | train_acc 0.9293 | val_loss 0.2163 | val_acc 0.9317
Epoch [8/100] | train_loss 0.1065 | train_acc 0.9653 | val_loss 0.1708 | val_acc 0.9550
No improvement (1/15).
Epoch [9/100] | train_loss 0.1095 | train_acc 0.9597 | val_loss 0.1860 | val_acc 0.9450
No improvement (2/15).
Epoch [10/100] | train_loss 0.0996 | train_acc 0.9630 | val_loss 0.2111 | val_acc 0.9433
No improvement (3/15).
Epoch [11/100] | train_loss 0.1023 | train_acc 0.9613 | val_loss 0.2880 | val_acc 0.9217
No improvement (4/15).
Epoch [12/100] | train_loss 0.0962 | train_acc 0.9640 | val_loss 0.2819 | val_acc 0.9233
No improvement (5/15).
Epoch [13/100] | train_loss 0.0917 | train_acc 0.9680 | val_loss 0.2009 | val_acc 0.9500
No improvement (6/15).
Epoch [14/100] | train_loss 0.0854 | train_acc 0.9737 | val_loss 0.2552 | val_acc 0.9383
No improvement (7/15).
Epoch [15/100] | train_loss 0.0864 | train_acc 0.9700 | val_loss 0.2240 | val_acc 0.9467
Epoch [16/100] | train_loss 0.0880 | train_acc 0.9700 | val_loss 0.2022 | val_acc 0.9567
Epoch [17/100] | train_loss 0.0813 | train_acc 0.9710 | val_loss 0.2123 | val_acc 0.9600
No improvement (1/15).
Epoch [18/100] | train_loss 0.0811 | train_acc 0.9720 | val_loss 0.2031 | val_acc 0.9533
No improvement (2/15).
Epoch [19/100] | train_loss 0.0786 | train_acc 0.9710 | val_loss 0.2047 | val_acc 0.9517
No improvement (3/15).
Epoch [20/100] | train_loss 0.0611 | train_acc 0.9783 | val_loss 0.2373 | val_acc 0.9450
No improvement (4/15).
Epoch [21/100] | train_loss 0.0640 | train_acc 0.9780 | val_loss 0.3778 | val_acc 0.9150
No improvement (5/15).
Epoch [22/100] | train_loss 0.0675 | train_acc 0.9760 | val_loss 0.3544 | val_acc 0.9100
No improvement (6/15).
Epoch [23/100] | train_loss 0.0718 | train_acc 0.9740 | val_loss 0.3216 | val_acc 0.9167
No improvement (7/15).
Epoch [24/100] | train_loss 0.0701 | train_acc 0.9750 | val_loss 0.3162 | val_acc 0.9217
No improvement (8/15).
Epoch [25/100] | train_loss 0.0668 | train_acc 0.9770 | val_loss 0.3687 | val_acc 0.9150
No improvement (9/15).
Epoch [26/100] | train_loss 0.0589 | train_acc 0.9800 | val_loss 0.2168 | val_acc 0.9533
No improvement (10/15).
Epoch [27/100] | train_loss 0.0502 | train_acc 0.9847 | val_loss 0.2406 | val_acc 0.9483
No improvement (11/15).
Epoch [28/100] | train_loss 0.0527 | train_acc 0.9827 | val_loss 0.2381 | val_acc 0.9500
No improvement (12/15).
Epoch [29/100] | train_loss 0.0509 | train_acc 0.9817 | val_loss 0.2411 | val_acc 0.9533
No improvement (13/15).
Epoch [30/100] | train_loss 0.0493 | train_acc 0.9833 | val_loss 0.2466 | val_acc 0.9550
No improvement (14/15).
Epoch [31/100] | train_loss 0.0475 | train_acc 0.9833 | val_loss 0.2437 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 57 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.8107 | train_acc 0.6080 | val_loss 0.5941 | val_acc 0.7567
Epoch [2/100] | train_loss 0.4188 | train_acc 0.8003 | val_loss 0.5198 | val_acc 0.8033
Epoch [3/100] | train_loss 0.3340 | train_acc 0.8593 | val_loss 0.4394 | val_acc 0.8283
No improvement (1/15).
Epoch [4/100] | train_loss 0.3284 | train_acc 0.8583 | val_loss 0.5133 | val_acc 0.8000
Epoch [5/100] | train_loss 0.2969 | train_acc 0.8680 | val_loss 0.4267 | val_acc 0.8367
No improvement (1/15).
Epoch [6/100] | train_loss 0.2934 | train_acc 0.8823 | val_loss 0.7015 | val_acc 0.7350
No improvement (2/15).
Epoch [7/100] | train_loss 0.2853 | train_acc 0.8820 | val_loss 0.5858 | val_acc 0.8033
No improvement (3/15).
Epoch [8/100] | train_loss 0.2546 | train_acc 0.9023 | val_loss 0.5110 | val_acc 0.8283
No improvement (4/15).
Epoch [9/100] | train_loss 0.2616 | train_acc 0.8890 | val_loss 0.5528 | val_acc 0.8250
Epoch [10/100] | train_loss 0.2980 | train_acc 0.8787 | val_loss 0.4425 | val_acc 0.8533
Epoch [11/100] | train_loss 0.2406 | train_acc 0.9037 | val_loss 0.3696 | val_acc 0.8567
No improvement (1/15).
Epoch [12/100] | train_loss 0.2708 | train_acc 0.8900 | val_loss 0.4963 | val_acc 0.8283
No improvement (2/15).
Epoch [13/100] | train_loss 0.2298 | train_acc 0.9083 | val_loss 0.5409 | val_acc 0.8100
No improvement (3/15).
Epoch [14/100] | train_loss 0.2158 | train_acc 0.9137 | val_loss 0.6174 | val_acc 0.7867
No improvement (4/15).
Epoch [15/100] | train_loss 0.2112 | train_acc 0.9183 | val_loss 0.5559 | val_acc 0.8200
No improvement (5/15).
Epoch [16/100] | train_loss 0.1988 | train_acc 0.9193 | val_loss 0.6352 | val_acc 0.7833
No improvement (6/15).
Epoch [17/100] | train_loss 0.1973 | train_acc 0.9207 | val_loss 0.5208 | val_acc 0.8250
No improvement (7/15).
Epoch [18/100] | train_loss 0.1842 | train_acc 0.9297 | val_loss 0.5045 | val_acc 0.8317
No improvement (8/15).
Epoch [19/100] | train_loss 0.1755 | train_acc 0.9357 | val_loss 0.4908 | val_acc 0.8433
No improvement (9/15).
Epoch [20/100] | train_loss 0.1628 | train_acc 0.9403 | val_loss 0.4995 | val_acc 0.8467
No improvement (10/15).
Epoch [21/100] | train_loss 0.1636 | train_acc 0.9400 | val_loss 0.4933 | val_acc 0.8517
No improvement (11/15).
Epoch [22/100] | train_loss 0.1566 | train_acc 0.9447 | val_loss 0.5149 | val_acc 0.8483
No improvement (12/15).
Epoch [23/100] | train_loss 0.1548 | train_acc 0.9427 | val_loss 0.5351 | val_acc 0.8333
No improvement (13/15).
Epoch [24/100] | train_loss 0.1522 | train_acc 0.9477 | val_loss 0.5188 | val_acc 0.8417
Epoch [25/100] | train_loss 0.1371 | train_acc 0.9510 | val_loss 0.5301 | val_acc 0.8583
Epoch [26/100] | train_loss 0.1373 | train_acc 0.9533 | val_loss 0.5444 | val_acc 0.8617
No improvement (1/15).
Epoch [27/100] | train_loss 0.1355 | train_acc 0.9537 | val_loss 0.5498 | val_acc 0.8617
No improvement (2/15).
Epoch [28/100] | train_loss 0.1333 | train_acc 0.9557 | val_loss 0.5549 | val_acc 0.8533
No improvement (3/15).
Epoch [29/100] | train_loss 0.1310 | train_acc 0.9527 | val_loss 0.5678 | val_acc 0.8567
No improvement (4/15).
Epoch [30/100] | train_loss 0.1309 | train_acc 0.9580 | val_loss 0.5792 | val_acc 0.8550
No improvement (5/15).
Epoch [31/100] | train_loss 0.1214 | train_acc 0.9597 | val_loss 0.5384 | val_acc 0.8550
No improvement (6/15).
Epoch [32/100] | train_loss 0.1171 | train_acc 0.9603 | val_loss 0.5600 | val_acc 0.8600
No improvement (7/15).
Epoch [33/100] | train_loss 0.1169 | train_acc 0.9597 | val_loss 0.5704 | val_acc 0.8617
No improvement (8/15).
Epoch [34/100] | train_loss 0.1147 | train_acc 0.9593 | val_loss 0.5828 | val_acc 0.8617
No improvement (9/15).
Epoch [35/100] | train_loss 0.1156 | train_acc 0.9633 | val_loss 0.6062 | val_acc 0.8567
No improvement (10/15).
Epoch [36/100] | train_loss 0.1106 | train_acc 0.9640 | val_loss 0.5977 | val_acc 0.8567
No improvement (11/15).
Epoch [37/100] | train_loss 0.1092 | train_acc 0.9600 | val_loss 0.5876 | val_acc 0.8567
No improvement (12/15).
Epoch [38/100] | train_loss 0.1085 | train_acc 0.9637 | val_loss 0.5997 | val_acc 0.8583
No improvement (13/15).
Epoch [39/100] | train_loss 0.1076 | train_acc 0.9633 | val_loss 0.6070 | val_acc 0.8533
No improvement (14/15).
Epoch [40/100] | train_loss 0.1058 | train_acc 0.9657 | val_loss 0.6548 | val_acc 0.8517
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 58 Finished. Best Val Acc: 86.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7231 | train_acc 0.5437 | val_loss 0.6492 | val_acc 0.5900
Epoch [2/100] | train_loss 0.4198 | train_acc 0.7893 | val_loss 0.4267 | val_acc 0.8383
No improvement (1/15).
Epoch [3/100] | train_loss 0.2328 | train_acc 0.9100 | val_loss 0.5551 | val_acc 0.8000
Epoch [4/100] | train_loss 0.2292 | train_acc 0.9087 | val_loss 0.2978 | val_acc 0.8733wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–†â–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–†â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–„â–ˆâ–â–‚â–â–â–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–…â–†â–‡â–†â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡
wandb:   val/loss â–ˆâ–…â–‡â–ƒâ–‚â–ƒâ–â–‚â–â–â–‚â–‚â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95167
wandb:             epoch 30
wandb:         grad/norm 0.19058
wandb:                lr 0.00013
wandb:         train/acc 0.99
wandb:        train/loss 0.02667
wandb: training_time_sec 12987.73807
wandb:           val/acc 0.92167
wandb:          val/loss 0.35559
wandb: 
wandb: ğŸš€ View run Trial59_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wzzk911f
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_102352-wzzk911f/logs
[I 2026-02-06 14:00:21,794] Trial 59 finished with value: 0.9516666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0021119795206852076, 'dropout': 0.4030216658881626}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 39s9og6r
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_140022-39s9og6r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial60_[CV-Variation]_L4_H8_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/39s9og6r
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 1-1
wandb: ğŸš€ View run Trial60_[CV-Variation]_L4_H8_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/39s9og6r
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_140022-39s9og6r/logs
[I 2026-02-06 14:07:02,372] Trial 60 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 16, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0007821135586251957, 'dropout': 0.3670733387264324}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run icqzee06
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_140702-icqzee06
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial61_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/icqzee06
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading summary, console lines 68-69
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–‡â–‚â–‡â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–‡â–†â–ƒâ–ˆâ–ƒâ–…â–‚â–ƒâ–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95667
wandb:             epoch 38
wandb:         grad/norm 1.0
wandb:                lr 4e-05
wandb:         train/acc 0.98233
wandb:        train/loss 0.04778
wandb: training_time_sec 12264.81215
wandb:           val/acc 0.95333
wandb:          val/loss 0.2078
wandb: 
wandb: ğŸš€ View run Trial61_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/icqzee06
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_140702-icqzee06/logs
[I 2026-02-06 17:31:29,689] Trial 61 finished with value: 0.9566666666666667 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.001184934217646239, 'dropout': 0.44472018646832073}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 16uh1di0
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_173130-16uh1di0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial62_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/16uh1di0

Epoch [5/100] | train_loss 0.1680 | train_acc 0.9360 | val_loss 0.2086 | val_acc 0.9233
No improvement (1/15).
Epoch [6/100] | train_loss 0.1951 | train_acc 0.9240 | val_loss 0.3224 | val_acc 0.8683
Epoch [7/100] | train_loss 0.1589 | train_acc 0.9403 | val_loss 0.1740 | val_acc 0.9417
No improvement (1/15).
Epoch [8/100] | train_loss 0.1122 | train_acc 0.9590 | val_loss 0.2527 | val_acc 0.9200
No improvement (2/15).
Epoch [9/100] | train_loss 0.1008 | train_acc 0.9620 | val_loss 0.1881 | val_acc 0.9350
No improvement (3/15).
Epoch [10/100] | train_loss 0.0901 | train_acc 0.9677 | val_loss 0.1908 | val_acc 0.9317
No improvement (4/15).
Epoch [11/100] | train_loss 0.0871 | train_acc 0.9653 | val_loss 0.2357 | val_acc 0.9183
No improvement (5/15).
Epoch [12/100] | train_loss 0.0975 | train_acc 0.9647 | val_loss 0.2331 | val_acc 0.9067
No improvement (6/15).
Epoch [13/100] | train_loss 0.0818 | train_acc 0.9697 | val_loss 0.3029 | val_acc 0.8800
No improvement (7/15).
Epoch [14/100] | train_loss 0.0732 | train_acc 0.9730 | val_loss 0.2675 | val_acc 0.9300
Epoch [15/100] | train_loss 0.0777 | train_acc 0.9720 | val_loss 0.1913 | val_acc 0.9483
Epoch [16/100] | train_loss 0.0778 | train_acc 0.9687 | val_loss 0.1700 | val_acc 0.9517
No improvement (1/15).
Epoch [17/100] | train_loss 0.0645 | train_acc 0.9753 | val_loss 0.2120 | val_acc 0.9417
No improvement (2/15).
Epoch [18/100] | train_loss 0.0837 | train_acc 0.9670 | val_loss 0.1923 | val_acc 0.9500
No improvement (3/15).
Epoch [19/100] | train_loss 0.0585 | train_acc 0.9757 | val_loss 0.2318 | val_acc 0.9417
No improvement (4/15).
Epoch [20/100] | train_loss 0.0561 | train_acc 0.9783 | val_loss 0.2155 | val_acc 0.9383
No improvement (5/15).
Epoch [21/100] | train_loss 0.0429 | train_acc 0.9860 | val_loss 0.2419 | val_acc 0.9317
No improvement (6/15).
Epoch [22/100] | train_loss 0.0455 | train_acc 0.9833 | val_loss 0.2610 | val_acc 0.9367
No improvement (7/15).
Epoch [23/100] | train_loss 0.0390 | train_acc 0.9873 | val_loss 0.2895 | val_acc 0.9317
No improvement (8/15).
Epoch [24/100] | train_loss 0.0397 | train_acc 0.9870 | val_loss 0.2643 | val_acc 0.9283
No improvement (9/15).
Epoch [25/100] | train_loss 0.0504 | train_acc 0.9820 | val_loss 0.3523 | val_acc 0.9217
No improvement (10/15).
Epoch [26/100] | train_loss 0.0384 | train_acc 0.9863 | val_loss 0.2963 | val_acc 0.9233
No improvement (11/15).
Epoch [27/100] | train_loss 0.0458 | train_acc 0.9807 | val_loss 0.2776 | val_acc 0.9317
No improvement (12/15).
Epoch [28/100] | train_loss 0.0303 | train_acc 0.9887 | val_loss 0.3015 | val_acc 0.9250
No improvement (13/15).
Epoch [29/100] | train_loss 0.0316 | train_acc 0.9877 | val_loss 0.3046 | val_acc 0.9350
No improvement (14/15).
Epoch [30/100] | train_loss 0.0267 | train_acc 0.9900 | val_loss 0.3556 | val_acc 0.9217
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 59 Finished. Best Val Acc: 95.17%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 60 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.32 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.96 MiB is allocated by PyTorch, and 3.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6970 | train_acc 0.5437 | val_loss 0.6797 | val_acc 0.5983
Epoch [2/100] | train_loss 0.6196 | train_acc 0.6473 | val_loss 0.6317 | val_acc 0.6617
Epoch [3/100] | train_loss 0.3550 | train_acc 0.8353 | val_loss 0.3548 | val_acc 0.8900
No improvement (1/15).
Epoch [4/100] | train_loss 0.3016 | train_acc 0.8690 | val_loss 0.8114 | val_acc 0.6700
Epoch [5/100] | train_loss 0.3230 | train_acc 0.8820 | val_loss 0.3156 | val_acc 0.8950
No improvement (1/15).
Epoch [6/100] | train_loss 0.2372 | train_acc 0.9083 | val_loss 0.4907 | val_acc 0.8600
Epoch [7/100] | train_loss 0.2229 | train_acc 0.9147 | val_loss 0.2707 | val_acc 0.9150
No improvement (1/15).
Epoch [8/100] | train_loss 0.1420 | train_acc 0.9443 | val_loss 0.3041 | val_acc 0.8850
Epoch [9/100] | train_loss 0.1589 | train_acc 0.9393 | val_loss 0.1685 | val_acc 0.9467
Epoch [10/100] | train_loss 0.1315 | train_acc 0.9530 | val_loss 0.1552 | val_acc 0.9517
No improvement (1/15).
Epoch [11/100] | train_loss 0.1051 | train_acc 0.9617 | val_loss 0.1888 | val_acc 0.9483
No improvement (2/15).
Epoch [12/100] | train_loss 0.0986 | train_acc 0.9623 | val_loss 0.1971 | val_acc 0.9367
No improvement (3/15).
Epoch [13/100] | train_loss 0.1098 | train_acc 0.9600 | val_loss 0.1591 | val_acc 0.9517
No improvement (4/15).
Epoch [14/100] | train_loss 0.0895 | train_acc 0.9690 | val_loss 0.1889 | val_acc 0.9500
Epoch [15/100] | train_loss 0.0907 | train_acc 0.9677 | val_loss 0.1503 | val_acc 0.9550
No improvement (1/15).
Epoch [16/100] | train_loss 0.0899 | train_acc 0.9660 | val_loss 0.2559 | val_acc 0.9233
No improvement (2/15).
Epoch [17/100] | train_loss 0.0952 | train_acc 0.9657 | val_loss 0.1490 | val_acc 0.9550
No improvement (3/15).
Epoch [18/100] | train_loss 0.0727 | train_acc 0.9737 | val_loss 0.1716 | val_acc 0.9517
No improvement (4/15).
Epoch [19/100] | train_loss 0.0828 | train_acc 0.9693 | val_loss 0.1798 | val_acc 0.9483
No improvement (5/15).
Epoch [20/100] | train_loss 0.0921 | train_acc 0.9667 | val_loss 0.1895 | val_acc 0.9467
No improvement (6/15).
Epoch [21/100] | train_loss 0.0837 | train_acc 0.9703 | val_loss 0.1837 | val_acc 0.9500
No improvement (7/15).
Epoch [22/100] | train_loss 0.0993 | train_acc 0.9623 | val_loss 0.1535 | val_acc 0.9533
No improvement (8/15).
Epoch [23/100] | train_loss 0.0636 | train_acc 0.9767 | val_loss 0.1931 | val_acc 0.9533
Epoch [24/100] | train_loss 0.0606 | train_acc 0.9780 | val_loss 0.1819 | val_acc 0.9567
No improvement (1/15).
Epoch [25/100] | train_loss 0.0626 | train_acc 0.9787 | val_loss 0.2043 | val_acc 0.9533
No improvement (2/15).
Epoch [26/100] | train_loss 0.0629 | train_acc 0.9797 | val_loss 0.2050 | val_acc 0.9533
No improvement (3/15).
Epoch [27/100] | train_loss 0.0640 | train_acc 0.9780 | val_loss 0.1955 | val_acc 0.9567
No improvement (4/15).
Epoch [28/100] | train_loss 0.0597 | train_acc 0.9787 | val_loss 0.2186 | val_acc 0.9517
No improvement (5/15).
Epoch [29/100] | train_loss 0.0608 | train_acc 0.9790 | val_loss 0.1820 | val_acc 0.9533
No improvement (6/15).
Epoch [30/100] | train_loss 0.0553 | train_acc 0.9807 | val_loss 0.1918 | val_acc 0.9550
No improvement (7/15).
Epoch [31/100] | train_loss 0.0525 | train_acc 0.9827 | val_loss 0.1892 | val_acc 0.9550
No improvement (8/15).
Epoch [32/100] | train_loss 0.0519 | train_acc 0.9817 | val_loss 0.2024 | val_acc 0.9567
No improvement (9/15).
Epoch [33/100] | train_loss 0.0489 | train_acc 0.9827 | val_loss 0.1995 | val_acc 0.9567
No improvement (10/15).
Epoch [34/100] | train_loss 0.0521 | train_acc 0.9820 | val_loss 0.2016 | val_acc 0.9567
No improvement (11/15).
Epoch [35/100] | train_loss 0.0494 | train_acc 0.9833 | val_loss 0.2029 | val_acc 0.9517
No improvement (12/15).
Epoch [36/100] | train_loss 0.0486 | train_acc 0.9837 | val_loss 0.2050 | val_acc 0.9517
No improvement (13/15).
Epoch [37/100] | train_loss 0.0492 | train_acc 0.9820 | val_loss 0.2088 | val_acc 0.9533
No improvement (14/15).
Epoch [38/100] | train_loss 0.0478 | train_acc 0.9823 | val_loss 0.2078 | val_acc 0.9533
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 61 Finished. Best Val Acc: 95.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7265 | train_acc 0.5377 | val_loss 0.6780 | val_acc 0.5917
Epoch [2/100] | train_loss 0.6466 | train_acc 0.6163 | val_loss 0.5889 | val_acc 0.6317
Epoch [3/100] | train_loss 0.4891 | train_acc 0.7497 | val_loss 0.4210 | val_acc 0.8117
Epoch [4/100] | train_loss 0.2914 | train_acc 0.8743 | val_loss 0.3435 | val_acc 0.8717
Epoch [5/100] | train_loss 0.2459 | train_acc 0.9057 | val_loss 0.2028 | val_acc 0.9200wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–„â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–†â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–†â–„â–ˆâ–â–‚â–‚â–â–‚â–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–„â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–„â–ƒâ–â–â–â–â–â–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95
wandb:             epoch 31
wandb:         grad/norm 0.32063
wandb:                lr 4e-05
wandb:         train/acc 0.98733
wandb:        train/loss 0.04456
wandb: training_time_sec 10055.49744
wandb:           val/acc 0.945
wandb:          val/loss 0.3247
wandb: 
wandb: ğŸš€ View run Trial62_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/16uh1di0
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_173130-16uh1di0/logs
[I 2026-02-06 20:19:07,294] Trial 62 finished with value: 0.95 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0013590509576235441, 'dropout': 0.45378981786183903}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 289bw6ou
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_201907-289bw6ou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial63_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/289bw6ou
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 57-58
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–„â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–„â–ˆâ–…â–ˆâ–ˆâ–„â–ˆâ–ƒâ–„â–ƒâ–„â–‚â–â–ƒâ–…
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–„â–‡â–‡â–‡â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–‡â–†â–ˆâ–‚â–‚â–‚â–…â–ƒâ–ƒâ–‚â–‚â–â–â–ƒâ–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96167
wandb:             epoch 33
wandb:         grad/norm 0.61628
wandb:                lr 3e-05
wandb:         train/acc 0.98
wandb:        train/loss 0.06554
wandb: training_time_sec 10678.88967
wandb:           val/acc 0.93333
wandb:          val/loss 0.26133
wandb: 
wandb: ğŸš€ View run Trial63_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/289bw6ou
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_201907-289bw6ou/logs
[I 2026-02-06 23:17:08,517] Trial 63 finished with value: 0.9616666666666667 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.000989038774516702, 'dropout': 0.4378293643116034}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run q297sm0u
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260206_231708-q297sm0u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial64_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/q297sm0u

Epoch [6/100] | train_loss 0.1602 | train_acc 0.9407 | val_loss 0.2133 | val_acc 0.9217
Epoch [7/100] | train_loss 0.1310 | train_acc 0.9500 | val_loss 0.1887 | val_acc 0.9433
No improvement (1/15).
Epoch [8/100] | train_loss 0.0992 | train_acc 0.9670 | val_loss 0.2189 | val_acc 0.9383
No improvement (2/15).
Epoch [9/100] | train_loss 0.0896 | train_acc 0.9677 | val_loss 0.2097 | val_acc 0.9433
No improvement (3/15).
Epoch [10/100] | train_loss 0.0887 | train_acc 0.9683 | val_loss 0.2975 | val_acc 0.9317
No improvement (4/15).
Epoch [11/100] | train_loss 0.0903 | train_acc 0.9680 | val_loss 0.2785 | val_acc 0.9250
No improvement (5/15).
Epoch [12/100] | train_loss 0.0816 | train_acc 0.9713 | val_loss 0.3935 | val_acc 0.9100
No improvement (6/15).
Epoch [13/100] | train_loss 0.0831 | train_acc 0.9703 | val_loss 0.2786 | val_acc 0.9183
No improvement (7/15).
Epoch [14/100] | train_loss 0.0801 | train_acc 0.9703 | val_loss 0.2477 | val_acc 0.9333
No improvement (8/15).
Epoch [15/100] | train_loss 0.0770 | train_acc 0.9737 | val_loss 0.2402 | val_acc 0.9433
Epoch [16/100] | train_loss 0.0738 | train_acc 0.9757 | val_loss 0.2406 | val_acc 0.9450
Epoch [17/100] | train_loss 0.0710 | train_acc 0.9770 | val_loss 0.2295 | val_acc 0.9500
No improvement (1/15).
Epoch [18/100] | train_loss 0.0685 | train_acc 0.9777 | val_loss 0.2299 | val_acc 0.9483
No improvement (2/15).
Epoch [19/100] | train_loss 0.0646 | train_acc 0.9797 | val_loss 0.2476 | val_acc 0.9467
No improvement (3/15).
Epoch [20/100] | train_loss 0.0567 | train_acc 0.9803 | val_loss 0.2587 | val_acc 0.9450
No improvement (4/15).
Epoch [21/100] | train_loss 0.0532 | train_acc 0.9840 | val_loss 0.2519 | val_acc 0.9500
No improvement (5/15).
Epoch [22/100] | train_loss 0.0499 | train_acc 0.9847 | val_loss 0.2527 | val_acc 0.9483
No improvement (6/15).
Epoch [23/100] | train_loss 0.0510 | train_acc 0.9847 | val_loss 0.2706 | val_acc 0.9467
No improvement (7/15).
Epoch [24/100] | train_loss 0.0497 | train_acc 0.9867 | val_loss 0.2699 | val_acc 0.9467
No improvement (8/15).
Epoch [25/100] | train_loss 0.0498 | train_acc 0.9837 | val_loss 0.2964 | val_acc 0.9433
No improvement (9/15).
Epoch [26/100] | train_loss 0.0509 | train_acc 0.9837 | val_loss 0.2780 | val_acc 0.9483
No improvement (10/15).
Epoch [27/100] | train_loss 0.0473 | train_acc 0.9847 | val_loss 0.3061 | val_acc 0.9450
No improvement (11/15).
Epoch [28/100] | train_loss 0.0472 | train_acc 0.9847 | val_loss 0.3000 | val_acc 0.9417
No improvement (12/15).
Epoch [29/100] | train_loss 0.0454 | train_acc 0.9850 | val_loss 0.3075 | val_acc 0.9433
No improvement (13/15).
Epoch [30/100] | train_loss 0.0430 | train_acc 0.9863 | val_loss 0.3217 | val_acc 0.9417
No improvement (14/15).
Epoch [31/100] | train_loss 0.0446 | train_acc 0.9873 | val_loss 0.3247 | val_acc 0.9450
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 62 Finished. Best Val Acc: 95.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6900 | train_acc 0.5420 | val_loss 0.6797 | val_acc 0.5867
No improvement (1/15).
Epoch [2/100] | train_loss 0.5990 | train_acc 0.6367 | val_loss 0.5966 | val_acc 0.5583
Epoch [3/100] | train_loss 0.3724 | train_acc 0.8300 | val_loss 0.7276 | val_acc 0.7083
Epoch [4/100] | train_loss 0.2760 | train_acc 0.8813 | val_loss 0.2668 | val_acc 0.8950
Epoch [5/100] | train_loss 0.1991 | train_acc 0.9187 | val_loss 0.2417 | val_acc 0.9067
Epoch [6/100] | train_loss 0.1609 | train_acc 0.9360 | val_loss 0.2301 | val_acc 0.9117
No improvement (1/15).
Epoch [7/100] | train_loss 0.1634 | train_acc 0.9337 | val_loss 0.4786 | val_acc 0.8133
No improvement (2/15).
Epoch [8/100] | train_loss 0.1699 | train_acc 0.9337 | val_loss 0.3242 | val_acc 0.8900
No improvement (3/15).
Epoch [9/100] | train_loss 0.1452 | train_acc 0.9480 | val_loss 0.3050 | val_acc 0.8967
Epoch [10/100] | train_loss 0.1120 | train_acc 0.9597 | val_loss 0.2264 | val_acc 0.9350
Epoch [11/100] | train_loss 0.1086 | train_acc 0.9630 | val_loss 0.2464 | val_acc 0.9367
Epoch [12/100] | train_loss 0.1131 | train_acc 0.9590 | val_loss 0.1896 | val_acc 0.9500
No improvement (1/15).
Epoch [13/100] | train_loss 0.0906 | train_acc 0.9690 | val_loss 0.1931 | val_acc 0.9450
No improvement (2/15).
Epoch [14/100] | train_loss 0.1023 | train_acc 0.9660 | val_loss 0.2909 | val_acc 0.9100
No improvement (3/15).
Epoch [15/100] | train_loss 0.1232 | train_acc 0.9540 | val_loss 0.2313 | val_acc 0.9367
Epoch [16/100] | train_loss 0.1096 | train_acc 0.9577 | val_loss 0.1588 | val_acc 0.9583
No improvement (1/15).
Epoch [17/100] | train_loss 0.0821 | train_acc 0.9713 | val_loss 0.1646 | val_acc 0.9583
No improvement (2/15).
Epoch [18/100] | train_loss 0.1086 | train_acc 0.9570 | val_loss 0.1634 | val_acc 0.9583
Epoch [19/100] | train_loss 0.0982 | train_acc 0.9630 | val_loss 0.1567 | val_acc 0.9617
No improvement (1/15).
Epoch [20/100] | train_loss 0.0777 | train_acc 0.9760 | val_loss 0.1871 | val_acc 0.9450
No improvement (2/15).
Epoch [21/100] | train_loss 0.0967 | train_acc 0.9633 | val_loss 0.1689 | val_acc 0.9617
No improvement (3/15).
Epoch [22/100] | train_loss 0.0752 | train_acc 0.9747 | val_loss 0.1987 | val_acc 0.9500
No improvement (4/15).
Epoch [23/100] | train_loss 0.0758 | train_acc 0.9737 | val_loss 0.2147 | val_acc 0.9450
No improvement (5/15).
Epoch [24/100] | train_loss 0.0723 | train_acc 0.9750 | val_loss 0.2072 | val_acc 0.9450
No improvement (6/15).
Epoch [25/100] | train_loss 0.0730 | train_acc 0.9750 | val_loss 0.2168 | val_acc 0.9450
No improvement (7/15).
Epoch [26/100] | train_loss 0.0710 | train_acc 0.9767 | val_loss 0.2123 | val_acc 0.9467
No improvement (8/15).
Epoch [27/100] | train_loss 0.0696 | train_acc 0.9770 | val_loss 0.2054 | val_acc 0.9467
No improvement (9/15).
Epoch [28/100] | train_loss 0.0660 | train_acc 0.9793 | val_loss 0.2051 | val_acc 0.9483
No improvement (10/15).
Epoch [29/100] | train_loss 0.0663 | train_acc 0.9803 | val_loss 0.2366 | val_acc 0.9383
No improvement (11/15).
Epoch [30/100] | train_loss 0.0665 | train_acc 0.9793 | val_loss 0.2335 | val_acc 0.9367
No improvement (12/15).
Epoch [31/100] | train_loss 0.0676 | train_acc 0.9797 | val_loss 0.2489 | val_acc 0.9350
No improvement (13/15).
Epoch [32/100] | train_loss 0.0695 | train_acc 0.9787 | val_loss 0.2372 | val_acc 0.9350
No improvement (14/15).
Epoch [33/100] | train_loss 0.0655 | train_acc 0.9800 | val_loss 0.2613 | val_acc 0.9333
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 63 Finished. Best Val Acc: 96.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6941 | train_acc 0.5153 | val_loss 0.6828 | val_acc 0.5983
Epoch [2/100] | train_loss 0.6645 | train_acc 0.6217 | val_loss 0.6689 | val_acc 0.6300
Epoch [3/100] | train_loss 0.6321 | train_acc 0.6400 | val_loss 0.5793 | val_acc 0.6367
Epoch [4/100] | train_loss 0.5025 | train_acc 0.7500 | val_loss 0.4047 | val_acc 0.8233
No improvement (1/15).
Epoch [5/100] | train_loss 0.3410 | train_acc 0.8510 | val_loss 0.6654 | val_acc 0.7983
Epoch [6/100] | train_loss 0.2612 | train_acc 0.8883 | val_loss 0.3152 | val_acc 0.8783
Epoch [7/100] | train_loss 0.2300 | train_acc 0.9053 | val_loss 0.3137 | val_acc 0.8883
Epoch [8/100] | train_loss 0.1628 | train_acc 0.9403 | val_loss 0.2742 | val_acc 0.9117
Epoch [9/100] | train_loss 0.1567 | train_acc 0.9420 | val_loss 0.2732 | val_acc 0.9217
No improvement (1/15).
Epoch [10/100] | train_loss 0.1428 | train_acc 0.9473 | val_loss 0.2623 | val_acc 0.9217
Epoch [11/100] | train_loss 0.1340 | train_acc 0.9517 | val_loss 0.2650 | val_acc 0.9233
No improvement (1/15).
Epoch [12/100] | train_loss 0.1365 | train_acc 0.9503 | val_loss 0.2800 | val_acc 0.9233
Epoch [13/100] | train_loss 0.1325 | train_acc 0.9463 | val_loss 0.2307 | val_acc 0.9317
Epoch [14/100] | train_loss 0.1114 | train_acc 0.9587 | val_loss 0.1864 | val_acc 0.9467
No improvement (1/15).
Epoch [15/100] | train_loss 0.1089 | train_acc 0.9597 | val_loss 0.1818 | val_acc 0.9467
Epoch [16/100] | train_loss 0.1035 | train_acc 0.9613 | val_loss 0.1904 | val_acc 0.9483
No improvement (1/15).
Epoch [17/100] | train_loss 0.1042 | train_acc 0.9627 | val_loss 0.1894 | val_acc 0.9467wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–ƒâ–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–‚â–…â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–‡â–„â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95333
wandb:             epoch 38
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.97033
wandb:        train/loss 0.08066
wandb: training_time_sec 12283.9681
wandb:           val/acc 0.945
wandb:          val/loss 0.21459
wandb: 
wandb: ğŸš€ View run Trial64_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/q297sm0u
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260206_231708-q297sm0u/logs
[I 2026-02-07 02:41:54,450] Trial 64 finished with value: 0.9533333333333334 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0005736146167458492, 'dropout': 0.4341484815136538}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run o4ocd75d
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_024154-o4ocd75d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial65_[CV-Variation]_L3_H8_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/o4ocd75d
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 1-1
wandb: ğŸš€ View run Trial65_[CV-Variation]_L3_H8_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/o4ocd75d
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_024154-o4ocd75d/logs
[I 2026-02-07 02:46:59,205] Trial 65 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 32, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0018149380977374263, 'dropout': 0.4840003386894712}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run ofkyshnu
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_024659-ofkyshnu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial66_[CV-Variation]_L2_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ofkyshnu
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 40-41
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–‚â–ˆâ–ˆâ–„
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–†â–†â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–ƒâ–…â–â–ƒâ–â–â–â–â–â–â–â–â–â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95167
wandb:             epoch 23
wandb:         grad/norm 0.58165
wandb:                lr 0.00035
wandb:         train/acc 0.985
wandb:        train/loss 0.04383
wandb: training_time_sec 4989.96681
wandb:           val/acc 0.93667
wandb:          val/loss 0.20755
wandb: 
wandb: ğŸš€ View run Trial66_[CV-Variation]_L2_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ofkyshnu
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_024659-ofkyshnu/logs
[I 2026-02-07 04:10:11,401] Trial 66 finished with value: 0.9516666666666667 and parameters: {'nhead': 8, 'num_layers': 2, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0027832132070462227, 'dropout': 0.1538712661084658}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 3987yt6q
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_041011-3987yt6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial67_[CV-Variation]_L4_H2_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3987yt6q
wandb: updating run metadata; uploading console lines 0-0
wandb: uploading requirements.txt; uploading output.log; uploading config.yaml
wandb: ğŸš€ View run Trial67_[CV-Variation]_L4_H2_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3987yt6q
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_041011-3987yt6q/logs
[I 2026-02-07 04:10:13,536] Trial 67 finished with value: 0.0 and parameters: {'nhead': 2, 'num_layers': 4, 'd_model': 128, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0010205477508543012, 'dropout': 0.39270234476027094}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_041013-sn6ek3ie
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial68_[CV-Variation]_L1_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sn6ek3ie

No improvement (2/15).
Epoch [18/100] | train_loss 0.1048 | train_acc 0.9640 | val_loss 0.1889 | val_acc 0.9433
No improvement (3/15).
Epoch [19/100] | train_loss 0.1044 | train_acc 0.9623 | val_loss 0.2163 | val_acc 0.9350
Epoch [20/100] | train_loss 0.1009 | train_acc 0.9613 | val_loss 0.1757 | val_acc 0.9500
Epoch [21/100] | train_loss 0.0958 | train_acc 0.9630 | val_loss 0.1756 | val_acc 0.9517
No improvement (1/15).
Epoch [22/100] | train_loss 0.0922 | train_acc 0.9650 | val_loss 0.1788 | val_acc 0.9517
No improvement (2/15).
Epoch [23/100] | train_loss 0.0914 | train_acc 0.9657 | val_loss 0.1814 | val_acc 0.9517
Epoch [24/100] | train_loss 0.0912 | train_acc 0.9653 | val_loss 0.1840 | val_acc 0.9533
No improvement (1/15).
Epoch [25/100] | train_loss 0.0926 | train_acc 0.9637 | val_loss 0.1834 | val_acc 0.9500
No improvement (2/15).
Epoch [26/100] | train_loss 0.0861 | train_acc 0.9677 | val_loss 0.2000 | val_acc 0.9483
No improvement (3/15).
Epoch [27/100] | train_loss 0.0867 | train_acc 0.9673 | val_loss 0.1932 | val_acc 0.9483
No improvement (4/15).
Epoch [28/100] | train_loss 0.0857 | train_acc 0.9677 | val_loss 0.1955 | val_acc 0.9500
No improvement (5/15).
Epoch [29/100] | train_loss 0.0863 | train_acc 0.9700 | val_loss 0.2001 | val_acc 0.9483
No improvement (6/15).
Epoch [30/100] | train_loss 0.0853 | train_acc 0.9703 | val_loss 0.1943 | val_acc 0.9483
No improvement (7/15).
Epoch [31/100] | train_loss 0.0833 | train_acc 0.9677 | val_loss 0.2001 | val_acc 0.9483
No improvement (8/15).
Epoch [32/100] | train_loss 0.0820 | train_acc 0.9713 | val_loss 0.2166 | val_acc 0.9433
No improvement (9/15).
Epoch [33/100] | train_loss 0.0827 | train_acc 0.9700 | val_loss 0.2169 | val_acc 0.9433
No improvement (10/15).
Epoch [34/100] | train_loss 0.0814 | train_acc 0.9710 | val_loss 0.2151 | val_acc 0.9433
No improvement (11/15).
Epoch [35/100] | train_loss 0.0820 | train_acc 0.9690 | val_loss 0.2185 | val_acc 0.9450
No improvement (12/15).
Epoch [36/100] | train_loss 0.0816 | train_acc 0.9710 | val_loss 0.2179 | val_acc 0.9467
No improvement (13/15).
Epoch [37/100] | train_loss 0.0811 | train_acc 0.9707 | val_loss 0.2130 | val_acc 0.9467
No improvement (14/15).
Epoch [38/100] | train_loss 0.0807 | train_acc 0.9703 | val_loss 0.2146 | val_acc 0.9450
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 64 Finished. Best Val Acc: 95.33%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 65 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.23 GiB is free. Including non-PyTorch memory, this process has 378.00 MiB memory in use. Of the allocated memory 219.00 MiB is allocated by PyTorch, and 3.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7336 | train_acc 0.5390 | val_loss 0.6802 | val_acc 0.5950
Epoch [2/100] | train_loss 0.6030 | train_acc 0.6567 | val_loss 0.4723 | val_acc 0.8200
Epoch [3/100] | train_loss 0.3681 | train_acc 0.8243 | val_loss 0.3242 | val_acc 0.8683
Epoch [4/100] | train_loss 0.2241 | train_acc 0.9107 | val_loss 0.4269 | val_acc 0.8700
Epoch [5/100] | train_loss 0.1758 | train_acc 0.9343 | val_loss 0.1693 | val_acc 0.9333
No improvement (1/15).
Epoch [6/100] | train_loss 0.1539 | train_acc 0.9410 | val_loss 0.3085 | val_acc 0.8983
Epoch [7/100] | train_loss 0.1264 | train_acc 0.9513 | val_loss 0.1550 | val_acc 0.9500
No improvement (1/15).
Epoch [8/100] | train_loss 0.1034 | train_acc 0.9600 | val_loss 0.1781 | val_acc 0.9433
Epoch [9/100] | train_loss 0.0956 | train_acc 0.9640 | val_loss 0.1553 | val_acc 0.9517
No improvement (1/15).
Epoch [10/100] | train_loss 0.0864 | train_acc 0.9670 | val_loss 0.1835 | val_acc 0.9417
No improvement (2/15).
Epoch [11/100] | train_loss 0.0802 | train_acc 0.9740 | val_loss 0.1864 | val_acc 0.9467
No improvement (3/15).
Epoch [12/100] | train_loss 0.0730 | train_acc 0.9740 | val_loss 0.1898 | val_acc 0.9433
No improvement (4/15).
Epoch [13/100] | train_loss 0.0882 | train_acc 0.9667 | val_loss 0.1764 | val_acc 0.9467
No improvement (5/15).
Epoch [14/100] | train_loss 0.0599 | train_acc 0.9797 | val_loss 0.1783 | val_acc 0.9400
No improvement (6/15).
Epoch [15/100] | train_loss 0.0649 | train_acc 0.9757 | val_loss 0.1600 | val_acc 0.9483
No improvement (7/15).
Epoch [16/100] | train_loss 0.0723 | train_acc 0.9733 | val_loss 0.2334 | val_acc 0.9150
No improvement (8/15).
Epoch [17/100] | train_loss 0.0498 | train_acc 0.9827 | val_loss 0.1706 | val_acc 0.9433
No improvement (9/15).
Epoch [18/100] | train_loss 0.0565 | train_acc 0.9800 | val_loss 0.2930 | val_acc 0.9117
No improvement (10/15).
Epoch [19/100] | train_loss 0.0474 | train_acc 0.9843 | val_loss 0.2651 | val_acc 0.9150
No improvement (11/15).
Epoch [20/100] | train_loss 0.0416 | train_acc 0.9870 | val_loss 0.2525 | val_acc 0.9250
No improvement (12/15).
Epoch [21/100] | train_loss 0.0575 | train_acc 0.9770 | val_loss 0.2025 | val_acc 0.9417
No improvement (13/15).
Epoch [22/100] | train_loss 0.0509 | train_acc 0.9813 | val_loss 0.2098 | val_acc 0.9367
No improvement (14/15).
Epoch [23/100] | train_loss 0.0438 | train_acc 0.9850 | val_loss 0.2076 | val_acc 0.9367
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 66 Finished. Best Val Acc: 95.17%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 67 Failed: CUDA out of memory. Tried to allocate 626.00 MiB. GPU 0 has a total capacity of 7.78 GiB of which 211.81 MiB is free. Including non-PyTorch memory, this process has 7.39 GiB memory in use. Of the allocated memory 7.12 GiB is allocated by PyTorch, and 123.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6959 | train_acc 0.5083 | val_loss 0.6944 | val_acc 0.4900
Epoch [2/100] | train_loss 0.6924 | train_acc 0.5187 | val_loss 0.6912 | val_acc 0.5333
Epoch [3/100] | train_loss 0.6656 | train_acc 0.5867 | val_loss 0.6777 | val_acc 0.5600
No improvement (1/15).
Epoch [4/100] | train_loss 0.6148 | train_acc 0.6080 | val_loss 0.5855 | val_acc 0.5517
Epoch [5/100] | train_loss 0.5499 | train_acc 0.6620 | val_loss 0.5568 | val_acc 0.6050
Epoch [6/100] | train_loss 0.4336 | train_acc 0.7800 | val_loss 0.4190 | val_acc 0.8000
Epoch [7/100] | train_loss 0.3397 | train_acc 0.8493 | val_loss 0.4351 | val_acc 0.8100
Epoch [8/100] | train_loss 0.2989 | train_acc 0.8753 | val_loss 0.3959 | val_acc 0.8300
Epoch [9/100] | train_loss 0.2829 | train_acc 0.8833 | val_loss 0.3904 | val_acc 0.8317
Epoch [10/100] | train_loss 0.2700 | train_acc 0.8893 | val_loss 0.3790 | val_acc 0.8433
Epoch [11/100] | train_loss 0.2572 | train_acc 0.8987 | val_loss 0.3795 | val_acc 0.8483
No improvement (1/15).
Epoch [12/100] | train_loss 0.2472 | train_acc 0.9050 | val_loss 0.3778 | val_acc 0.8400
No improvement (2/15).
Epoch [13/100] | train_loss 0.2381 | train_acc 0.9067 | val_loss 0.3702 | val_acc 0.8450
Epoch [14/100] | train_loss 0.2202 | train_acc 0.9160 | val_loss 0.3317 | val_acc 0.8533
Epoch [15/100] | train_loss 0.2147 | train_acc 0.9163 | val_loss 0.3295 | val_acc 0.8567
No improvement (1/15).
Epoch [16/100] | train_loss 0.2102 | train_acc 0.9173 | val_loss 0.3293 | val_acc 0.8567
Epoch [17/100] | train_loss 0.2069 | train_acc 0.9193 | val_loss 0.3330 | val_acc 0.8633
No improvement (1/15).
Epoch [18/100] | train_loss 0.2038 | train_acc 0.9210 | val_loss 0.3343 | val_acc 0.8633
No improvement (2/15).
Epoch [19/100] | train_loss 0.2006 | train_acc 0.9207 | val_loss 0.3337 | val_acc 0.8617
No improvement (3/15).
Epoch [20/100] | train_loss 0.1944 | train_acc 0.9253 | val_loss 0.3341 | val_acc 0.8633
No improvement (4/15).
Epoch [21/100] | train_loss 0.1927 | train_acc 0.9253 | val_loss 0.3342 | val_acc 0.8633wandb: updating run metadata
wandb: uploading wandb-summary.json
wandb: uploading summary, console lines 65-66
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–â–‚â–ƒâ–„â–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–‡â–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–‚â–‚â–ƒâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–†â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.865
wandb:             epoch 39
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.93133
wandb:        train/loss 0.17896
wandb: training_time_sec 4208.09883
wandb:           val/acc 0.86
wandb:          val/loss 0.35247
wandb: 
wandb: ğŸš€ View run Trial68_[CV-Variation]_L1_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sn6ek3ie
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_041013-sn6ek3ie/logs
[I 2026-02-07 05:20:23,664] Trial 68 finished with value: 0.865 and parameters: {'nhead': 8, 'num_layers': 1, 'd_model': 64, 'batch_size': 32, 'use_conv1d': True, 'lr': 0.0007286695114917025, 'dropout': 0.4192939074303624}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run g5humq3e
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_052023-g5humq3e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial69_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/g5humq3e
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–†â–‡â–‡â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–†â–ˆâ–…â–ƒâ–†â–‚â–‚â–‚â–â–â–‚â–„â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94667
wandb:             epoch 25
wandb:         grad/norm 1.0
wandb:                lr 0.00012
wandb:         train/acc 0.97767
wandb:        train/loss 0.06685
wandb: training_time_sec 10867.89567
wandb:           val/acc 0.945
wandb:          val/loss 0.23332
wandb: 
wandb: ğŸš€ View run Trial69_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/g5humq3e
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_052023-g5humq3e/logs
[I 2026-02-07 08:21:33,731] Trial 69 finished with value: 0.9466666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0009349391427057306, 'dropout': 0.4658737546678029}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run qehba851
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_082134-qehba851
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial70_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qehba851
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–†
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–†â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–†â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–„â–ˆâ–‚â–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94833
wandb:             epoch 27
wandb:         grad/norm 0.81097
wandb:                lr 0.0001
wandb:         train/acc 0.983
wandb:        train/loss 0.05049
wandb: training_time_sec 11761.14402
wandb:           val/acc 0.94667
wandb:          val/loss 0.24024
wandb: 
wandb: ğŸš€ View run Trial70_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qehba851
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_082134-qehba851/logs
[I 2026-02-07 11:37:36,796] Trial 70 finished with value: 0.9483333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0015347547190329468, 'dropout': 0.4812400709738433}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run zs1uqb0b
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_113737-zs1uqb0b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial71_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zs1uqb0b

No improvement (5/15).
Epoch [22/100] | train_loss 0.1914 | train_acc 0.9263 | val_loss 0.3353 | val_acc 0.8633
No improvement (6/15).
Epoch [23/100] | train_loss 0.1898 | train_acc 0.9273 | val_loss 0.3362 | val_acc 0.8633
No improvement (7/15).
Epoch [24/100] | train_loss 0.1882 | train_acc 0.9280 | val_loss 0.3359 | val_acc 0.8633
Epoch [25/100] | train_loss 0.1875 | train_acc 0.9297 | val_loss 0.3363 | val_acc 0.8650
No improvement (1/15).
Epoch [26/100] | train_loss 0.1845 | train_acc 0.9317 | val_loss 0.3599 | val_acc 0.8533
No improvement (2/15).
Epoch [27/100] | train_loss 0.1838 | train_acc 0.9303 | val_loss 0.3565 | val_acc 0.8567
No improvement (3/15).
Epoch [28/100] | train_loss 0.1834 | train_acc 0.9303 | val_loss 0.3587 | val_acc 0.8567
No improvement (4/15).
Epoch [29/100] | train_loss 0.1832 | train_acc 0.9310 | val_loss 0.3594 | val_acc 0.8567
No improvement (5/15).
Epoch [30/100] | train_loss 0.1822 | train_acc 0.9313 | val_loss 0.3595 | val_acc 0.8583
No improvement (6/15).
Epoch [31/100] | train_loss 0.1821 | train_acc 0.9317 | val_loss 0.3612 | val_acc 0.8600
No improvement (7/15).
Epoch [32/100] | train_loss 0.1808 | train_acc 0.9307 | val_loss 0.3701 | val_acc 0.8550
No improvement (8/15).
Epoch [33/100] | train_loss 0.1816 | train_acc 0.9290 | val_loss 0.3707 | val_acc 0.8567
No improvement (9/15).
Epoch [34/100] | train_loss 0.1813 | train_acc 0.9300 | val_loss 0.3715 | val_acc 0.8567
No improvement (10/15).
Epoch [35/100] | train_loss 0.1807 | train_acc 0.9303 | val_loss 0.3711 | val_acc 0.8600
No improvement (11/15).
Epoch [36/100] | train_loss 0.1806 | train_acc 0.9307 | val_loss 0.3715 | val_acc 0.8600
No improvement (12/15).
Epoch [37/100] | train_loss 0.1804 | train_acc 0.9317 | val_loss 0.3716 | val_acc 0.8600
No improvement (13/15).
Epoch [38/100] | train_loss 0.1806 | train_acc 0.9323 | val_loss 0.3508 | val_acc 0.8600
No improvement (14/15).
Epoch [39/100] | train_loss 0.1790 | train_acc 0.9313 | val_loss 0.3525 | val_acc 0.8600
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 68 Finished. Best Val Acc: 86.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7216 | train_acc 0.5290 | val_loss 0.7034 | val_acc 0.5517
Epoch [2/100] | train_loss 0.5841 | train_acc 0.6630 | val_loss 0.5648 | val_acc 0.7583
Epoch [3/100] | train_loss 0.2820 | train_acc 0.8800 | val_loss 0.7048 | val_acc 0.8533
Epoch [4/100] | train_loss 0.2306 | train_acc 0.9053 | val_loss 0.4988 | val_acc 0.8750
No improvement (1/15).
Epoch [5/100] | train_loss 0.2273 | train_acc 0.9160 | val_loss 0.3681 | val_acc 0.8667
No improvement (2/15).
Epoch [6/100] | train_loss 0.1625 | train_acc 0.9423 | val_loss 0.5557 | val_acc 0.8500
Epoch [7/100] | train_loss 0.1446 | train_acc 0.9447 | val_loss 0.2875 | val_acc 0.9017
Epoch [8/100] | train_loss 0.1587 | train_acc 0.9350 | val_loss 0.2694 | val_acc 0.9200
No improvement (1/15).
Epoch [9/100] | train_loss 0.1525 | train_acc 0.9447 | val_loss 0.2521 | val_acc 0.9083
Epoch [10/100] | train_loss 0.1032 | train_acc 0.9637 | val_loss 0.2138 | val_acc 0.9433
Epoch [11/100] | train_loss 0.0976 | train_acc 0.9597 | val_loss 0.2241 | val_acc 0.9467
No improvement (1/15).
Epoch [12/100] | train_loss 0.1062 | train_acc 0.9587 | val_loss 0.2898 | val_acc 0.9083
No improvement (2/15).
Epoch [13/100] | train_loss 0.0937 | train_acc 0.9657 | val_loss 0.3899 | val_acc 0.8783
No improvement (3/15).
Epoch [14/100] | train_loss 0.0891 | train_acc 0.9647 | val_loss 0.2416 | val_acc 0.9383
No improvement (4/15).
Epoch [15/100] | train_loss 0.0869 | train_acc 0.9690 | val_loss 0.2261 | val_acc 0.9467
No improvement (5/15).
Epoch [16/100] | train_loss 0.0780 | train_acc 0.9723 | val_loss 0.2627 | val_acc 0.9350
No improvement (6/15).
Epoch [17/100] | train_loss 0.0832 | train_acc 0.9710 | val_loss 0.2606 | val_acc 0.9350
No improvement (7/15).
Epoch [18/100] | train_loss 0.0812 | train_acc 0.9720 | val_loss 0.2416 | val_acc 0.9417
No improvement (8/15).
Epoch [19/100] | train_loss 0.0795 | train_acc 0.9720 | val_loss 0.2382 | val_acc 0.9417
No improvement (9/15).
Epoch [20/100] | train_loss 0.0766 | train_acc 0.9723 | val_loss 0.2523 | val_acc 0.9417
No improvement (10/15).
Epoch [21/100] | train_loss 0.0770 | train_acc 0.9747 | val_loss 0.3148 | val_acc 0.9217
No improvement (11/15).
Epoch [22/100] | train_loss 0.0724 | train_acc 0.9763 | val_loss 0.2250 | val_acc 0.9433
No improvement (12/15).
Epoch [23/100] | train_loss 0.0690 | train_acc 0.9757 | val_loss 0.2370 | val_acc 0.9467
No improvement (13/15).
Epoch [24/100] | train_loss 0.0687 | train_acc 0.9757 | val_loss 0.2424 | val_acc 0.9450
No improvement (14/15).
Epoch [25/100] | train_loss 0.0668 | train_acc 0.9777 | val_loss 0.2333 | val_acc 0.9450
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 69 Finished. Best Val Acc: 94.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7233 | train_acc 0.5613 | val_loss 0.6533 | val_acc 0.6000
Epoch [2/100] | train_loss 0.5664 | train_acc 0.7093 | val_loss 1.3901 | val_acc 0.6167
Epoch [3/100] | train_loss 0.4908 | train_acc 0.7927 | val_loss 0.2972 | val_acc 0.8733
Epoch [4/100] | train_loss 0.2303 | train_acc 0.9063 | val_loss 0.3321 | val_acc 0.8833
No improvement (1/15).
Epoch [5/100] | train_loss 0.1776 | train_acc 0.9367 | val_loss 0.4792 | val_acc 0.8533
Epoch [6/100] | train_loss 0.2038 | train_acc 0.9173 | val_loss 0.2568 | val_acc 0.9117
Epoch [7/100] | train_loss 0.1556 | train_acc 0.9453 | val_loss 0.2127 | val_acc 0.9300
No improvement (1/15).
Epoch [8/100] | train_loss 0.1628 | train_acc 0.9327 | val_loss 0.2224 | val_acc 0.9283
Epoch [9/100] | train_loss 0.1158 | train_acc 0.9543 | val_loss 0.1904 | val_acc 0.9367
No improvement (1/15).
Epoch [10/100] | train_loss 0.1051 | train_acc 0.9603 | val_loss 0.2297 | val_acc 0.9117
No improvement (2/15).
Epoch [11/100] | train_loss 0.0964 | train_acc 0.9640 | val_loss 0.2508 | val_acc 0.9133
Epoch [12/100] | train_loss 0.0902 | train_acc 0.9640 | val_loss 0.1945 | val_acc 0.9417
Epoch [13/100] | train_loss 0.0835 | train_acc 0.9687 | val_loss 0.2176 | val_acc 0.9483
No improvement (1/15).
Epoch [14/100] | train_loss 0.0998 | train_acc 0.9637 | val_loss 0.2354 | val_acc 0.9283
No improvement (2/15).
Epoch [15/100] | train_loss 0.0808 | train_acc 0.9727 | val_loss 0.2617 | val_acc 0.9200
No improvement (3/15).
Epoch [16/100] | train_loss 0.0805 | train_acc 0.9727 | val_loss 0.2506 | val_acc 0.9200
No improvement (4/15).
Epoch [17/100] | train_loss 0.0737 | train_acc 0.9753 | val_loss 0.2516 | val_acc 0.9233
No improvement (5/15).
Epoch [18/100] | train_loss 0.0766 | train_acc 0.9737 | val_loss 0.2657 | val_acc 0.9200
No improvement (6/15).
Epoch [19/100] | train_loss 0.0775 | train_acc 0.9733 | val_loss 0.2185 | val_acc 0.9450
No improvement (7/15).
Epoch [20/100] | train_loss 0.0578 | train_acc 0.9817 | val_loss 0.3124 | val_acc 0.9367
No improvement (8/15).
Epoch [21/100] | train_loss 0.0632 | train_acc 0.9790 | val_loss 0.3024 | val_acc 0.9300
No improvement (9/15).
Epoch [22/100] | train_loss 0.0809 | train_acc 0.9717 | val_loss 0.2761 | val_acc 0.9350
No improvement (10/15).
Epoch [23/100] | train_loss 0.0600 | train_acc 0.9807 | val_loss 0.3241 | val_acc 0.9317
No improvement (11/15).
Epoch [24/100] | train_loss 0.0599 | train_acc 0.9787 | val_loss 0.3355 | val_acc 0.9300
No improvement (12/15).
Epoch [25/100] | train_loss 0.0639 | train_acc 0.9770 | val_loss 0.3379 | val_acc 0.9333
No improvement (13/15).
Epoch [26/100] | train_loss 0.0710 | train_acc 0.9740 | val_loss 0.2414 | val_acc 0.9450
No improvement (14/15).
Epoch [27/100] | train_loss 0.0505 | train_acc 0.9830 | val_loss 0.2402 | val_acc 0.9467
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 70 Finished. Best Val Acc: 94.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7040 | train_acc 0.5240 | val_loss 0.6795 | val_acc 0.5633
Epoch [2/100] | train_loss 0.5723 | train_acc 0.6763 | val_loss 0.9851 | val_acc 0.6717
Epoch [3/100] | train_loss 0.3196 | train_acc 0.8587 | val_loss 0.7707 | val_acc 0.7900
Epoch [4/100] | train_loss 0.2715 | train_acc 0.8873 | val_loss 0.3103 | val_acc 0.8800wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–†â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–†â–…â–ˆâ–„â–†â–ƒâ–…
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–…â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–…â–ˆâ–†â–‚â–ƒâ–â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95333
wandb:             epoch 35
wandb:         grad/norm 0.66232
wandb:                lr 3e-05
wandb:         train/acc 0.97833
wandb:        train/loss 0.06284
wandb: training_time_sec 11321.19183
wandb:           val/acc 0.93667
wandb:          val/loss 0.22762
wandb: 
wandb: ğŸš€ View run Trial71_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zs1uqb0b
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_113737-zs1uqb0b/logs
[I 2026-02-07 14:46:19,995] Trial 71 finished with value: 0.9533333333333334 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0010584238229942146, 'dropout': 0.45417072922371726}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 8lrq31ty
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_144620-8lrq31ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial72_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8lrq31ty
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–‡â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–†â–†â–‡â–‡â–†â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–†â–ƒâ–…â–‚â–ƒâ–†â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95667
wandb:             epoch 26
wandb:         grad/norm 1.0
wandb:                lr 8e-05
wandb:         train/acc 0.97833
wandb:        train/loss 0.06506
wandb: training_time_sec 8464.80177
wandb:           val/acc 0.93667
wandb:          val/loss 0.2732
wandb: 
wandb: ğŸš€ View run Trial72_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8lrq31ty
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_144620-8lrq31ty/logs
[I 2026-02-07 17:07:26,736] Trial 72 finished with value: 0.9566666666666667 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00130937695470768, 'dropout': 0.431965376925785}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_170727-1ibzk98w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial73_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1ibzk98w

No improvement (1/15).
Epoch [5/100] | train_loss 0.1894 | train_acc 0.9243 | val_loss 0.3735 | val_acc 0.8783
Epoch [6/100] | train_loss 0.1666 | train_acc 0.9390 | val_loss 0.2274 | val_acc 0.9283
No improvement (1/15).
Epoch [7/100] | train_loss 0.1653 | train_acc 0.9357 | val_loss 0.2903 | val_acc 0.8983
No improvement (2/15).
Epoch [8/100] | train_loss 0.1454 | train_acc 0.9437 | val_loss 0.2576 | val_acc 0.9167
Epoch [9/100] | train_loss 0.1259 | train_acc 0.9533 | val_loss 0.1841 | val_acc 0.9433
No improvement (1/15).
Epoch [10/100] | train_loss 0.1056 | train_acc 0.9633 | val_loss 0.2696 | val_acc 0.9217
No improvement (2/15).
Epoch [11/100] | train_loss 0.1107 | train_acc 0.9570 | val_loss 0.2771 | val_acc 0.9167
No improvement (3/15).
Epoch [12/100] | train_loss 0.1091 | train_acc 0.9593 | val_loss 0.2501 | val_acc 0.9233
No improvement (4/15).
Epoch [13/100] | train_loss 0.1207 | train_acc 0.9527 | val_loss 0.2178 | val_acc 0.9233
No improvement (5/15).
Epoch [14/100] | train_loss 0.1018 | train_acc 0.9590 | val_loss 0.2269 | val_acc 0.9283
No improvement (6/15).
Epoch [15/100] | train_loss 0.0880 | train_acc 0.9670 | val_loss 0.2192 | val_acc 0.9267
No improvement (7/15).
Epoch [16/100] | train_loss 0.0893 | train_acc 0.9650 | val_loss 0.2287 | val_acc 0.9233
No improvement (8/15).
Epoch [17/100] | train_loss 0.0864 | train_acc 0.9690 | val_loss 0.2002 | val_acc 0.9367
No improvement (9/15).
Epoch [18/100] | train_loss 0.0815 | train_acc 0.9730 | val_loss 0.2081 | val_acc 0.9333
No improvement (10/15).
Epoch [19/100] | train_loss 0.0827 | train_acc 0.9720 | val_loss 0.2151 | val_acc 0.9317
No improvement (11/15).
Epoch [20/100] | train_loss 0.0847 | train_acc 0.9720 | val_loss 0.2162 | val_acc 0.9333
Epoch [21/100] | train_loss 0.0756 | train_acc 0.9733 | val_loss 0.1783 | val_acc 0.9533
No improvement (1/15).
Epoch [22/100] | train_loss 0.0735 | train_acc 0.9727 | val_loss 0.1863 | val_acc 0.9517
No improvement (2/15).
Epoch [23/100] | train_loss 0.0740 | train_acc 0.9747 | val_loss 0.1849 | val_acc 0.9533
No improvement (3/15).
Epoch [24/100] | train_loss 0.0715 | train_acc 0.9750 | val_loss 0.1861 | val_acc 0.9517
No improvement (4/15).
Epoch [25/100] | train_loss 0.0720 | train_acc 0.9760 | val_loss 0.1906 | val_acc 0.9517
No improvement (5/15).
Epoch [26/100] | train_loss 0.0683 | train_acc 0.9787 | val_loss 0.1875 | val_acc 0.9517
No improvement (6/15).
Epoch [27/100] | train_loss 0.0749 | train_acc 0.9737 | val_loss 0.2081 | val_acc 0.9450
No improvement (7/15).
Epoch [28/100] | train_loss 0.0700 | train_acc 0.9763 | val_loss 0.2070 | val_acc 0.9467
No improvement (8/15).
Epoch [29/100] | train_loss 0.0677 | train_acc 0.9767 | val_loss 0.2085 | val_acc 0.9500
No improvement (9/15).
Epoch [30/100] | train_loss 0.0689 | train_acc 0.9763 | val_loss 0.2112 | val_acc 0.9450
No improvement (10/15).
Epoch [31/100] | train_loss 0.0680 | train_acc 0.9750 | val_loss 0.2084 | val_acc 0.9483
No improvement (11/15).
Epoch [32/100] | train_loss 0.0670 | train_acc 0.9777 | val_loss 0.2174 | val_acc 0.9500
No improvement (12/15).
Epoch [33/100] | train_loss 0.0648 | train_acc 0.9777 | val_loss 0.2244 | val_acc 0.9433
No improvement (13/15).
Epoch [34/100] | train_loss 0.0631 | train_acc 0.9787 | val_loss 0.2287 | val_acc 0.9367
No improvement (14/15).
Epoch [35/100] | train_loss 0.0628 | train_acc 0.9783 | val_loss 0.2276 | val_acc 0.9367
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 71 Finished. Best Val Acc: 95.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7276 | train_acc 0.5220 | val_loss 0.6810 | val_acc 0.6000
Epoch [2/100] | train_loss 0.6083 | train_acc 0.6617 | val_loss 0.5700 | val_acc 0.7050
Epoch [3/100] | train_loss 0.2810 | train_acc 0.8840 | val_loss 0.3426 | val_acc 0.8633
No improvement (1/15).
Epoch [4/100] | train_loss 0.2327 | train_acc 0.9100 | val_loss 0.4536 | val_acc 0.8500
Epoch [5/100] | train_loss 0.1855 | train_acc 0.9307 | val_loss 0.2558 | val_acc 0.9217
No improvement (1/15).
Epoch [6/100] | train_loss 0.1968 | train_acc 0.9217 | val_loss 0.3090 | val_acc 0.9000
No improvement (2/15).
Epoch [7/100] | train_loss 0.1599 | train_acc 0.9457 | val_loss 0.5169 | val_acc 0.8317
Epoch [8/100] | train_loss 0.1369 | train_acc 0.9487 | val_loss 0.2267 | val_acc 0.9433
Epoch [9/100] | train_loss 0.1255 | train_acc 0.9500 | val_loss 0.2058 | val_acc 0.9500
No improvement (1/15).
Epoch [10/100] | train_loss 0.1068 | train_acc 0.9593 | val_loss 0.2424 | val_acc 0.9267
No improvement (2/15).
Epoch [11/100] | train_loss 0.0990 | train_acc 0.9620 | val_loss 0.2105 | val_acc 0.9350
Epoch [12/100] | train_loss 0.0923 | train_acc 0.9657 | val_loss 0.1837 | val_acc 0.9567
No improvement (1/15).
Epoch [13/100] | train_loss 0.0937 | train_acc 0.9643 | val_loss 0.2148 | val_acc 0.9367
No improvement (2/15).
Epoch [14/100] | train_loss 0.0974 | train_acc 0.9620 | val_loss 0.1934 | val_acc 0.9417
No improvement (3/15).
Epoch [15/100] | train_loss 0.0934 | train_acc 0.9647 | val_loss 0.2073 | val_acc 0.9367
No improvement (4/15).
Epoch [16/100] | train_loss 0.0926 | train_acc 0.9647 | val_loss 0.2022 | val_acc 0.9450
No improvement (5/15).
Epoch [17/100] | train_loss 0.0884 | train_acc 0.9697 | val_loss 0.2128 | val_acc 0.9383
No improvement (6/15).
Epoch [18/100] | train_loss 0.0882 | train_acc 0.9647 | val_loss 0.2224 | val_acc 0.9333
No improvement (7/15).
Epoch [19/100] | train_loss 0.0884 | train_acc 0.9693 | val_loss 0.2051 | val_acc 0.9383
No improvement (8/15).
Epoch [20/100] | train_loss 0.0695 | train_acc 0.9753 | val_loss 0.1978 | val_acc 0.9533
No improvement (9/15).
Epoch [21/100] | train_loss 0.0665 | train_acc 0.9767 | val_loss 0.1970 | val_acc 0.9500
No improvement (10/15).
Epoch [22/100] | train_loss 0.0641 | train_acc 0.9780 | val_loss 0.2196 | val_acc 0.9517
No improvement (11/15).
Epoch [23/100] | train_loss 0.0666 | train_acc 0.9777 | val_loss 0.2097 | val_acc 0.9467
No improvement (12/15).
Epoch [24/100] | train_loss 0.0647 | train_acc 0.9780 | val_loss 0.2231 | val_acc 0.9483
No improvement (13/15).
Epoch [25/100] | train_loss 0.0634 | train_acc 0.9780 | val_loss 0.2153 | val_acc 0.9450
No improvement (14/15).
Epoch [26/100] | train_loss 0.0651 | train_acc 0.9783 | val_loss 0.2732 | val_acc 0.9367
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 72 Finished. Best Val Acc: 95.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6897 | train_acc 0.5330 | val_loss 0.6894 | val_acc 0.5683
Epoch [2/100] | train_loss 0.6834 | train_acc 0.5690 | val_loss 0.6830 | val_acc 0.5933
Epoch [3/100] | train_loss 0.6748 | train_acc 0.6113 | val_loss 0.6746 | val_acc 0.6000
Epoch [4/100] | train_loss 0.6639 | train_acc 0.6190 | val_loss 0.6616 | val_acc 0.6167
Epoch [5/100] | train_loss 0.6527 | train_acc 0.6307 | val_loss 0.6483 | val_acc 0.6267
Epoch [6/100] | train_loss 0.6419 | train_acc 0.6440 | val_loss 0.6377 | val_acc 0.6467
Epoch [7/100] | train_loss 0.6274 | train_acc 0.6593 | val_loss 0.6210 | val_acc 0.6567
No improvement (1/15).
Epoch [8/100] | train_loss 0.6057 | train_acc 0.6743 | val_loss 0.6043 | val_acc 0.6550
Epoch [9/100] | train_loss 0.5813 | train_acc 0.6817 | val_loss 0.5775 | val_acc 0.6733
Epoch [10/100] | train_loss 0.5456 | train_acc 0.7033 | val_loss 0.5388 | val_acc 0.7050
Epoch [11/100] | train_loss 0.4972 | train_acc 0.7383 | val_loss 0.4914 | val_acc 0.7717
Epoch [12/100] | train_loss 0.4434 | train_acc 0.7923 | val_loss 0.4445 | val_acc 0.8033
Epoch [13/100] | train_loss 0.3951 | train_acc 0.8250 | val_loss 0.4140 | val_acc 0.8200
No improvement (1/15).
Epoch [14/100] | train_loss 0.3621 | train_acc 0.8460 | val_loss 0.4050 | val_acc 0.8133
No improvement (2/15).
Epoch [15/100] | train_loss 0.3437 | train_acc 0.8590 | val_loss 0.4001 | val_acc 0.8183
Epoch [16/100] | train_loss 0.3274 | train_acc 0.8667 | val_loss 0.3930 | val_acc 0.8267
Epoch [17/100] | train_loss 0.3145 | train_acc 0.8723 | val_loss 0.3866 | val_acc 0.8350
Epoch [18/100] | train_loss 0.3011 | train_acc 0.8793 | val_loss 0.3789 | val_acc 0.8383
Epoch [19/100] | train_loss 0.2903 | train_acc 0.8830 | val_loss 0.3728 | val_acc 0.8500wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–‚â–‚â–‚â–„â–„â–ƒâ–ƒâ–…â–…â–†â–ˆâ–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.87
wandb:             epoch 46
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.908
wandb:        train/loss 0.23682
wandb: training_time_sec 14772.50365
wandb:           val/acc 0.87
wandb:          val/loss 0.3299
wandb: 
wandb: ğŸš€ View run Trial73_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1ibzk98w
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_170727-1ibzk98w/logs
[I 2026-02-07 21:13:41,073] Trial 73 finished with value: 0.87 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00010001350098429645, 'dropout': 0.38000971091694585}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run xnv4w8an
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_211341-xnv4w8an
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial74_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xnv4w8an
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  train/acc â–â–‚â–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–…â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:   val/loss â–ˆâ–†â–†â–ƒâ–â–„â–„â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 22
wandb:         grad/norm 1.0
wandb:                lr 8e-05
wandb:         train/acc 0.97333
wandb:        train/loss 0.0793
wandb: training_time_sec 7270.02945
wandb:           val/acc 0.94833
wandb:          val/loss 0.19913
wandb: 
wandb: ğŸš€ View run Trial74_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xnv4w8an
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_211341-xnv4w8an/logs
[I 2026-02-07 23:14:53,086] Trial 74 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.000655098596686489, 'dropout': 0.44240759960864706}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 9x8bezfg
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260207_231453-9x8bezfg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial75_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9x8bezfg

No improvement (1/15).
Epoch [20/100] | train_loss 0.2816 | train_acc 0.8913 | val_loss 0.3728 | val_acc 0.8483
Epoch [21/100] | train_loss 0.2773 | train_acc 0.8933 | val_loss 0.3699 | val_acc 0.8517
Epoch [22/100] | train_loss 0.2718 | train_acc 0.8947 | val_loss 0.3676 | val_acc 0.8533
Epoch [23/100] | train_loss 0.2683 | train_acc 0.8953 | val_loss 0.3640 | val_acc 0.8567
Epoch [24/100] | train_loss 0.2643 | train_acc 0.8973 | val_loss 0.3606 | val_acc 0.8600
No improvement (1/15).
Epoch [25/100] | train_loss 0.2594 | train_acc 0.8997 | val_loss 0.3573 | val_acc 0.8600
Epoch [26/100] | train_loss 0.2588 | train_acc 0.9010 | val_loss 0.3518 | val_acc 0.8667
No improvement (1/15).
Epoch [27/100] | train_loss 0.2560 | train_acc 0.9000 | val_loss 0.3496 | val_acc 0.8667
No improvement (2/15).
Epoch [28/100] | train_loss 0.2528 | train_acc 0.9023 | val_loss 0.3473 | val_acc 0.8667
No improvement (3/15).
Epoch [29/100] | train_loss 0.2511 | train_acc 0.9010 | val_loss 0.3461 | val_acc 0.8667
No improvement (4/15).
Epoch [30/100] | train_loss 0.2495 | train_acc 0.9047 | val_loss 0.3447 | val_acc 0.8667
Epoch [31/100] | train_loss 0.2481 | train_acc 0.9027 | val_loss 0.3431 | val_acc 0.8683
Epoch [32/100] | train_loss 0.2468 | train_acc 0.9063 | val_loss 0.3376 | val_acc 0.8700
No improvement (1/15).
Epoch [33/100] | train_loss 0.2443 | train_acc 0.9057 | val_loss 0.3373 | val_acc 0.8667
No improvement (2/15).
Epoch [34/100] | train_loss 0.2435 | train_acc 0.9053 | val_loss 0.3367 | val_acc 0.8700
No improvement (3/15).
Epoch [35/100] | train_loss 0.2424 | train_acc 0.9060 | val_loss 0.3360 | val_acc 0.8667
No improvement (4/15).
Epoch [36/100] | train_loss 0.2435 | train_acc 0.9060 | val_loss 0.3344 | val_acc 0.8700
No improvement (5/15).
Epoch [37/100] | train_loss 0.2412 | train_acc 0.9063 | val_loss 0.3342 | val_acc 0.8683
No improvement (6/15).
Epoch [38/100] | train_loss 0.2399 | train_acc 0.9070 | val_loss 0.3327 | val_acc 0.8700
No improvement (7/15).
Epoch [39/100] | train_loss 0.2400 | train_acc 0.9057 | val_loss 0.3320 | val_acc 0.8700
No improvement (8/15).
Epoch [40/100] | train_loss 0.2388 | train_acc 0.9077 | val_loss 0.3318 | val_acc 0.8683
No improvement (9/15).
Epoch [41/100] | train_loss 0.2373 | train_acc 0.9067 | val_loss 0.3315 | val_acc 0.8700
No improvement (10/15).
Epoch [42/100] | train_loss 0.2379 | train_acc 0.9070 | val_loss 0.3311 | val_acc 0.8700
No improvement (11/15).
Epoch [43/100] | train_loss 0.2381 | train_acc 0.9077 | val_loss 0.3307 | val_acc 0.8683
No improvement (12/15).
Epoch [44/100] | train_loss 0.2370 | train_acc 0.9077 | val_loss 0.3303 | val_acc 0.8683
No improvement (13/15).
Epoch [45/100] | train_loss 0.2367 | train_acc 0.9060 | val_loss 0.3301 | val_acc 0.8683
No improvement (14/15).
Epoch [46/100] | train_loss 0.2368 | train_acc 0.9080 | val_loss 0.3299 | val_acc 0.8700
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 73 Finished. Best Val Acc: 87.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6982 | train_acc 0.5397 | val_loss 0.6830 | val_acc 0.5850
Epoch [2/100] | train_loss 0.6285 | train_acc 0.6283 | val_loss 0.5598 | val_acc 0.6850
Epoch [3/100] | train_loss 0.3874 | train_acc 0.8187 | val_loss 0.5217 | val_acc 0.8000
Epoch [4/100] | train_loss 0.2420 | train_acc 0.8990 | val_loss 0.3212 | val_acc 0.8967
Epoch [5/100] | train_loss 0.1738 | train_acc 0.9363 | val_loss 0.1665 | val_acc 0.9450
No improvement (1/15).
Epoch [6/100] | train_loss 0.1647 | train_acc 0.9433 | val_loss 0.4056 | val_acc 0.8950
No improvement (2/15).
Epoch [7/100] | train_loss 0.1809 | train_acc 0.9323 | val_loss 0.3676 | val_acc 0.8950
Epoch [8/100] | train_loss 0.1219 | train_acc 0.9530 | val_loss 0.1441 | val_acc 0.9600
No improvement (1/15).
Epoch [9/100] | train_loss 0.1058 | train_acc 0.9610 | val_loss 0.1501 | val_acc 0.9533
No improvement (2/15).
Epoch [10/100] | train_loss 0.1118 | train_acc 0.9563 | val_loss 0.2514 | val_acc 0.9367
No improvement (3/15).
Epoch [11/100] | train_loss 0.1106 | train_acc 0.9603 | val_loss 0.1919 | val_acc 0.9417
No improvement (4/15).
Epoch [12/100] | train_loss 0.1013 | train_acc 0.9623 | val_loss 0.1912 | val_acc 0.9500
No improvement (5/15).
Epoch [13/100] | train_loss 0.0999 | train_acc 0.9613 | val_loss 0.2125 | val_acc 0.9400
No improvement (6/15).
Epoch [14/100] | train_loss 0.1032 | train_acc 0.9603 | val_loss 0.1693 | val_acc 0.9567
No improvement (7/15).
Epoch [15/100] | train_loss 0.0930 | train_acc 0.9613 | val_loss 0.1810 | val_acc 0.9533
No improvement (8/15).
Epoch [16/100] | train_loss 0.0921 | train_acc 0.9663 | val_loss 0.1902 | val_acc 0.9550
No improvement (9/15).
Epoch [17/100] | train_loss 0.1109 | train_acc 0.9577 | val_loss 0.3065 | val_acc 0.9183
No improvement (10/15).
Epoch [18/100] | train_loss 0.0860 | train_acc 0.9667 | val_loss 0.1880 | val_acc 0.9517
No improvement (11/15).
Epoch [19/100] | train_loss 0.0982 | train_acc 0.9633 | val_loss 0.1869 | val_acc 0.9467
No improvement (12/15).
Epoch [20/100] | train_loss 0.0782 | train_acc 0.9733 | val_loss 0.2051 | val_acc 0.9433
No improvement (13/15).
Epoch [21/100] | train_loss 0.0817 | train_acc 0.9727 | val_loss 0.2493 | val_acc 0.9300
No improvement (14/15).
Epoch [22/100] | train_loss 0.0793 | train_acc 0.9733 | val_loss 0.1991 | val_acc 0.9483
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 74 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7000 | train_acc 0.5607 | val_loss 0.6695 | val_acc 0.5883
Epoch [2/100] | train_loss 0.5366 | train_acc 0.7017 | val_loss 1.4110 | val_acc 0.6100
Epoch [3/100] | train_loss 0.2958 | train_acc 0.8753 | val_loss 0.2668 | val_acc 0.8967
Epoch [4/100] | train_loss 0.2104 | train_acc 0.9153 | val_loss 0.2329 | val_acc 0.9183
No improvement (1/15).
Epoch [5/100] | train_loss 0.1839 | train_acc 0.9287 | val_loss 0.3173 | val_acc 0.8900
Epoch [6/100] | train_loss 0.1608 | train_acc 0.9363 | val_loss 0.1701 | val_acc 0.9500
No improvement (1/15).
Epoch [7/100] | train_loss 0.1506 | train_acc 0.9473 | val_loss 0.1917 | val_acc 0.9367
No improvement (2/15).
Epoch [8/100] | train_loss 0.1403 | train_acc 0.9500 | val_loss 0.2392 | val_acc 0.9267
No improvement (3/15).
Epoch [9/100] | train_loss 0.1031 | train_acc 0.9597 | val_loss 0.2164 | val_acc 0.9350
No improvement (4/15).
Epoch [10/100] | train_loss 0.0991 | train_acc 0.9617 | val_loss 0.2320 | val_acc 0.9317
No improvement (5/15).
Epoch [11/100] | train_loss 0.0940 | train_acc 0.9647 | val_loss 0.2246 | val_acc 0.9300
No improvement (6/15).
Epoch [12/100] | train_loss 0.0831 | train_acc 0.9687 | val_loss 0.1836 | val_acc 0.9450
No improvement (7/15).
Epoch [13/100] | train_loss 0.0845 | train_acc 0.9687 | val_loss 0.2778 | val_acc 0.9267
No improvement (8/15).
Epoch [14/100] | train_loss 0.1248 | train_acc 0.9520 | val_loss 0.2149 | val_acc 0.9300
No improvement (9/15).
Epoch [15/100] | train_loss 0.0661 | train_acc 0.9787 | val_loss 0.2093 | val_acc 0.9467
Epoch [16/100] | train_loss 0.0719 | train_acc 0.9727 | val_loss 0.1833 | val_acc 0.9567
No improvement (1/15).
Epoch [17/100] | train_loss 0.0580 | train_acc 0.9827 | val_loss 0.1977 | val_acc 0.9517
Epoch [18/100] | train_loss 0.0499 | train_acc 0.9863 | val_loss 0.2052 | val_acc 0.9600
No improvement (1/15).
Epoch [19/100] | train_loss 0.0587 | train_acc 0.9790 | val_loss 0.1987 | val_acc 0.9533
No improvement (2/15).
Epoch [20/100] | train_loss 0.0443 | train_acc 0.9867 | val_loss 0.2272 | val_acc 0.9500
No improvement (3/15).
Epoch [21/100] | train_loss 0.0372 | train_acc 0.9897 | val_loss 0.2661 | val_acc 0.9417
No improvement (4/15).
Epoch [22/100] | train_loss 0.0324 | train_acc 0.9890 | val_loss 0.2790 | val_acc 0.9433
No improvement (5/15).
Epoch [23/100] | train_loss 0.0291 | train_acc 0.9907 | val_loss 0.2868 | val_acc 0.9417
No improvement (6/15).
Epoch [24/100] | train_loss 0.0271 | train_acc 0.9920 | val_loss 0.3060 | val_acc 0.9383
No improvement (7/15).
Epoch [25/100] | train_loss 0.0271 | train_acc 0.9920 | val_loss 0.3263 | val_acc 0.9383
No improvement (8/15).
Epoch [26/100] | train_loss 0.0300 | train_acc 0.9903 | val_loss 0.3043 | val_acc 0.9417wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–„â–ƒâ–„â–‡â–„â–…â–…â–ˆâ–…â–ˆâ–„â–â–‚â–„â–â–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–„â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 32
wandb:         grad/norm 0.20685
wandb:                lr 6e-05
wandb:         train/acc 0.993
wandb:        train/loss 0.02253
wandb: training_time_sec 10407.18801
wandb:           val/acc 0.945
wandb:          val/loss 0.29544
wandb: 
wandb: ğŸš€ View run Trial75_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9x8bezfg
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260207_231453-9x8bezfg/logs
[I 2026-02-08 02:08:22,251] Trial 75 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0020261625119603536, 'dropout': 0.40502404033578066}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_020822-plmzmegk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial76_[CV-Variation]_L4_H2_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/plmzmegk
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 59-60
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–‚â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–†â–…â–‡â–„â–„â–‡â–‡â–„â–ˆâ–…â–ˆâ–ˆâ–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡
wandb:   val/loss â–„â–ƒâ–ƒâ–„â–â–…â–†â–‚â–‚â–ˆâ–â–†â–â–â–„â–‚â–â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–ƒâ–„â–ƒâ–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95167
wandb:             epoch 33
wandb:         grad/norm 1.0
wandb:                lr 0.00011
wandb:         train/acc 0.972
wandb:        train/loss 0.07072
wandb: training_time_sec 4328.90569
wandb:           val/acc 0.89167
wandb:          val/loss 0.58057
wandb: 
wandb: ğŸš€ View run Trial76_[CV-Variation]_L4_H2_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/plmzmegk
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_020822-plmzmegk/logs
[I 2026-02-08 03:20:33,280] Trial 76 finished with value: 0.9516666666666667 and parameters: {'nhead': 2, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0008716168750839995, 'dropout': 0.46025897751506956}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_032033-2oo7q17k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial77_[CV-Variation]_L3_H8_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2oo7q17k
wandb: updating run metadata
wandb: uploading wandb-summary.json
wandb: ğŸš€ View run Trial77_[CV-Variation]_L3_H8_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2oo7q17k
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_032033-2oo7q17k/logs
[I 2026-02-08 03:25:31,163] Trial 77 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 16, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0005445723914319639, 'dropout': 0.35771091488474177}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run lanzvxj1
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_032531-lanzvxj1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial78_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/lanzvxj1
wandb: updating run metadata
wandb: uploading wandb-summary.json
wandb: uploading summary, console lines 43-44
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:  train/acc â–‚â–„â–â–ƒâ–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–†â–…â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–„â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–â–â–â–â–â–â–â–
wandb:   val/loss â–ˆâ–„â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.51
wandb:             epoch 22
wandb:         grad/norm 0.06703
wandb:                lr 5e-05
wandb:         train/acc 0.51833
wandb:        train/loss 0.69256
wandb: training_time_sec 5187.8453
wandb:           val/acc 0.49
wandb:          val/loss 0.69452
wandb: 
wandb: ğŸš€ View run Trial78_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/lanzvxj1
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_032531-lanzvxj1/logs
[I 2026-02-08 04:52:01,328] Trial 78 finished with value: 0.51 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': True, 'lr': 0.00040490522252357465, 'dropout': 0.498495712107515}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run zr2p1e23
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_045201-zr2p1e23
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial79_[CV-Variation]_L4_H8_D32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zr2p1e23

No improvement (9/15).
Epoch [27/100] | train_loss 0.0303 | train_acc 0.9880 | val_loss 0.2538 | val_acc 0.9500
No improvement (10/15).
Epoch [28/100] | train_loss 0.0287 | train_acc 0.9890 | val_loss 0.2731 | val_acc 0.9467
No improvement (11/15).
Epoch [29/100] | train_loss 0.0280 | train_acc 0.9907 | val_loss 0.2762 | val_acc 0.9467
No improvement (12/15).
Epoch [30/100] | train_loss 0.0299 | train_acc 0.9890 | val_loss 0.2887 | val_acc 0.9467
No improvement (13/15).
Epoch [31/100] | train_loss 0.0226 | train_acc 0.9930 | val_loss 0.3055 | val_acc 0.9433
No improvement (14/15).
Epoch [32/100] | train_loss 0.0225 | train_acc 0.9930 | val_loss 0.2954 | val_acc 0.9450
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 75 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6922 | train_acc 0.5600 | val_loss 0.6758 | val_acc 0.5850
Epoch [2/100] | train_loss 0.6236 | train_acc 0.6100 | val_loss 0.5526 | val_acc 0.6483
Epoch [3/100] | train_loss 0.4224 | train_acc 0.7987 | val_loss 0.6127 | val_acc 0.8383
No improvement (1/15).
Epoch [4/100] | train_loss 0.3099 | train_acc 0.8773 | val_loss 0.7095 | val_acc 0.8133
Epoch [5/100] | train_loss 0.2383 | train_acc 0.9067 | val_loss 0.2842 | val_acc 0.9133
No improvement (1/15).
Epoch [6/100] | train_loss 0.2566 | train_acc 0.8947 | val_loss 0.8659 | val_acc 0.7500
No improvement (2/15).
Epoch [7/100] | train_loss 0.2411 | train_acc 0.9003 | val_loss 1.0400 | val_acc 0.7567
Epoch [8/100] | train_loss 0.1871 | train_acc 0.9317 | val_loss 0.3705 | val_acc 0.9233
No improvement (1/15).
Epoch [9/100] | train_loss 0.1873 | train_acc 0.9250 | val_loss 0.3870 | val_acc 0.9017
No improvement (2/15).
Epoch [10/100] | train_loss 0.1680 | train_acc 0.9323 | val_loss 1.4467 | val_acc 0.7267
Epoch [11/100] | train_loss 0.1689 | train_acc 0.9323 | val_loss 0.2666 | val_acc 0.9350
No improvement (1/15).
Epoch [12/100] | train_loss 0.1917 | train_acc 0.9243 | val_loss 1.1690 | val_acc 0.7700
No improvement (2/15).
Epoch [13/100] | train_loss 0.1661 | train_acc 0.9337 | val_loss 0.2879 | val_acc 0.9300
Epoch [14/100] | train_loss 0.1300 | train_acc 0.9543 | val_loss 0.2453 | val_acc 0.9417
No improvement (1/15).
Epoch [15/100] | train_loss 0.1411 | train_acc 0.9447 | val_loss 0.7761 | val_acc 0.8250
No improvement (2/15).
Epoch [16/100] | train_loss 0.1312 | train_acc 0.9503 | val_loss 0.4561 | val_acc 0.9067
No improvement (3/15).
Epoch [17/100] | train_loss 0.0961 | train_acc 0.9640 | val_loss 0.3070 | val_acc 0.9267
No improvement (4/15).
Epoch [18/100] | train_loss 0.1099 | train_acc 0.9600 | val_loss 0.3508 | val_acc 0.9233
Epoch [19/100] | train_loss 0.1146 | train_acc 0.9577 | val_loss 0.2434 | val_acc 0.9517
No improvement (1/15).
Epoch [20/100] | train_loss 0.0986 | train_acc 0.9640 | val_loss 0.2512 | val_acc 0.9450
No improvement (2/15).
Epoch [21/100] | train_loss 0.0894 | train_acc 0.9670 | val_loss 0.2565 | val_acc 0.9483
No improvement (3/15).
Epoch [22/100] | train_loss 0.0827 | train_acc 0.9670 | val_loss 0.3348 | val_acc 0.9267
No improvement (4/15).
Epoch [23/100] | train_loss 0.0682 | train_acc 0.9743 | val_loss 0.2823 | val_acc 0.9433
No improvement (5/15).
Epoch [24/100] | train_loss 0.0802 | train_acc 0.9707 | val_loss 0.2748 | val_acc 0.9417
No improvement (6/15).
Epoch [25/100] | train_loss 0.1063 | train_acc 0.9610 | val_loss 0.2934 | val_acc 0.9333
No improvement (7/15).
Epoch [26/100] | train_loss 0.1099 | train_acc 0.9580 | val_loss 0.3136 | val_acc 0.9350
No improvement (8/15).
Epoch [27/100] | train_loss 0.0719 | train_acc 0.9743 | val_loss 0.3170 | val_acc 0.9417
No improvement (9/15).
Epoch [28/100] | train_loss 0.0805 | train_acc 0.9723 | val_loss 0.3536 | val_acc 0.9400
No improvement (10/15).
Epoch [29/100] | train_loss 0.0833 | train_acc 0.9697 | val_loss 0.5084 | val_acc 0.9100
No improvement (11/15).
Epoch [30/100] | train_loss 0.0791 | train_acc 0.9710 | val_loss 0.6752 | val_acc 0.8733
No improvement (12/15).
Epoch [31/100] | train_loss 0.0769 | train_acc 0.9717 | val_loss 0.5594 | val_acc 0.8900
No improvement (13/15).
Epoch [32/100] | train_loss 0.0660 | train_acc 0.9750 | val_loss 0.4344 | val_acc 0.9167
No improvement (14/15).
Epoch [33/100] | train_loss 0.0707 | train_acc 0.9720 | val_loss 0.5806 | val_acc 0.8917
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 76 Finished. Best Val Acc: 95.17%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 77 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.32 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.89 MiB is allocated by PyTorch, and 3.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7046 | train_acc 0.5020 | val_loss 0.7223 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6976 | train_acc 0.5063 | val_loss 0.7052 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6972 | train_acc 0.4997 | val_loss 0.6953 | val_acc 0.4900
No improvement (3/15).
Epoch [4/100] | train_loss 0.6950 | train_acc 0.5053 | val_loss 0.6940 | val_acc 0.4900
No improvement (4/15).
Epoch [5/100] | train_loss 0.6935 | train_acc 0.5093 | val_loss 0.6954 | val_acc 0.4900
No improvement (5/15).
Epoch [6/100] | train_loss 0.6934 | train_acc 0.5143 | val_loss 0.6959 | val_acc 0.4900
No improvement (6/15).
Epoch [7/100] | train_loss 0.6935 | train_acc 0.5160 | val_loss 0.6971 | val_acc 0.4900
Epoch [8/100] | train_loss 0.6935 | train_acc 0.5173 | val_loss 0.6930 | val_acc 0.5100
No improvement (1/15).
Epoch [9/100] | train_loss 0.6929 | train_acc 0.5183 | val_loss 0.6929 | val_acc 0.5100
No improvement (2/15).
Epoch [10/100] | train_loss 0.6930 | train_acc 0.5177 | val_loss 0.6929 | val_acc 0.5100
No improvement (3/15).
Epoch [11/100] | train_loss 0.6930 | train_acc 0.5177 | val_loss 0.6929 | val_acc 0.5100
No improvement (4/15).
Epoch [12/100] | train_loss 0.6929 | train_acc 0.5167 | val_loss 0.6929 | val_acc 0.5100
No improvement (5/15).
Epoch [13/100] | train_loss 0.6928 | train_acc 0.5180 | val_loss 0.6929 | val_acc 0.5100
No improvement (6/15).
Epoch [14/100] | train_loss 0.6931 | train_acc 0.5143 | val_loss 0.6934 | val_acc 0.4900
No improvement (7/15).
Epoch [15/100] | train_loss 0.6933 | train_acc 0.5093 | val_loss 0.6940 | val_acc 0.4900
No improvement (8/15).
Epoch [16/100] | train_loss 0.6931 | train_acc 0.5177 | val_loss 0.6939 | val_acc 0.4900
No improvement (9/15).
Epoch [17/100] | train_loss 0.6931 | train_acc 0.5157 | val_loss 0.6939 | val_acc 0.4900
No improvement (10/15).
Epoch [18/100] | train_loss 0.6930 | train_acc 0.5183 | val_loss 0.6937 | val_acc 0.4900
No improvement (11/15).
Epoch [19/100] | train_loss 0.6931 | train_acc 0.5173 | val_loss 0.6936 | val_acc 0.4900
No improvement (12/15).
Epoch [20/100] | train_loss 0.6928 | train_acc 0.5183 | val_loss 0.6946 | val_acc 0.4900
No improvement (13/15).
Epoch [21/100] | train_loss 0.6926 | train_acc 0.5183 | val_loss 0.6945 | val_acc 0.4900
No improvement (14/15).
Epoch [22/100] | train_loss 0.6926 | train_acc 0.5183 | val_loss 0.6945 | val_acc 0.4900
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 78 Finished. Best Val Acc: 51.00%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 79 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.22 GiB is free. Including non-PyTorch memory, this process has 378.00 MiB memory in use. Of the allocated memory 219.20 MiB is allocated by PyTorch, and 2.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run Trial79_[CV-Variation]_L4_H8_D32 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zr2p1e23
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_045201-zr2p1e23/logs
[I 2026-02-08 04:58:46,467] Trial 79 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 32, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0016232355209852213, 'dropout': 0.42083642900083684}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 8404u635
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_045846-8404u635
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial80_[CV-Variation]_L2_H8_D128
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8404u635
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 47-48
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–â–„â–â–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–†â–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–ƒâ–‚â–‚â–â–ƒâ–â–‚â–â–â–‚â–â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94833
wandb:             epoch 27
wandb:         grad/norm 0.47433
wandb:                lr 7e-05
wandb:         train/acc 0.98467
wandb:        train/loss 0.04105
wandb: training_time_sec 6260.58776
wandb:           val/acc 0.94
wandb:          val/loss 0.3139
wandb: 
wandb: ğŸš€ View run Trial80_[CV-Variation]_L2_H8_D128 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8404u635
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_045846-8404u635/logs
[I 2026-02-08 06:43:09,177] Trial 80 finished with value: 0.9483333333333334 and parameters: {'nhead': 8, 'num_layers': 2, 'd_model': 128, 'batch_size': 32, 'use_conv1d': False, 'lr': 0.0011849869841235314, 'dropout': 0.4834422262090017}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run apurdvaj
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_064309-apurdvaj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial81_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/apurdvaj
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 52-53
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–„â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–†â–„â–‚â–ƒâ–†â–ƒâ–â–„â–â–â–â–â–â–â–â–â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95333
wandb:             epoch 30
wandb:         grad/norm 1.0
wandb:                lr 4e-05
wandb:         train/acc 0.97267
wandb:        train/loss 0.06778
wandb: training_time_sec 9750.52518
wandb:           val/acc 0.95
wandb:          val/loss 0.2332
wandb: 
wandb: ğŸš€ View run Trial81_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/apurdvaj
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_064309-apurdvaj/logs
[I 2026-02-08 09:25:41,953] Trial 81 finished with value: 0.9533333333333334 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0006461845826655626, 'dropout': 0.4356808222986719}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run eyhuhy9s
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_092542-eyhuhy9s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial82_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/eyhuhy9s

ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7221 | train_acc 0.5537 | val_loss 0.6497 | val_acc 0.5917
Epoch [2/100] | train_loss 0.5087 | train_acc 0.7247 | val_loss 0.4612 | val_acc 0.8367
Epoch [3/100] | train_loss 0.3348 | train_acc 0.8463 | val_loss 0.3319 | val_acc 0.8683
Epoch [4/100] | train_loss 0.2450 | train_acc 0.8973 | val_loss 0.2701 | val_acc 0.9000
Epoch [5/100] | train_loss 0.2070 | train_acc 0.9183 | val_loss 0.2347 | val_acc 0.9183
Epoch [6/100] | train_loss 0.1877 | train_acc 0.9263 | val_loss 0.2297 | val_acc 0.9333
No improvement (1/15).
Epoch [7/100] | train_loss 0.1827 | train_acc 0.9273 | val_loss 0.3206 | val_acc 0.9050
Epoch [8/100] | train_loss 0.1242 | train_acc 0.9523 | val_loss 0.2188 | val_acc 0.9417
No improvement (1/15).
Epoch [9/100] | train_loss 0.1211 | train_acc 0.9557 | val_loss 0.2746 | val_acc 0.9167
No improvement (2/15).
Epoch [10/100] | train_loss 0.1212 | train_acc 0.9550 | val_loss 0.2237 | val_acc 0.9333
No improvement (3/15).
Epoch [11/100] | train_loss 0.0918 | train_acc 0.9657 | val_loss 0.2068 | val_acc 0.9417
No improvement (4/15).
Epoch [12/100] | train_loss 0.1155 | train_acc 0.9550 | val_loss 0.2406 | val_acc 0.9250
Epoch [13/100] | train_loss 0.0950 | train_acc 0.9653 | val_loss 0.2018 | val_acc 0.9483
No improvement (1/15).
Epoch [14/100] | train_loss 0.0729 | train_acc 0.9740 | val_loss 0.2498 | val_acc 0.9267
No improvement (2/15).
Epoch [15/100] | train_loss 0.0968 | train_acc 0.9637 | val_loss 0.3019 | val_acc 0.9167
No improvement (3/15).
Epoch [16/100] | train_loss 0.0839 | train_acc 0.9673 | val_loss 0.3019 | val_acc 0.9133
No improvement (4/15).
Epoch [17/100] | train_loss 0.0827 | train_acc 0.9690 | val_loss 0.2691 | val_acc 0.9150
No improvement (5/15).
Epoch [18/100] | train_loss 0.0742 | train_acc 0.9720 | val_loss 0.2915 | val_acc 0.9117
No improvement (6/15).
Epoch [19/100] | train_loss 0.0756 | train_acc 0.9717 | val_loss 0.3331 | val_acc 0.9133
No improvement (7/15).
Epoch [20/100] | train_loss 0.0633 | train_acc 0.9787 | val_loss 0.2265 | val_acc 0.9467
No improvement (8/15).
Epoch [21/100] | train_loss 0.0516 | train_acc 0.9833 | val_loss 0.2380 | val_acc 0.9450
No improvement (9/15).
Epoch [22/100] | train_loss 0.0518 | train_acc 0.9813 | val_loss 0.2573 | val_acc 0.9417
No improvement (10/15).
Epoch [23/100] | train_loss 0.0509 | train_acc 0.9840 | val_loss 0.2805 | val_acc 0.9367
No improvement (11/15).
Epoch [24/100] | train_loss 0.0490 | train_acc 0.9867 | val_loss 0.2678 | val_acc 0.9417
No improvement (12/15).
Epoch [25/100] | train_loss 0.0470 | train_acc 0.9830 | val_loss 0.2900 | val_acc 0.9450
No improvement (13/15).
Epoch [26/100] | train_loss 0.0406 | train_acc 0.9863 | val_loss 0.3080 | val_acc 0.9383
No improvement (14/15).
Epoch [27/100] | train_loss 0.0411 | train_acc 0.9847 | val_loss 0.3139 | val_acc 0.9400
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 80 Finished. Best Val Acc: 94.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6991 | train_acc 0.5173 | val_loss 0.6928 | val_acc 0.4950
Epoch [2/100] | train_loss 0.6701 | train_acc 0.5967 | val_loss 0.6504 | val_acc 0.6233
Epoch [3/100] | train_loss 0.6117 | train_acc 0.6477 | val_loss 0.5662 | val_acc 0.7233
Epoch [4/100] | train_loss 0.3042 | train_acc 0.8733 | val_loss 0.4293 | val_acc 0.8617
Epoch [5/100] | train_loss 0.2135 | train_acc 0.9103 | val_loss 0.2772 | val_acc 0.9067
No improvement (1/15).
Epoch [6/100] | train_loss 0.2010 | train_acc 0.9133 | val_loss 0.3350 | val_acc 0.8967
No improvement (2/15).
Epoch [7/100] | train_loss 0.1711 | train_acc 0.9370 | val_loss 0.5676 | val_acc 0.8733
No improvement (3/15).
Epoch [8/100] | train_loss 0.1398 | train_acc 0.9450 | val_loss 0.2920 | val_acc 0.9067
Epoch [9/100] | train_loss 0.1165 | train_acc 0.9600 | val_loss 0.1850 | val_acc 0.9433
No improvement (1/15).
Epoch [10/100] | train_loss 0.1184 | train_acc 0.9540 | val_loss 0.3662 | val_acc 0.8900
Epoch [11/100] | train_loss 0.1034 | train_acc 0.9610 | val_loss 0.1947 | val_acc 0.9500
No improvement (1/15).
Epoch [12/100] | train_loss 0.1057 | train_acc 0.9603 | val_loss 0.1702 | val_acc 0.9467
Epoch [13/100] | train_loss 0.1093 | train_acc 0.9607 | val_loss 0.1763 | val_acc 0.9517
No improvement (1/15).
Epoch [14/100] | train_loss 0.0911 | train_acc 0.9650 | val_loss 0.1783 | val_acc 0.9500
No improvement (2/15).
Epoch [15/100] | train_loss 0.0950 | train_acc 0.9633 | val_loss 0.1765 | val_acc 0.9517
Epoch [16/100] | train_loss 0.0884 | train_acc 0.9647 | val_loss 0.1850 | val_acc 0.9533
No improvement (1/15).
Epoch [17/100] | train_loss 0.0888 | train_acc 0.9683 | val_loss 0.2040 | val_acc 0.9500
No improvement (2/15).
Epoch [18/100] | train_loss 0.0853 | train_acc 0.9697 | val_loss 0.2022 | val_acc 0.9517
No improvement (3/15).
Epoch [19/100] | train_loss 0.0794 | train_acc 0.9723 | val_loss 0.2190 | val_acc 0.9467
No improvement (4/15).
Epoch [20/100] | train_loss 0.0792 | train_acc 0.9693 | val_loss 0.2860 | val_acc 0.9233
No improvement (5/15).
Epoch [21/100] | train_loss 0.0734 | train_acc 0.9743 | val_loss 0.3524 | val_acc 0.9167
No improvement (6/15).
Epoch [22/100] | train_loss 0.0753 | train_acc 0.9710 | val_loss 0.4048 | val_acc 0.9083
No improvement (7/15).
Epoch [23/100] | train_loss 0.0786 | train_acc 0.9720 | val_loss 0.3484 | val_acc 0.9200
No improvement (8/15).
Epoch [24/100] | train_loss 0.0770 | train_acc 0.9720 | val_loss 0.3357 | val_acc 0.9217
No improvement (9/15).
Epoch [25/100] | train_loss 0.0747 | train_acc 0.9740 | val_loss 0.4095 | val_acc 0.9133
No improvement (10/15).
Epoch [26/100] | train_loss 0.0728 | train_acc 0.9733 | val_loss 0.2303 | val_acc 0.9533
No improvement (11/15).
Epoch [27/100] | train_loss 0.0666 | train_acc 0.9757 | val_loss 0.2325 | val_acc 0.9500
No improvement (12/15).
Epoch [28/100] | train_loss 0.0656 | train_acc 0.9740 | val_loss 0.2407 | val_acc 0.9450
No improvement (13/15).
Epoch [29/100] | train_loss 0.0658 | train_acc 0.9750 | val_loss 0.2533 | val_acc 0.9417
No improvement (14/15).
Epoch [30/100] | train_loss 0.0678 | train_acc 0.9727 | val_loss 0.2332 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 81 Finished. Best Val Acc: 95.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6907 | train_acc 0.5380 | val_loss 0.6989 | val_acc 0.5733
Epoch [2/100] | train_loss 0.6510 | train_acc 0.6383 | val_loss 0.6594 | val_acc 0.6383
Epoch [3/100] | train_loss 0.5546 | train_acc 0.7093 | val_loss 0.5066 | val_acc 0.7733
Epoch [4/100] | train_loss 0.3904 | train_acc 0.8237 | val_loss 0.5332 | val_acc 0.8100
Epoch [5/100] | train_loss 0.2965 | train_acc 0.8743 | val_loss 0.4429 | val_acc 0.8550
Epoch [6/100] | train_loss 0.2133 | train_acc 0.9207 | val_loss 0.3200 | val_acc 0.9000
Epoch [7/100] | train_loss 0.1890 | train_acc 0.9257 | val_loss 0.2550 | val_acc 0.9250
No improvement (1/15).
Epoch [8/100] | train_loss 0.1494 | train_acc 0.9423 | val_loss 0.3625 | val_acc 0.8917
Epoch [9/100] | train_loss 0.1335 | train_acc 0.9523 | val_loss 0.2342 | val_acc 0.9333
No improvement (1/15).
Epoch [10/100] | train_loss 0.1328 | train_acc 0.9550 | val_loss 0.2623 | val_acc 0.9317
No improvement (2/15).
Epoch [11/100] | train_loss 0.1291 | train_acc 0.9547 | val_loss 0.2601 | val_acc 0.9333
Epoch [12/100] | train_loss 0.1091 | train_acc 0.9633 | val_loss 0.1901 | val_acc 0.9550
No improvement (1/15).
Epoch [13/100] | train_loss 0.1070 | train_acc 0.9640 | val_loss 0.2277 | val_acc 0.9433
No improvement (2/15).
Epoch [14/100] | train_loss 0.1091 | train_acc 0.9610 | val_loss 0.2231 | val_acc 0.9417
No improvement (3/15).
Epoch [15/100] | train_loss 0.1043 | train_acc 0.9623 | val_loss 0.2267 | val_acc 0.9333
No improvement (4/15).
Epoch [16/100] | train_loss 0.0924 | train_acc 0.9687 | val_loss 0.2408 | val_acc 0.9400
No improvement (5/15).
Epoch [17/100] | train_loss 0.0998 | train_acc 0.9637 | val_loss 0.2256 | val_acc 0.9417
No improvement (6/15).
Epoch [18/100] | train_loss 0.0910 | train_acc 0.9677 | val_loss 0.2494 | val_acc 0.9383
No improvement (7/15).
Epoch [19/100] | train_loss 0.0940 | train_acc 0.9647 | val_loss 0.2386 | val_acc 0.9400wandb: updating run metadata
wandb: uploading wandb-summary.json
wandb: uploading summary, console lines 44-45
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–„â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–„â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–†â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–†â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.955
wandb:             epoch 26
wandb:         grad/norm 1.0
wandb:                lr 4e-05
wandb:         train/acc 0.97267
wandb:        train/loss 0.07889
wandb: training_time_sec 8473.03751
wandb:           val/acc 0.945
wandb:          val/loss 0.22796
wandb: 
wandb: ğŸš€ View run Trial82_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/eyhuhy9s
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_092542-eyhuhy9s/logs
[I 2026-02-08 11:46:57,170] Trial 82 finished with value: 0.955 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.000712220611563249, 'dropout': 0.44827556088310033}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 85ucwyqf
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_114657-85ucwyqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial83_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/85ucwyqf
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–ƒâ–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–ƒâ–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–‡â–†â–‡â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96333
wandb:             epoch 52
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.969
wandb:        train/loss 0.08592
wandb: training_time_sec 16635.42219
wandb:           val/acc 0.96167
wandb:          val/loss 0.2072
wandb: 
wandb: ğŸš€ View run Trial83_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/85ucwyqf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_114657-85ucwyqf/logs
[I 2026-02-08 16:24:14,775] Trial 83 finished with value: 0.9633333333333334 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00043788664644411906, 'dropout': 0.4666123700275191}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run uz267tah
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_162415-uz267tah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial84_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uz267tah

No improvement (8/15).
Epoch [20/100] | train_loss 0.0842 | train_acc 0.9720 | val_loss 0.2263 | val_acc 0.9450
No improvement (9/15).
Epoch [21/100] | train_loss 0.0850 | train_acc 0.9707 | val_loss 0.2300 | val_acc 0.9483
No improvement (10/15).
Epoch [22/100] | train_loss 0.0823 | train_acc 0.9720 | val_loss 0.2312 | val_acc 0.9483
No improvement (11/15).
Epoch [23/100] | train_loss 0.0810 | train_acc 0.9723 | val_loss 0.2241 | val_acc 0.9500
No improvement (12/15).
Epoch [24/100] | train_loss 0.0792 | train_acc 0.9733 | val_loss 0.2280 | val_acc 0.9500
No improvement (13/15).
Epoch [25/100] | train_loss 0.0777 | train_acc 0.9743 | val_loss 0.2651 | val_acc 0.9433
No improvement (14/15).
Epoch [26/100] | train_loss 0.0789 | train_acc 0.9727 | val_loss 0.2280 | val_acc 0.9450
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 82 Finished. Best Val Acc: 95.50%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6870 | train_acc 0.5450 | val_loss 0.6887 | val_acc 0.5733
Epoch [2/100] | train_loss 0.6610 | train_acc 0.6260 | val_loss 0.6624 | val_acc 0.6333
Epoch [3/100] | train_loss 0.6408 | train_acc 0.6450 | val_loss 0.6224 | val_acc 0.6350
Epoch [4/100] | train_loss 0.5838 | train_acc 0.6817 | val_loss 0.6381 | val_acc 0.6850
Epoch [5/100] | train_loss 0.4249 | train_acc 0.7980 | val_loss 0.5341 | val_acc 0.8150
Epoch [6/100] | train_loss 0.2479 | train_acc 0.9003 | val_loss 0.6111 | val_acc 0.8267
Epoch [7/100] | train_loss 0.2404 | train_acc 0.9007 | val_loss 0.2613 | val_acc 0.9150
No improvement (1/15).
Epoch [8/100] | train_loss 0.1936 | train_acc 0.9240 | val_loss 0.2979 | val_acc 0.9117
Epoch [9/100] | train_loss 0.1620 | train_acc 0.9373 | val_loss 0.2532 | val_acc 0.9283
No improvement (1/15).
Epoch [10/100] | train_loss 0.1534 | train_acc 0.9420 | val_loss 0.2757 | val_acc 0.9200
No improvement (2/15).
Epoch [11/100] | train_loss 0.1479 | train_acc 0.9420 | val_loss 0.2538 | val_acc 0.9183
No improvement (3/15).
Epoch [12/100] | train_loss 0.1481 | train_acc 0.9400 | val_loss 0.2596 | val_acc 0.9267
Epoch [13/100] | train_loss 0.1254 | train_acc 0.9520 | val_loss 0.2320 | val_acc 0.9400
No improvement (1/15).
Epoch [14/100] | train_loss 0.1171 | train_acc 0.9580 | val_loss 0.2348 | val_acc 0.9367
No improvement (2/15).
Epoch [15/100] | train_loss 0.1152 | train_acc 0.9560 | val_loss 0.2416 | val_acc 0.9317
No improvement (3/15).
Epoch [16/100] | train_loss 0.1139 | train_acc 0.9590 | val_loss 0.2507 | val_acc 0.9283
No improvement (4/15).
Epoch [17/100] | train_loss 0.1104 | train_acc 0.9607 | val_loss 0.2570 | val_acc 0.9267
No improvement (5/15).
Epoch [18/100] | train_loss 0.1070 | train_acc 0.9640 | val_loss 0.2673 | val_acc 0.9267
No improvement (6/15).
Epoch [19/100] | train_loss 0.1062 | train_acc 0.9603 | val_loss 0.2847 | val_acc 0.9233
Epoch [20/100] | train_loss 0.1018 | train_acc 0.9610 | val_loss 0.2183 | val_acc 0.9417
Epoch [21/100] | train_loss 0.1020 | train_acc 0.9633 | val_loss 0.2106 | val_acc 0.9483
Epoch [22/100] | train_loss 0.0996 | train_acc 0.9633 | val_loss 0.2104 | val_acc 0.9500
No improvement (1/15).
Epoch [23/100] | train_loss 0.0996 | train_acc 0.9623 | val_loss 0.2189 | val_acc 0.9450
No improvement (2/15).
Epoch [24/100] | train_loss 0.0985 | train_acc 0.9640 | val_loss 0.2192 | val_acc 0.9433
No improvement (3/15).
Epoch [25/100] | train_loss 0.0965 | train_acc 0.9653 | val_loss 0.2202 | val_acc 0.9467
Epoch [26/100] | train_loss 0.0962 | train_acc 0.9660 | val_loss 0.2060 | val_acc 0.9550
No improvement (1/15).
Epoch [27/100] | train_loss 0.0921 | train_acc 0.9660 | val_loss 0.2041 | val_acc 0.9550
Epoch [28/100] | train_loss 0.0914 | train_acc 0.9690 | val_loss 0.2031 | val_acc 0.9583
No improvement (1/15).
Epoch [29/100] | train_loss 0.0919 | train_acc 0.9670 | val_loss 0.2077 | val_acc 0.9550
No improvement (2/15).
Epoch [30/100] | train_loss 0.0899 | train_acc 0.9697 | val_loss 0.2073 | val_acc 0.9533
No improvement (3/15).
Epoch [31/100] | train_loss 0.0917 | train_acc 0.9683 | val_loss 0.2102 | val_acc 0.9533
Epoch [32/100] | train_loss 0.0901 | train_acc 0.9690 | val_loss 0.2040 | val_acc 0.9600
No improvement (1/15).
Epoch [33/100] | train_loss 0.0896 | train_acc 0.9683 | val_loss 0.2038 | val_acc 0.9583
No improvement (2/15).
Epoch [34/100] | train_loss 0.0877 | train_acc 0.9703 | val_loss 0.2046 | val_acc 0.9600
Epoch [35/100] | train_loss 0.0890 | train_acc 0.9693 | val_loss 0.2051 | val_acc 0.9617
No improvement (1/15).
Epoch [36/100] | train_loss 0.0875 | train_acc 0.9700 | val_loss 0.2063 | val_acc 0.9600
No improvement (2/15).
Epoch [37/100] | train_loss 0.0882 | train_acc 0.9693 | val_loss 0.2072 | val_acc 0.9600
Epoch [38/100] | train_loss 0.0882 | train_acc 0.9690 | val_loss 0.2061 | val_acc 0.9633
No improvement (1/15).
Epoch [39/100] | train_loss 0.0865 | train_acc 0.9697 | val_loss 0.2062 | val_acc 0.9600
No improvement (2/15).
Epoch [40/100] | train_loss 0.0882 | train_acc 0.9680 | val_loss 0.2062 | val_acc 0.9633
No improvement (3/15).
Epoch [41/100] | train_loss 0.0863 | train_acc 0.9693 | val_loss 0.2061 | val_acc 0.9617
No improvement (4/15).
Epoch [42/100] | train_loss 0.0863 | train_acc 0.9717 | val_loss 0.2065 | val_acc 0.9617
No improvement (5/15).
Epoch [43/100] | train_loss 0.0871 | train_acc 0.9700 | val_loss 0.2066 | val_acc 0.9633
No improvement (6/15).
Epoch [44/100] | train_loss 0.0863 | train_acc 0.9693 | val_loss 0.2067 | val_acc 0.9600
No improvement (7/15).
Epoch [45/100] | train_loss 0.0874 | train_acc 0.9690 | val_loss 0.2066 | val_acc 0.9617
No improvement (8/15).
Epoch [46/100] | train_loss 0.0847 | train_acc 0.9700 | val_loss 0.2066 | val_acc 0.9617
No improvement (9/15).
Epoch [47/100] | train_loss 0.0858 | train_acc 0.9680 | val_loss 0.2068 | val_acc 0.9600
No improvement (10/15).
Epoch [48/100] | train_loss 0.0860 | train_acc 0.9710 | val_loss 0.2069 | val_acc 0.9617
No improvement (11/15).
Epoch [49/100] | train_loss 0.0874 | train_acc 0.9700 | val_loss 0.2069 | val_acc 0.9617
No improvement (12/15).
Epoch [50/100] | train_loss 0.0857 | train_acc 0.9700 | val_loss 0.2069 | val_acc 0.9617
No improvement (13/15).
Epoch [51/100] | train_loss 0.0852 | train_acc 0.9690 | val_loss 0.2071 | val_acc 0.9617
No improvement (14/15).
Epoch [52/100] | train_loss 0.0859 | train_acc 0.9690 | val_loss 0.2072 | val_acc 0.9617
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 83 Finished. Best Val Acc: 96.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6910 | train_acc 0.5460 | val_loss 0.6933 | val_acc 0.5433
Epoch [2/100] | train_loss 0.6635 | train_acc 0.6187 | val_loss 0.6711 | val_acc 0.6133
Epoch [3/100] | train_loss 0.6435 | train_acc 0.6423 | val_loss 0.6342 | val_acc 0.6683
Epoch [4/100] | train_loss 0.5995 | train_acc 0.6910 | val_loss 0.5919 | val_acc 0.6900
Epoch [5/100] | train_loss 0.5038 | train_acc 0.7527 | val_loss 0.5285 | val_acc 0.7350
No improvement (1/15).
Epoch [6/100] | train_loss 0.4018 | train_acc 0.8177 | val_loss 0.5857 | val_acc 0.7350
Epoch [7/100] | train_loss 0.3296 | train_acc 0.8487 | val_loss 0.3869 | val_acc 0.8600
No improvement (1/15).
Epoch [8/100] | train_loss 0.2410 | train_acc 0.9070 | val_loss 0.4989 | val_acc 0.8150
No improvement (2/15).
Epoch [9/100] | train_loss 0.2272 | train_acc 0.9133 | val_loss 0.4362 | val_acc 0.8483
No improvement (3/15).
Epoch [10/100] | train_loss 0.2130 | train_acc 0.9187 | val_loss 0.5113 | val_acc 0.8133
No improvement (4/15).
Epoch [11/100] | train_loss 0.1913 | train_acc 0.9267 | val_loss 0.5312 | val_acc 0.8200
No improvement (5/15).
Epoch [12/100] | train_loss 0.1885 | train_acc 0.9300 | val_loss 0.5234 | val_acc 0.8317
Epoch [13/100] | train_loss 0.1841 | train_acc 0.9327 | val_loss 0.4049 | val_acc 0.8650
No improvement (1/15).
Epoch [14/100] | train_loss 0.1601 | train_acc 0.9420 | val_loss 0.3984 | val_acc 0.8650
No improvement (2/15).
Epoch [15/100] | train_loss 0.1590 | train_acc 0.9390 | val_loss 0.4261 | val_acc 0.8583
No improvement (3/15).
Epoch [16/100] | train_loss 0.1526 | train_acc 0.9443 | val_loss 0.4366 | val_acc 0.8550
Epoch [17/100] | train_loss 0.1504 | train_acc 0.9463 | val_loss 0.4427 | val_acc 0.8683wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–‡â–†â–…â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–„â–…â–‡â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–†â–‡â–ƒâ–…â–†â–†â–ƒâ–ƒâ–„â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.92167
wandb:             epoch 58
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.96167
wandb:        train/loss 0.10957
wandb: training_time_sec 18517.59402
wandb:           val/acc 0.92167
wandb:          val/loss 0.31368
wandb: 
wandb: ğŸš€ View run Trial84_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uz267tah
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_162415-uz267tah/logs
[I 2026-02-08 21:32:54,407] Trial 84 finished with value: 0.9216666666666666 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00038659210581020034, 'dropout': 0.46615950724744126}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run 3ejrl35m
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260208_213254-3ejrl35m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial85_[CV-Variation]_L3_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3ejrl35m
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading summary, console lines 57-58
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–„â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–„â–‡â–„â–â–ƒâ–ƒâ–‚â–„â–‚â–‚â–‚â–ƒâ–„â–‚
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–ƒâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:   val/loss â–†â–…â–‡â–â–â–ƒâ–â–‚â–…â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–ƒâ–‚â–ƒâ–…â–…â–…â–ˆâ–…â–†â–ˆâ–ˆâ–†â–…
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.94833
wandb:             epoch 31
wandb:         grad/norm 0.15419
wandb:                lr 0.00018
wandb:         train/acc 0.97633
wandb:        train/loss 0.06448
wandb: training_time_sec 10084.89291
wandb:           val/acc 0.91333
wandb:          val/loss 0.65627
wandb: 
wandb: ğŸš€ View run Trial85_[CV-Variation]_L3_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3ejrl35m
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260208_213254-3ejrl35m/logs
[I 2026-02-09 00:21:01,433] Trial 85 finished with value: 0.9483333333333334 and parameters: {'nhead': 8, 'num_layers': 3, 'd_model': 64, 'batch_size': 128, 'use_conv1d': False, 'lr': 0.0014464416287430482, 'dropout': 0.4749793436877948}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run tmnn14er
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_002101-tmnn14er
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial86_[CV-Variation]_L1_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tmnn14er

Epoch [18/100] | train_loss 0.1476 | train_acc 0.9480 | val_loss 0.3626 | val_acc 0.8900
Epoch [19/100] | train_loss 0.1395 | train_acc 0.9497 | val_loss 0.3810 | val_acc 0.8917
Epoch [20/100] | train_loss 0.1336 | train_acc 0.9490 | val_loss 0.3363 | val_acc 0.9000
Epoch [21/100] | train_loss 0.1317 | train_acc 0.9530 | val_loss 0.3282 | val_acc 0.9067
No improvement (1/15).
Epoch [22/100] | train_loss 0.1301 | train_acc 0.9527 | val_loss 0.3345 | val_acc 0.9067
Epoch [23/100] | train_loss 0.1308 | train_acc 0.9533 | val_loss 0.3256 | val_acc 0.9083
No improvement (1/15).
Epoch [24/100] | train_loss 0.1266 | train_acc 0.9577 | val_loss 0.3292 | val_acc 0.9050
Epoch [25/100] | train_loss 0.1259 | train_acc 0.9567 | val_loss 0.3289 | val_acc 0.9117
Epoch [26/100] | train_loss 0.1234 | train_acc 0.9550 | val_loss 0.3191 | val_acc 0.9133
Epoch [27/100] | train_loss 0.1209 | train_acc 0.9563 | val_loss 0.3126 | val_acc 0.9150
No improvement (1/15).
Epoch [28/100] | train_loss 0.1215 | train_acc 0.9577 | val_loss 0.3093 | val_acc 0.9150
No improvement (2/15).
Epoch [29/100] | train_loss 0.1175 | train_acc 0.9600 | val_loss 0.3100 | val_acc 0.9150
Epoch [30/100] | train_loss 0.1193 | train_acc 0.9563 | val_loss 0.3124 | val_acc 0.9167
No improvement (1/15).
Epoch [31/100] | train_loss 0.1218 | train_acc 0.9547 | val_loss 0.3113 | val_acc 0.9167
No improvement (2/15).
Epoch [32/100] | train_loss 0.1181 | train_acc 0.9567 | val_loss 0.3053 | val_acc 0.9167
No improvement (3/15).
Epoch [33/100] | train_loss 0.1161 | train_acc 0.9577 | val_loss 0.3081 | val_acc 0.9167
No improvement (4/15).
Epoch [34/100] | train_loss 0.1189 | train_acc 0.9563 | val_loss 0.3128 | val_acc 0.9167
Epoch [35/100] | train_loss 0.1157 | train_acc 0.9587 | val_loss 0.3101 | val_acc 0.9183
No improvement (1/15).
Epoch [36/100] | train_loss 0.1146 | train_acc 0.9593 | val_loss 0.3130 | val_acc 0.9183
No improvement (2/15).
Epoch [37/100] | train_loss 0.1178 | train_acc 0.9577 | val_loss 0.3152 | val_acc 0.9167
No improvement (3/15).
Epoch [38/100] | train_loss 0.1163 | train_acc 0.9593 | val_loss 0.3108 | val_acc 0.9183
No improvement (4/15).
Epoch [39/100] | train_loss 0.1179 | train_acc 0.9593 | val_loss 0.3110 | val_acc 0.9167
No improvement (5/15).
Epoch [40/100] | train_loss 0.1140 | train_acc 0.9580 | val_loss 0.3107 | val_acc 0.9167
No improvement (6/15).
Epoch [41/100] | train_loss 0.1158 | train_acc 0.9563 | val_loss 0.3107 | val_acc 0.9167
Epoch [42/100] | train_loss 0.1151 | train_acc 0.9603 | val_loss 0.3120 | val_acc 0.9200
No improvement (1/15).
Epoch [43/100] | train_loss 0.1146 | train_acc 0.9577 | val_loss 0.3111 | val_acc 0.9183
Epoch [44/100] | train_loss 0.1156 | train_acc 0.9603 | val_loss 0.3134 | val_acc 0.9217
No improvement (1/15).
Epoch [45/100] | train_loss 0.1131 | train_acc 0.9603 | val_loss 0.3120 | val_acc 0.9200
No improvement (2/15).
Epoch [46/100] | train_loss 0.1134 | train_acc 0.9597 | val_loss 0.3122 | val_acc 0.9200
No improvement (3/15).
Epoch [47/100] | train_loss 0.1138 | train_acc 0.9577 | val_loss 0.3127 | val_acc 0.9217
No improvement (4/15).
Epoch [48/100] | train_loss 0.1123 | train_acc 0.9623 | val_loss 0.3114 | val_acc 0.9183
No improvement (5/15).
Epoch [49/100] | train_loss 0.1117 | train_acc 0.9600 | val_loss 0.3110 | val_acc 0.9200
No improvement (6/15).
Epoch [50/100] | train_loss 0.1112 | train_acc 0.9597 | val_loss 0.3126 | val_acc 0.9217
No improvement (7/15).
Epoch [51/100] | train_loss 0.1140 | train_acc 0.9587 | val_loss 0.3135 | val_acc 0.9217
No improvement (8/15).
Epoch [52/100] | train_loss 0.1097 | train_acc 0.9593 | val_loss 0.3129 | val_acc 0.9217
No improvement (9/15).
Epoch [53/100] | train_loss 0.1099 | train_acc 0.9600 | val_loss 0.3129 | val_acc 0.9217
No improvement (10/15).
Epoch [54/100] | train_loss 0.1121 | train_acc 0.9600 | val_loss 0.3136 | val_acc 0.9217
No improvement (11/15).
Epoch [55/100] | train_loss 0.1096 | train_acc 0.9633 | val_loss 0.3138 | val_acc 0.9217
No improvement (12/15).
Epoch [56/100] | train_loss 0.1103 | train_acc 0.9600 | val_loss 0.3133 | val_acc 0.9217
No improvement (13/15).
Epoch [57/100] | train_loss 0.1101 | train_acc 0.9610 | val_loss 0.3138 | val_acc 0.9217
No improvement (14/15).
Epoch [58/100] | train_loss 0.1096 | train_acc 0.9617 | val_loss 0.3137 | val_acc 0.9217
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 84 Finished. Best Val Acc: 92.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7197 | train_acc 0.5167 | val_loss 0.6787 | val_acc 0.5817
Epoch [2/100] | train_loss 0.6546 | train_acc 0.6183 | val_loss 0.5894 | val_acc 0.7033
No improvement (1/15).
Epoch [3/100] | train_loss 0.4026 | train_acc 0.8257 | val_loss 0.8744 | val_acc 0.6767
Epoch [4/100] | train_loss 0.2455 | train_acc 0.9013 | val_loss 0.2206 | val_acc 0.9267
Epoch [5/100] | train_loss 0.1643 | train_acc 0.9383 | val_loss 0.1764 | val_acc 0.9433
No improvement (1/15).
Epoch [6/100] | train_loss 0.1494 | train_acc 0.9460 | val_loss 0.4430 | val_acc 0.8650
No improvement (2/15).
Epoch [7/100] | train_loss 0.1325 | train_acc 0.9507 | val_loss 0.2182 | val_acc 0.9317
No improvement (3/15).
Epoch [8/100] | train_loss 0.1332 | train_acc 0.9527 | val_loss 0.2472 | val_acc 0.9350
No improvement (4/15).
Epoch [9/100] | train_loss 0.1303 | train_acc 0.9543 | val_loss 0.6152 | val_acc 0.8750
No improvement (5/15).
Epoch [10/100] | train_loss 0.1040 | train_acc 0.9640 | val_loss 0.4094 | val_acc 0.9283
No improvement (6/15).
Epoch [11/100] | train_loss 0.0833 | train_acc 0.9710 | val_loss 0.4429 | val_acc 0.9167
No improvement (7/15).
Epoch [12/100] | train_loss 0.0814 | train_acc 0.9723 | val_loss 0.3130 | val_acc 0.9400
Epoch [13/100] | train_loss 0.0782 | train_acc 0.9707 | val_loss 0.2441 | val_acc 0.9467
No improvement (1/15).
Epoch [14/100] | train_loss 0.0847 | train_acc 0.9683 | val_loss 0.2350 | val_acc 0.9450
No improvement (2/15).
Epoch [15/100] | train_loss 0.0684 | train_acc 0.9783 | val_loss 0.3764 | val_acc 0.9383
No improvement (3/15).
Epoch [16/100] | train_loss 0.0659 | train_acc 0.9790 | val_loss 0.3384 | val_acc 0.9367
Epoch [17/100] | train_loss 0.0794 | train_acc 0.9720 | val_loss 0.2195 | val_acc 0.9483
No improvement (1/15).
Epoch [18/100] | train_loss 0.0737 | train_acc 0.9760 | val_loss 0.2680 | val_acc 0.9450
No improvement (2/15).
Epoch [19/100] | train_loss 0.0617 | train_acc 0.9810 | val_loss 0.3663 | val_acc 0.9383
No improvement (3/15).
Epoch [20/100] | train_loss 0.0559 | train_acc 0.9827 | val_loss 0.3280 | val_acc 0.9450
No improvement (4/15).
Epoch [21/100] | train_loss 0.0582 | train_acc 0.9830 | val_loss 0.3929 | val_acc 0.9367
No improvement (5/15).
Epoch [22/100] | train_loss 0.0743 | train_acc 0.9743 | val_loss 0.6484 | val_acc 0.9083
No improvement (6/15).
Epoch [23/100] | train_loss 0.0758 | train_acc 0.9730 | val_loss 0.6366 | val_acc 0.9067
No improvement (7/15).
Epoch [24/100] | train_loss 0.0732 | train_acc 0.9747 | val_loss 0.6472 | val_acc 0.9083
No improvement (8/15).
Epoch [25/100] | train_loss 0.0639 | train_acc 0.9780 | val_loss 0.9072 | val_acc 0.8800
No improvement (9/15).
Epoch [26/100] | train_loss 0.0735 | train_acc 0.9740 | val_loss 0.6616 | val_acc 0.9117
No improvement (10/15).
Epoch [27/100] | train_loss 0.0650 | train_acc 0.9763 | val_loss 0.7601 | val_acc 0.8983
No improvement (11/15).
Epoch [28/100] | train_loss 0.0649 | train_acc 0.9770 | val_loss 0.9564 | val_acc 0.8867
No improvement (12/15).
Epoch [29/100] | train_loss 0.0648 | train_acc 0.9780 | val_loss 0.9473 | val_acc 0.8883
No improvement (13/15).
Epoch [30/100] | train_loss 0.0670 | train_acc 0.9777 | val_loss 0.7280 | val_acc 0.9083
No improvement (14/15).
Epoch [31/100] | train_loss 0.0645 | train_acc 0.9763 | val_loss 0.6563 | val_acc 0.9133
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 85 Finished. Best Val Acc: 94.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6934 | train_acc 0.5127 | val_loss 0.6920 | val_acc 0.5100
Epoch [2/100] | train_loss 0.6905 | train_acc 0.5350 | val_loss 0.6898 | val_acc 0.5217
Epoch [3/100] | train_loss 0.6872 | train_acc 0.5510 | val_loss 0.6859 | val_acc 0.5250wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–â–‚â–‚â–‚â–‚â–ƒâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–…â–…â–…â–…â–…â–…â–„â–„â–„â–…â–…â–…â–„â–„
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–‚â–ƒâ–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–‚â–ƒâ–…â–„â–„â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:   val/loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.71833
wandb:             epoch 33
wandb:         grad/norm 0.63076
wandb:                lr 0.0
wandb:         train/acc 0.726
wandb:        train/loss 0.50892
wandb: training_time_sec 1913.95682
wandb:           val/acc 0.70167
wandb:          val/loss 0.54306
wandb: 
wandb: ğŸš€ View run Trial86_[CV-Variation]_L1_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tmnn14er
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_002101-tmnn14er/logs
[I 2026-02-09 00:52:57,376] Trial 86 finished with value: 0.7183333333333334 and parameters: {'nhead': 4, 'num_layers': 1, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00014162720064743003, 'dropout': 0.3944846086002336}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run ue6ukq7n
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_005257-ue6ukq7n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial87_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ue6ukq7n
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–†â–‡â–ˆâ–‡â–ˆâ–‡â–…â–„â–„â–…â–…â–ˆâ–‡â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–ƒâ–„â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–‡â–‡â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–‚â–„â–†â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–‡â–†â–†â–…â–ƒâ–ˆâ–„â–‚â–‚â–â–â–â–â–â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95167
wandb:             epoch 45
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.97567
wandb:        train/loss 0.07238
wandb: training_time_sec 19245.84041
wandb:           val/acc 0.94833
wandb:          val/loss 0.22191
wandb: 
wandb: ğŸš€ View run Trial87_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ue6ukq7n
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_005257-ue6ukq7n/logs
[I 2026-02-09 06:13:45,215] Trial 87 finished with value: 0.9516666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.00032170819108469405, 'dropout': 0.41353191079277274}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run hajiqsdj
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_061345-hajiqsdj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial88_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hajiqsdj

Epoch [4/100] | train_loss 0.6813 | train_acc 0.5843 | val_loss 0.6791 | val_acc 0.5750
Epoch [5/100] | train_loss 0.6707 | train_acc 0.6197 | val_loss 0.6660 | val_acc 0.6167
No improvement (1/15).
Epoch [6/100] | train_loss 0.6513 | train_acc 0.6287 | val_loss 0.6448 | val_acc 0.6117
No improvement (2/15).
Epoch [7/100] | train_loss 0.6260 | train_acc 0.6393 | val_loss 0.6236 | val_acc 0.6100
Epoch [8/100] | train_loss 0.6078 | train_acc 0.6547 | val_loss 0.6134 | val_acc 0.6650
No improvement (1/15).
Epoch [9/100] | train_loss 0.5979 | train_acc 0.6683 | val_loss 0.6059 | val_acc 0.6650
Epoch [10/100] | train_loss 0.5891 | train_acc 0.6683 | val_loss 0.5986 | val_acc 0.6683
Epoch [11/100] | train_loss 0.5807 | train_acc 0.6733 | val_loss 0.5914 | val_acc 0.6717
Epoch [12/100] | train_loss 0.5724 | train_acc 0.6820 | val_loss 0.5843 | val_acc 0.6783
Epoch [13/100] | train_loss 0.5638 | train_acc 0.6873 | val_loss 0.5772 | val_acc 0.6900
Epoch [14/100] | train_loss 0.5571 | train_acc 0.6853 | val_loss 0.5739 | val_acc 0.7017
Epoch [15/100] | train_loss 0.5530 | train_acc 0.6883 | val_loss 0.5706 | val_acc 0.7067
No improvement (1/15).
Epoch [16/100] | train_loss 0.5485 | train_acc 0.6920 | val_loss 0.5674 | val_acc 0.7050
Epoch [17/100] | train_loss 0.5440 | train_acc 0.6950 | val_loss 0.5642 | val_acc 0.7083
Epoch [18/100] | train_loss 0.5396 | train_acc 0.6993 | val_loss 0.5610 | val_acc 0.7100
Epoch [19/100] | train_loss 0.5348 | train_acc 0.7010 | val_loss 0.5579 | val_acc 0.7183
No improvement (1/15).
Epoch [20/100] | train_loss 0.5312 | train_acc 0.7060 | val_loss 0.5559 | val_acc 0.7000
No improvement (2/15).
Epoch [21/100] | train_loss 0.5287 | train_acc 0.7077 | val_loss 0.5544 | val_acc 0.6950
No improvement (3/15).
Epoch [22/100] | train_loss 0.5262 | train_acc 0.7093 | val_loss 0.5528 | val_acc 0.6967
No improvement (4/15).
Epoch [23/100] | train_loss 0.5239 | train_acc 0.7113 | val_loss 0.5513 | val_acc 0.6983
No improvement (5/15).
Epoch [24/100] | train_loss 0.5215 | train_acc 0.7147 | val_loss 0.5498 | val_acc 0.7017
No improvement (6/15).
Epoch [25/100] | train_loss 0.5191 | train_acc 0.7177 | val_loss 0.5483 | val_acc 0.7083
No improvement (7/15).
Epoch [26/100] | train_loss 0.5169 | train_acc 0.7207 | val_loss 0.5476 | val_acc 0.6933
No improvement (8/15).
Epoch [27/100] | train_loss 0.5154 | train_acc 0.7183 | val_loss 0.5468 | val_acc 0.6950
No improvement (9/15).
Epoch [28/100] | train_loss 0.5144 | train_acc 0.7213 | val_loss 0.5461 | val_acc 0.6967
No improvement (10/15).
Epoch [29/100] | train_loss 0.5132 | train_acc 0.7223 | val_loss 0.5453 | val_acc 0.6933
No improvement (11/15).
Epoch [30/100] | train_loss 0.5117 | train_acc 0.7223 | val_loss 0.5445 | val_acc 0.6950
No improvement (12/15).
Epoch [31/100] | train_loss 0.5106 | train_acc 0.7247 | val_loss 0.5438 | val_acc 0.7017
No improvement (13/15).
Epoch [32/100] | train_loss 0.5096 | train_acc 0.7257 | val_loss 0.5434 | val_acc 0.7017
No improvement (14/15).
Epoch [33/100] | train_loss 0.5089 | train_acc 0.7260 | val_loss 0.5431 | val_acc 0.7017
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 86 Finished. Best Val Acc: 71.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6943 | train_acc 0.5387 | val_loss 0.6851 | val_acc 0.5983
Epoch [2/100] | train_loss 0.6641 | train_acc 0.6240 | val_loss 0.6558 | val_acc 0.6283
Epoch [3/100] | train_loss 0.6358 | train_acc 0.6433 | val_loss 0.6024 | val_acc 0.6517
Epoch [4/100] | train_loss 0.5683 | train_acc 0.7053 | val_loss 0.5322 | val_acc 0.7333
Epoch [5/100] | train_loss 0.4252 | train_acc 0.8177 | val_loss 0.3502 | val_acc 0.8483
No improvement (1/15).
Epoch [6/100] | train_loss 0.2957 | train_acc 0.8730 | val_loss 0.8124 | val_acc 0.7850
Epoch [7/100] | train_loss 0.2284 | train_acc 0.9063 | val_loss 0.4763 | val_acc 0.8667
Epoch [8/100] | train_loss 0.2438 | train_acc 0.9047 | val_loss 0.2881 | val_acc 0.9167
No improvement (1/15).
Epoch [9/100] | train_loss 0.2331 | train_acc 0.9053 | val_loss 0.3305 | val_acc 0.8883
Epoch [10/100] | train_loss 0.1751 | train_acc 0.9297 | val_loss 0.2561 | val_acc 0.9267
Epoch [11/100] | train_loss 0.1441 | train_acc 0.9473 | val_loss 0.2292 | val_acc 0.9317
No improvement (1/15).
Epoch [12/100] | train_loss 0.1428 | train_acc 0.9490 | val_loss 0.2180 | val_acc 0.9283
No improvement (2/15).
Epoch [13/100] | train_loss 0.1335 | train_acc 0.9480 | val_loss 0.1979 | val_acc 0.9317
Epoch [14/100] | train_loss 0.1197 | train_acc 0.9550 | val_loss 0.1943 | val_acc 0.9367
Epoch [15/100] | train_loss 0.1122 | train_acc 0.9583 | val_loss 0.1959 | val_acc 0.9417
No improvement (1/15).
Epoch [16/100] | train_loss 0.1171 | train_acc 0.9570 | val_loss 0.2056 | val_acc 0.9417
No improvement (2/15).
Epoch [17/100] | train_loss 0.1011 | train_acc 0.9660 | val_loss 0.1889 | val_acc 0.9417
Epoch [18/100] | train_loss 0.1011 | train_acc 0.9653 | val_loss 0.1878 | val_acc 0.9433
No improvement (1/15).
Epoch [19/100] | train_loss 0.0957 | train_acc 0.9650 | val_loss 0.3177 | val_acc 0.9017
No improvement (2/15).
Epoch [20/100] | train_loss 0.0985 | train_acc 0.9640 | val_loss 0.3256 | val_acc 0.9033
No improvement (3/15).
Epoch [21/100] | train_loss 0.1030 | train_acc 0.9610 | val_loss 0.3068 | val_acc 0.9033
No improvement (4/15).
Epoch [22/100] | train_loss 0.1011 | train_acc 0.9627 | val_loss 0.2859 | val_acc 0.9150
No improvement (5/15).
Epoch [23/100] | train_loss 0.0981 | train_acc 0.9627 | val_loss 0.2931 | val_acc 0.9100
No improvement (6/15).
Epoch [24/100] | train_loss 0.0969 | train_acc 0.9653 | val_loss 0.2404 | val_acc 0.9350
Epoch [25/100] | train_loss 0.0834 | train_acc 0.9697 | val_loss 0.1961 | val_acc 0.9500
No improvement (1/15).
Epoch [26/100] | train_loss 0.0807 | train_acc 0.9713 | val_loss 0.1998 | val_acc 0.9500
No improvement (2/15).
Epoch [27/100] | train_loss 0.0780 | train_acc 0.9740 | val_loss 0.2137 | val_acc 0.9500
No improvement (3/15).
Epoch [28/100] | train_loss 0.0814 | train_acc 0.9713 | val_loss 0.2204 | val_acc 0.9433
No improvement (4/15).
Epoch [29/100] | train_loss 0.0832 | train_acc 0.9687 | val_loss 0.2171 | val_acc 0.9483
No improvement (5/15).
Epoch [30/100] | train_loss 0.0803 | train_acc 0.9713 | val_loss 0.2204 | val_acc 0.9467
Epoch [31/100] | train_loss 0.0764 | train_acc 0.9723 | val_loss 0.2099 | val_acc 0.9517
No improvement (1/15).
Epoch [32/100] | train_loss 0.0758 | train_acc 0.9750 | val_loss 0.2156 | val_acc 0.9500
No improvement (2/15).
Epoch [33/100] | train_loss 0.0741 | train_acc 0.9740 | val_loss 0.2212 | val_acc 0.9500
No improvement (3/15).
Epoch [34/100] | train_loss 0.0746 | train_acc 0.9737 | val_loss 0.2204 | val_acc 0.9500
No improvement (4/15).
Epoch [35/100] | train_loss 0.0746 | train_acc 0.9727 | val_loss 0.2198 | val_acc 0.9500
No improvement (5/15).
Epoch [36/100] | train_loss 0.0772 | train_acc 0.9730 | val_loss 0.2239 | val_acc 0.9467
No improvement (6/15).
Epoch [37/100] | train_loss 0.0750 | train_acc 0.9740 | val_loss 0.2127 | val_acc 0.9517
No improvement (7/15).
Epoch [38/100] | train_loss 0.0732 | train_acc 0.9737 | val_loss 0.2153 | val_acc 0.9500
No improvement (8/15).
Epoch [39/100] | train_loss 0.0749 | train_acc 0.9747 | val_loss 0.2171 | val_acc 0.9483
No improvement (9/15).
Epoch [40/100] | train_loss 0.0729 | train_acc 0.9753 | val_loss 0.2104 | val_acc 0.9517
No improvement (10/15).
Epoch [41/100] | train_loss 0.0742 | train_acc 0.9750 | val_loss 0.2216 | val_acc 0.9467
No improvement (11/15).
Epoch [42/100] | train_loss 0.0726 | train_acc 0.9763 | val_loss 0.2176 | val_acc 0.9500
No improvement (12/15).
Epoch [43/100] | train_loss 0.0719 | train_acc 0.9763 | val_loss 0.2192 | val_acc 0.9483
No improvement (13/15).
Epoch [44/100] | train_loss 0.0717 | train_acc 0.9757 | val_loss 0.2182 | val_acc 0.9500
No improvement (14/15).
Epoch [45/100] | train_loss 0.0724 | train_acc 0.9757 | val_loss 0.2219 | val_acc 0.9483
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 87 Finished. Best Val Acc: 95.17%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7272 | train_acc 0.5483 | val_loss 0.6999 | val_acc 0.5750
Epoch [2/100] | train_loss 0.6460 | train_acc 0.6383 | val_loss 0.6056 | val_acc 0.6467wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–†â–…â–ˆâ–„â–ƒâ–ˆâ–‡â–„â–‡â–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–…â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–†â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96333
wandb:             epoch 48
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.986
wandb:        train/loss 0.0456
wandb: training_time_sec 20553.12784
wandb:           val/acc 0.96
wandb:          val/loss 0.15834
wandb: 
wandb: ğŸš€ View run Trial88_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hajiqsdj
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_061345-hajiqsdj/logs
[I 2026-02-09 11:56:20,515] Trial 88 finished with value: 0.9633333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0008414551300520405, 'dropout': 0.198531226195205}. Best is trial 47 with value: 0.9633333333333334.
wandb: setting up run lucbmw0c
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_115621-lucbmw0c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial89_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/lucbmw0c
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–„â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–…â–†â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–ˆâ–†â–„â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 24
wandb:         grad/norm 1.0
wandb:                lr 6e-05
wandb:         train/acc 0.96933
wandb:        train/loss 0.08659
wandb: training_time_sec 10540.83854
wandb:           val/acc 0.96
wandb:          val/loss 0.14176
wandb: 
wandb: ğŸš€ View run Trial89_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/lucbmw0c
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_115621-lucbmw0c/logs
[I 2026-02-09 14:52:03,777] Trial 89 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0004989591417726712, 'dropout': 0.2218184464595088}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_145204-fernow31
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial90_[CV-Variation]_L4_H4_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fernow31

Epoch [3/100] | train_loss 0.4236 | train_acc 0.8020 | val_loss 0.4790 | val_acc 0.8017
Epoch [4/100] | train_loss 0.2595 | train_acc 0.8960 | val_loss 0.5803 | val_acc 0.8417
Epoch [5/100] | train_loss 0.2239 | train_acc 0.9137 | val_loss 0.3286 | val_acc 0.8683
Epoch [6/100] | train_loss 0.1570 | train_acc 0.9417 | val_loss 0.1446 | val_acc 0.9500
No improvement (1/15).
Epoch [7/100] | train_loss 0.1607 | train_acc 0.9393 | val_loss 0.2545 | val_acc 0.9133
No improvement (2/15).
Epoch [8/100] | train_loss 0.1172 | train_acc 0.9567 | val_loss 0.1890 | val_acc 0.9483
No improvement (3/15).
Epoch [9/100] | train_loss 0.1071 | train_acc 0.9607 | val_loss 0.2152 | val_acc 0.9433
No improvement (4/15).
Epoch [10/100] | train_loss 0.1060 | train_acc 0.9597 | val_loss 0.1924 | val_acc 0.9383
Epoch [11/100] | train_loss 0.0984 | train_acc 0.9660 | val_loss 0.1720 | val_acc 0.9517
Epoch [12/100] | train_loss 0.0994 | train_acc 0.9643 | val_loss 0.1849 | val_acc 0.9550
No improvement (1/15).
Epoch [13/100] | train_loss 0.0878 | train_acc 0.9667 | val_loss 0.1906 | val_acc 0.9533
No improvement (2/15).
Epoch [14/100] | train_loss 0.0915 | train_acc 0.9650 | val_loss 0.1899 | val_acc 0.9450
No improvement (3/15).
Epoch [15/100] | train_loss 0.0813 | train_acc 0.9713 | val_loss 0.1786 | val_acc 0.9483
No improvement (4/15).
Epoch [16/100] | train_loss 0.0808 | train_acc 0.9720 | val_loss 0.1795 | val_acc 0.9500
No improvement (5/15).
Epoch [17/100] | train_loss 0.0753 | train_acc 0.9747 | val_loss 0.1763 | val_acc 0.9550
Epoch [18/100] | train_loss 0.0771 | train_acc 0.9740 | val_loss 0.1805 | val_acc 0.9567
No improvement (1/15).
Epoch [19/100] | train_loss 0.0766 | train_acc 0.9730 | val_loss 0.1808 | val_acc 0.9567
No improvement (2/15).
Epoch [20/100] | train_loss 0.0657 | train_acc 0.9797 | val_loss 0.1809 | val_acc 0.9450
No improvement (3/15).
Epoch [21/100] | train_loss 0.0641 | train_acc 0.9783 | val_loss 0.1693 | val_acc 0.9500
No improvement (4/15).
Epoch [22/100] | train_loss 0.0625 | train_acc 0.9797 | val_loss 0.1640 | val_acc 0.9517
No improvement (5/15).
Epoch [23/100] | train_loss 0.0591 | train_acc 0.9793 | val_loss 0.1632 | val_acc 0.9550
No improvement (6/15).
Epoch [24/100] | train_loss 0.0630 | train_acc 0.9787 | val_loss 0.2026 | val_acc 0.9467
Epoch [25/100] | train_loss 0.0606 | train_acc 0.9787 | val_loss 0.1651 | val_acc 0.9583
No improvement (1/15).
Epoch [26/100] | train_loss 0.0614 | train_acc 0.9780 | val_loss 0.1684 | val_acc 0.9483
No improvement (2/15).
Epoch [27/100] | train_loss 0.0623 | train_acc 0.9793 | val_loss 0.1662 | val_acc 0.9533
No improvement (3/15).
Epoch [28/100] | train_loss 0.0636 | train_acc 0.9777 | val_loss 0.1611 | val_acc 0.9550
No improvement (4/15).
Epoch [29/100] | train_loss 0.0616 | train_acc 0.9780 | val_loss 0.1646 | val_acc 0.9533
No improvement (5/15).
Epoch [30/100] | train_loss 0.0613 | train_acc 0.9797 | val_loss 0.1633 | val_acc 0.9533
No improvement (6/15).
Epoch [31/100] | train_loss 0.0589 | train_acc 0.9807 | val_loss 0.1636 | val_acc 0.9517
Epoch [32/100] | train_loss 0.0555 | train_acc 0.9837 | val_loss 0.1501 | val_acc 0.9617
No improvement (1/15).
Epoch [33/100] | train_loss 0.0520 | train_acc 0.9833 | val_loss 0.1523 | val_acc 0.9600
Epoch [34/100] | train_loss 0.0520 | train_acc 0.9837 | val_loss 0.1521 | val_acc 0.9633
No improvement (1/15).
Epoch [35/100] | train_loss 0.0507 | train_acc 0.9823 | val_loss 0.1535 | val_acc 0.9633
No improvement (2/15).
Epoch [36/100] | train_loss 0.0507 | train_acc 0.9833 | val_loss 0.1553 | val_acc 0.9600
No improvement (3/15).
Epoch [37/100] | train_loss 0.0514 | train_acc 0.9847 | val_loss 0.1538 | val_acc 0.9633
No improvement (4/15).
Epoch [38/100] | train_loss 0.0492 | train_acc 0.9857 | val_loss 0.1529 | val_acc 0.9617
No improvement (5/15).
Epoch [39/100] | train_loss 0.0486 | train_acc 0.9857 | val_loss 0.1540 | val_acc 0.9617
No improvement (6/15).
Epoch [40/100] | train_loss 0.0478 | train_acc 0.9857 | val_loss 0.1540 | val_acc 0.9617
No improvement (7/15).
Epoch [41/100] | train_loss 0.0471 | train_acc 0.9853 | val_loss 0.1540 | val_acc 0.9617
No improvement (8/15).
Epoch [42/100] | train_loss 0.0479 | train_acc 0.9860 | val_loss 0.1548 | val_acc 0.9600
No improvement (9/15).
Epoch [43/100] | train_loss 0.0480 | train_acc 0.9860 | val_loss 0.1550 | val_acc 0.9633
No improvement (10/15).
Epoch [44/100] | train_loss 0.0477 | train_acc 0.9850 | val_loss 0.1578 | val_acc 0.9600
No improvement (11/15).
Epoch [45/100] | train_loss 0.0467 | train_acc 0.9867 | val_loss 0.1573 | val_acc 0.9600
No improvement (12/15).
Epoch [46/100] | train_loss 0.0476 | train_acc 0.9850 | val_loss 0.1580 | val_acc 0.9617
No improvement (13/15).
Epoch [47/100] | train_loss 0.0458 | train_acc 0.9860 | val_loss 0.1584 | val_acc 0.9600
No improvement (14/15).
Epoch [48/100] | train_loss 0.0456 | train_acc 0.9860 | val_loss 0.1583 | val_acc 0.9600
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 88 Finished. Best Val Acc: 96.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7138 | train_acc 0.5220 | val_loss 0.6994 | val_acc 0.5317
Epoch [2/100] | train_loss 0.6497 | train_acc 0.6380 | val_loss 0.6680 | val_acc 0.6217
Epoch [3/100] | train_loss 0.5754 | train_acc 0.6890 | val_loss 0.5056 | val_acc 0.7550
Epoch [4/100] | train_loss 0.3787 | train_acc 0.8313 | val_loss 0.3922 | val_acc 0.8550
No improvement (1/15).
Epoch [5/100] | train_loss 0.2994 | train_acc 0.8760 | val_loss 0.3631 | val_acc 0.8500
Epoch [6/100] | train_loss 0.2245 | train_acc 0.9087 | val_loss 0.2523 | val_acc 0.9067
Epoch [7/100] | train_loss 0.1672 | train_acc 0.9383 | val_loss 0.1893 | val_acc 0.9317
Epoch [8/100] | train_loss 0.1316 | train_acc 0.9487 | val_loss 0.1567 | val_acc 0.9500
No improvement (1/15).
Epoch [9/100] | train_loss 0.1283 | train_acc 0.9533 | val_loss 0.1553 | val_acc 0.9483
Epoch [10/100] | train_loss 0.1174 | train_acc 0.9570 | val_loss 0.1511 | val_acc 0.9600
No improvement (1/15).
Epoch [11/100] | train_loss 0.1169 | train_acc 0.9573 | val_loss 0.1430 | val_acc 0.9567
No improvement (2/15).
Epoch [12/100] | train_loss 0.1101 | train_acc 0.9630 | val_loss 0.1757 | val_acc 0.9500
No improvement (3/15).
Epoch [13/100] | train_loss 0.1257 | train_acc 0.9540 | val_loss 0.1458 | val_acc 0.9583
No improvement (4/15).
Epoch [14/100] | train_loss 0.1038 | train_acc 0.9627 | val_loss 0.1544 | val_acc 0.9583
No improvement (5/15).
Epoch [15/100] | train_loss 0.1083 | train_acc 0.9610 | val_loss 0.1533 | val_acc 0.9533
No improvement (6/15).
Epoch [16/100] | train_loss 0.0990 | train_acc 0.9650 | val_loss 0.1431 | val_acc 0.9600
No improvement (7/15).
Epoch [17/100] | train_loss 0.0985 | train_acc 0.9657 | val_loss 0.1497 | val_acc 0.9583
No improvement (8/15).
Epoch [18/100] | train_loss 0.0966 | train_acc 0.9677 | val_loss 0.1476 | val_acc 0.9600
No improvement (9/15).
Epoch [19/100] | train_loss 0.0969 | train_acc 0.9670 | val_loss 0.1523 | val_acc 0.9583
No improvement (10/15).
Epoch [20/100] | train_loss 0.0951 | train_acc 0.9687 | val_loss 0.1470 | val_acc 0.9550
No improvement (11/15).
Epoch [21/100] | train_loss 0.0865 | train_acc 0.9670 | val_loss 0.1423 | val_acc 0.9567
No improvement (12/15).
Epoch [22/100] | train_loss 0.0854 | train_acc 0.9677 | val_loss 0.1410 | val_acc 0.9600
No improvement (13/15).
Epoch [23/100] | train_loss 0.0915 | train_acc 0.9657 | val_loss 0.1531 | val_acc 0.9517
No improvement (14/15).
Epoch [24/100] | train_loss 0.0866 | train_acc 0.9693 | val_loss 0.1418 | val_acc 0.9600
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 89 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7214 | train_acc 0.4957 | val_loss 0.7057 | val_acc 0.4900
No improvement (1/15).
Epoch [2/100] | train_loss 0.6964 | train_acc 0.4930 | val_loss 0.6937 | val_acc 0.4900
No improvement (2/15).
Epoch [3/100] | train_loss 0.6937 | train_acc 0.5090 | val_loss 0.6945 | val_acc 0.4900
No improvement (3/15).
Epoch [4/100] | train_loss 0.6935 | train_acc 0.5053 | val_loss 0.6930 | val_acc 0.4900
Epoch [5/100] | train_loss 0.6865 | train_acc 0.5497 | val_loss 0.7634 | val_acc 0.5200wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–‚â–â–â–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–â–â–â–‚â–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–â–â–â–†â–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–‡â–‡â–‡â–‡â–ˆâ–ƒâ–†â–‚â–‚â–‚â–‚â–ƒâ–â–â–‚â–ƒâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.92833
wandb:             epoch 52
wandb:         grad/norm 1.0
wandb:                lr 1e-05
wandb:         train/acc 0.984
wandb:        train/loss 0.03951
wandb: training_time_sec 11817.04156
wandb:           val/acc 0.92333
wandb:          val/loss 0.4621
wandb: 
wandb: ğŸš€ View run Trial90_[CV-Variation]_L4_H4_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fernow31
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_145204-fernow31/logs
[I 2026-02-09 18:09:03,283] Trial 90 finished with value: 0.9283333333333333 and parameters: {'nhead': 4, 'num_layers': 4, 'd_model': 64, 'batch_size': 128, 'use_conv1d': True, 'lr': 0.0018757386110192272, 'dropout': 0.2613855541286447}. Best is trial 47 with value: 0.9633333333333334.
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_180903-7m6szufy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial91_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7m6szufy
wandb: updating run metadata
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 40-41
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–„â–ƒâ–ƒâ–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–„â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96667
wandb:             epoch 25
wandb:         grad/norm 1.0
wandb:                lr 5e-05
wandb:         train/acc 0.979
wandb:        train/loss 0.06693
wandb: training_time_sec 10867.16713
wandb:           val/acc 0.94333
wandb:          val/loss 0.16541
wandb: 
wandb: ğŸš€ View run Trial91_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7m6szufy
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_180903-7m6szufy/logs
[I 2026-02-09 21:10:12,815] Trial 91 finished with value: 0.9666666666666667 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0008730925605143978, 'dropout': 0.12748163497467557}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run cinwloox
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260209_211013-cinwloox
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial92_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/cinwloox

Epoch [6/100] | train_loss 0.6528 | train_acc 0.6203 | val_loss 0.6343 | val_acc 0.6083
Epoch [7/100] | train_loss 0.5792 | train_acc 0.7010 | val_loss 0.4716 | val_acc 0.7850
No improvement (1/15).
Epoch [8/100] | train_loss 0.4682 | train_acc 0.7807 | val_loss 0.6284 | val_acc 0.7133
Epoch [9/100] | train_loss 0.4336 | train_acc 0.8013 | val_loss 0.3714 | val_acc 0.8500
No improvement (1/15).
Epoch [10/100] | train_loss 0.3780 | train_acc 0.8380 | val_loss 0.3769 | val_acc 0.8450
Epoch [11/100] | train_loss 0.3089 | train_acc 0.8683 | val_loss 0.3476 | val_acc 0.8650
No improvement (1/15).
Epoch [12/100] | train_loss 0.2870 | train_acc 0.8793 | val_loss 0.4674 | val_acc 0.8167
Epoch [13/100] | train_loss 0.2458 | train_acc 0.8990 | val_loss 0.3581 | val_acc 0.8833
No improvement (1/15).
Epoch [14/100] | train_loss 0.2379 | train_acc 0.9070 | val_loss 0.4457 | val_acc 0.8433
Epoch [15/100] | train_loss 0.2105 | train_acc 0.9163 | val_loss 0.3116 | val_acc 0.8933
No improvement (1/15).
Epoch [16/100] | train_loss 0.1648 | train_acc 0.9400 | val_loss 0.3390 | val_acc 0.8850
Epoch [17/100] | train_loss 0.1834 | train_acc 0.9307 | val_loss 0.3511 | val_acc 0.8983
No improvement (1/15).
Epoch [18/100] | train_loss 0.1490 | train_acc 0.9457 | val_loss 0.3428 | val_acc 0.8933
No improvement (2/15).
Epoch [19/100] | train_loss 0.1406 | train_acc 0.9467 | val_loss 0.4635 | val_acc 0.8850
Epoch [20/100] | train_loss 0.1409 | train_acc 0.9477 | val_loss 0.3165 | val_acc 0.9100
No improvement (1/15).
Epoch [21/100] | train_loss 0.1299 | train_acc 0.9500 | val_loss 0.3569 | val_acc 0.9083
Epoch [22/100] | train_loss 0.1168 | train_acc 0.9570 | val_loss 0.3244 | val_acc 0.9183
No improvement (1/15).
Epoch [23/100] | train_loss 0.1286 | train_acc 0.9477 | val_loss 0.3105 | val_acc 0.9150
No improvement (2/15).
Epoch [24/100] | train_loss 0.1002 | train_acc 0.9613 | val_loss 0.3963 | val_acc 0.8833
No improvement (3/15).
Epoch [25/100] | train_loss 0.1266 | train_acc 0.9487 | val_loss 0.3509 | val_acc 0.9017
No improvement (4/15).
Epoch [26/100] | train_loss 0.1175 | train_acc 0.9510 | val_loss 0.3725 | val_acc 0.9067
No improvement (5/15).
Epoch [27/100] | train_loss 0.0970 | train_acc 0.9623 | val_loss 0.3441 | val_acc 0.9100
No improvement (6/15).
Epoch [28/100] | train_loss 0.0938 | train_acc 0.9663 | val_loss 0.3551 | val_acc 0.9133
No improvement (7/15).
Epoch [29/100] | train_loss 0.0916 | train_acc 0.9637 | val_loss 0.3697 | val_acc 0.9133
Epoch [30/100] | train_loss 0.0782 | train_acc 0.9683 | val_loss 0.3439 | val_acc 0.9217
No improvement (1/15).
Epoch [31/100] | train_loss 0.0778 | train_acc 0.9687 | val_loss 0.3719 | val_acc 0.9133
No improvement (2/15).
Epoch [32/100] | train_loss 0.0788 | train_acc 0.9713 | val_loss 0.3601 | val_acc 0.9150
No improvement (3/15).
Epoch [33/100] | train_loss 0.0741 | train_acc 0.9720 | val_loss 0.3808 | val_acc 0.9083
No improvement (4/15).
Epoch [34/100] | train_loss 0.0724 | train_acc 0.9723 | val_loss 0.3625 | val_acc 0.9217
No improvement (5/15).
Epoch [35/100] | train_loss 0.0737 | train_acc 0.9720 | val_loss 0.3966 | val_acc 0.9183
Epoch [36/100] | train_loss 0.0675 | train_acc 0.9740 | val_loss 0.3735 | val_acc 0.9250
No improvement (1/15).
Epoch [37/100] | train_loss 0.0614 | train_acc 0.9767 | val_loss 0.3880 | val_acc 0.9200
Epoch [38/100] | train_loss 0.0572 | train_acc 0.9790 | val_loss 0.3780 | val_acc 0.9283
No improvement (1/15).
Epoch [39/100] | train_loss 0.0582 | train_acc 0.9773 | val_loss 0.3844 | val_acc 0.9233
No improvement (2/15).
Epoch [40/100] | train_loss 0.0508 | train_acc 0.9820 | val_loss 0.4068 | val_acc 0.9233
No improvement (3/15).
Epoch [41/100] | train_loss 0.0555 | train_acc 0.9810 | val_loss 0.4147 | val_acc 0.9233
No improvement (4/15).
Epoch [42/100] | train_loss 0.0455 | train_acc 0.9830 | val_loss 0.4342 | val_acc 0.9200
No improvement (5/15).
Epoch [43/100] | train_loss 0.0466 | train_acc 0.9807 | val_loss 0.4250 | val_acc 0.9217
No improvement (6/15).
Epoch [44/100] | train_loss 0.0499 | train_acc 0.9813 | val_loss 0.4347 | val_acc 0.9217
No improvement (7/15).
Epoch [45/100] | train_loss 0.0474 | train_acc 0.9810 | val_loss 0.4256 | val_acc 0.9217
No improvement (8/15).
Epoch [46/100] | train_loss 0.0425 | train_acc 0.9830 | val_loss 0.4324 | val_acc 0.9250
No improvement (9/15).
Epoch [47/100] | train_loss 0.0436 | train_acc 0.9827 | val_loss 0.4449 | val_acc 0.9217
No improvement (10/15).
Epoch [48/100] | train_loss 0.0448 | train_acc 0.9810 | val_loss 0.4322 | val_acc 0.9250
No improvement (11/15).
Epoch [49/100] | train_loss 0.0442 | train_acc 0.9817 | val_loss 0.4425 | val_acc 0.9267
No improvement (12/15).
Epoch [50/100] | train_loss 0.0410 | train_acc 0.9820 | val_loss 0.4541 | val_acc 0.9233
No improvement (13/15).
Epoch [51/100] | train_loss 0.0410 | train_acc 0.9853 | val_loss 0.4448 | val_acc 0.9283
No improvement (14/15).
Epoch [52/100] | train_loss 0.0395 | train_acc 0.9840 | val_loss 0.4621 | val_acc 0.9233
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 90 Finished. Best Val Acc: 92.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6963 | train_acc 0.5523 | val_loss 0.6612 | val_acc 0.5867
Epoch [2/100] | train_loss 0.5816 | train_acc 0.6673 | val_loss 0.4556 | val_acc 0.7700
Epoch [3/100] | train_loss 0.3630 | train_acc 0.8447 | val_loss 0.5452 | val_acc 0.7817
Epoch [4/100] | train_loss 0.2467 | train_acc 0.9027 | val_loss 0.3516 | val_acc 0.8517
Epoch [5/100] | train_loss 0.2308 | train_acc 0.9073 | val_loss 0.2548 | val_acc 0.8833
Epoch [6/100] | train_loss 0.1957 | train_acc 0.9233 | val_loss 0.2357 | val_acc 0.9117
Epoch [7/100] | train_loss 0.1580 | train_acc 0.9417 | val_loss 0.2113 | val_acc 0.9250
Epoch [8/100] | train_loss 0.1050 | train_acc 0.9623 | val_loss 0.1776 | val_acc 0.9350
Epoch [9/100] | train_loss 0.1048 | train_acc 0.9607 | val_loss 0.1777 | val_acc 0.9450
Epoch [10/100] | train_loss 0.1214 | train_acc 0.9527 | val_loss 0.1327 | val_acc 0.9567
Epoch [11/100] | train_loss 0.0991 | train_acc 0.9623 | val_loss 0.1279 | val_acc 0.9667
No improvement (1/15).
Epoch [12/100] | train_loss 0.0944 | train_acc 0.9660 | val_loss 0.1403 | val_acc 0.9583
No improvement (2/15).
Epoch [13/100] | train_loss 0.0982 | train_acc 0.9630 | val_loss 0.1333 | val_acc 0.9517
No improvement (3/15).
Epoch [14/100] | train_loss 0.0945 | train_acc 0.9643 | val_loss 0.1352 | val_acc 0.9567
No improvement (4/15).
Epoch [15/100] | train_loss 0.0911 | train_acc 0.9633 | val_loss 0.1790 | val_acc 0.9250
No improvement (5/15).
Epoch [16/100] | train_loss 0.1049 | train_acc 0.9597 | val_loss 0.1585 | val_acc 0.9350
No improvement (6/15).
Epoch [17/100] | train_loss 0.1081 | train_acc 0.9580 | val_loss 0.1328 | val_acc 0.9583
No improvement (7/15).
Epoch [18/100] | train_loss 0.0859 | train_acc 0.9680 | val_loss 0.1356 | val_acc 0.9600
No improvement (8/15).
Epoch [19/100] | train_loss 0.0834 | train_acc 0.9717 | val_loss 0.1658 | val_acc 0.9433
No improvement (9/15).
Epoch [20/100] | train_loss 0.0862 | train_acc 0.9677 | val_loss 0.2036 | val_acc 0.9383
No improvement (10/15).
Epoch [21/100] | train_loss 0.0862 | train_acc 0.9663 | val_loss 0.1772 | val_acc 0.9467
No improvement (11/15).
Epoch [22/100] | train_loss 0.0732 | train_acc 0.9737 | val_loss 0.1538 | val_acc 0.9533
No improvement (12/15).
Epoch [23/100] | train_loss 0.0718 | train_acc 0.9757 | val_loss 0.1700 | val_acc 0.9433
No improvement (13/15).
Epoch [24/100] | train_loss 0.0677 | train_acc 0.9773 | val_loss 0.1524 | val_acc 0.9550
No improvement (14/15).
Epoch [25/100] | train_loss 0.0669 | train_acc 0.9790 | val_loss 0.1654 | val_acc 0.9433
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 91 Finished. Best Val Acc: 96.67%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6978 | train_acc 0.5693 | val_loss 0.7183 | val_acc 0.5733
Epoch [2/100] | train_loss 0.6472 | train_acc 0.6203 | val_loss 0.5655 | val_acc 0.7083
Epoch [3/100] | train_loss 0.4473 | train_acc 0.7960 | val_loss 0.3317 | val_acc 0.8650
Epoch [4/100] | train_loss 0.2725 | train_acc 0.8817 | val_loss 0.3288 | val_acc 0.8800
Epoch [5/100] | train_loss 0.2088 | train_acc 0.9230 | val_loss 0.2588 | val_acc 0.9100wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 65-66
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–‡â–†â–†â–„â–†â–…â–…â–â–‚â–â–‚â–â–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–†â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–†â–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96
wandb:             epoch 37
wandb:         grad/norm 0.46697
wandb:                lr 1e-05
wandb:         train/acc 0.98367
wandb:        train/loss 0.04493
wandb: training_time_sec 15881.22452
wandb:           val/acc 0.95333
wandb:          val/loss 0.18221
wandb: 
wandb: ğŸš€ View run Trial92_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/cinwloox
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260209_211013-cinwloox/logs
[I 2026-02-10 01:34:56,319] Trial 92 finished with value: 0.96 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0008517196666798049, 'dropout': 0.1001424464875367}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run 9wgfeajl
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_013456-9wgfeajl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial93_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9wgfeajl
wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 48-49
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–…â–„â–ƒâ–‚â–‚â–â–â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96333
wandb:             epoch 28
wandb:         grad/norm 1.0
wandb:                lr 5e-05
wandb:         train/acc 0.98033
wandb:        train/loss 0.05946
wandb: training_time_sec 12143.26768
wandb:           val/acc 0.95667
wandb:          val/loss 0.15194
wandb: 
wandb: ğŸš€ View run Trial93_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9wgfeajl
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_013456-9wgfeajl/logs
[I 2026-02-10 04:57:21,804] Trial 93 finished with value: 0.9633333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0007546713303528312, 'dropout': 0.1879284161425177}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run zqeinygr
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_045722-zqeinygr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial94_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zqeinygr

Epoch [6/100] | train_loss 0.1828 | train_acc 0.9323 | val_loss 0.1814 | val_acc 0.9400
Epoch [7/100] | train_loss 0.1736 | train_acc 0.9293 | val_loss 0.1483 | val_acc 0.9467
No improvement (1/15).
Epoch [8/100] | train_loss 0.1060 | train_acc 0.9620 | val_loss 0.1720 | val_acc 0.9300
Epoch [9/100] | train_loss 0.1115 | train_acc 0.9567 | val_loss 0.1437 | val_acc 0.9567
No improvement (1/15).
Epoch [10/100] | train_loss 0.0944 | train_acc 0.9643 | val_loss 0.1580 | val_acc 0.9517
No improvement (2/15).
Epoch [11/100] | train_loss 0.1061 | train_acc 0.9603 | val_loss 0.1541 | val_acc 0.9500
Epoch [12/100] | train_loss 0.0996 | train_acc 0.9650 | val_loss 0.1518 | val_acc 0.9583
No improvement (1/15).
Epoch [13/100] | train_loss 0.0812 | train_acc 0.9707 | val_loss 0.1624 | val_acc 0.9550
No improvement (2/15).
Epoch [14/100] | train_loss 0.0804 | train_acc 0.9730 | val_loss 0.1655 | val_acc 0.9450
No improvement (3/15).
Epoch [15/100] | train_loss 0.0886 | train_acc 0.9677 | val_loss 0.1963 | val_acc 0.9400
No improvement (4/15).
Epoch [16/100] | train_loss 0.0927 | train_acc 0.9667 | val_loss 0.1535 | val_acc 0.9533
No improvement (5/15).
Epoch [17/100] | train_loss 0.0813 | train_acc 0.9713 | val_loss 0.1775 | val_acc 0.9433
No improvement (6/15).
Epoch [18/100] | train_loss 0.0773 | train_acc 0.9733 | val_loss 0.1661 | val_acc 0.9500
No improvement (7/15).
Epoch [19/100] | train_loss 0.0688 | train_acc 0.9790 | val_loss 0.1659 | val_acc 0.9533
No improvement (8/15).
Epoch [20/100] | train_loss 0.0657 | train_acc 0.9763 | val_loss 0.1892 | val_acc 0.9500
No improvement (9/15).
Epoch [21/100] | train_loss 0.0635 | train_acc 0.9810 | val_loss 0.2119 | val_acc 0.9450
No improvement (10/15).
Epoch [22/100] | train_loss 0.0640 | train_acc 0.9783 | val_loss 0.2356 | val_acc 0.9433
Epoch [23/100] | train_loss 0.0587 | train_acc 0.9807 | val_loss 0.1756 | val_acc 0.9600
No improvement (1/15).
Epoch [24/100] | train_loss 0.0619 | train_acc 0.9817 | val_loss 0.2184 | val_acc 0.9450
No improvement (2/15).
Epoch [25/100] | train_loss 0.0581 | train_acc 0.9817 | val_loss 0.1970 | val_acc 0.9500
No improvement (3/15).
Epoch [26/100] | train_loss 0.0601 | train_acc 0.9763 | val_loss 0.1840 | val_acc 0.9483
No improvement (4/15).
Epoch [27/100] | train_loss 0.0610 | train_acc 0.9753 | val_loss 0.1786 | val_acc 0.9500
No improvement (5/15).
Epoch [28/100] | train_loss 0.0574 | train_acc 0.9793 | val_loss 0.1733 | val_acc 0.9500
No improvement (6/15).
Epoch [29/100] | train_loss 0.0545 | train_acc 0.9817 | val_loss 0.1781 | val_acc 0.9500
No improvement (7/15).
Epoch [30/100] | train_loss 0.0519 | train_acc 0.9817 | val_loss 0.1799 | val_acc 0.9483
No improvement (8/15).
Epoch [31/100] | train_loss 0.0547 | train_acc 0.9810 | val_loss 0.1813 | val_acc 0.9483
No improvement (9/15).
Epoch [32/100] | train_loss 0.0504 | train_acc 0.9830 | val_loss 0.1740 | val_acc 0.9517
No improvement (10/15).
Epoch [33/100] | train_loss 0.0472 | train_acc 0.9840 | val_loss 0.1763 | val_acc 0.9533
No improvement (11/15).
Epoch [34/100] | train_loss 0.0455 | train_acc 0.9843 | val_loss 0.1766 | val_acc 0.9533
No improvement (12/15).
Epoch [35/100] | train_loss 0.0462 | train_acc 0.9840 | val_loss 0.1796 | val_acc 0.9517
No improvement (13/15).
Epoch [36/100] | train_loss 0.0469 | train_acc 0.9850 | val_loss 0.1805 | val_acc 0.9533
No improvement (14/15).
Epoch [37/100] | train_loss 0.0449 | train_acc 0.9837 | val_loss 0.1822 | val_acc 0.9533
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 92 Finished. Best Val Acc: 96.00%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7544 | train_acc 0.5147 | val_loss 0.6851 | val_acc 0.6100
Epoch [2/100] | train_loss 0.6543 | train_acc 0.6263 | val_loss 0.6199 | val_acc 0.6283
Epoch [3/100] | train_loss 0.4518 | train_acc 0.7877 | val_loss 0.4163 | val_acc 0.8533
Epoch [4/100] | train_loss 0.2367 | train_acc 0.9017 | val_loss 0.3616 | val_acc 0.8767
Epoch [5/100] | train_loss 0.1898 | train_acc 0.9247 | val_loss 0.2594 | val_acc 0.9017
Epoch [6/100] | train_loss 0.1657 | train_acc 0.9353 | val_loss 0.1803 | val_acc 0.9350
No improvement (1/15).
Epoch [7/100] | train_loss 0.1632 | train_acc 0.9407 | val_loss 0.2273 | val_acc 0.9083
Epoch [8/100] | train_loss 0.1009 | train_acc 0.9630 | val_loss 0.1434 | val_acc 0.9567
Epoch [9/100] | train_loss 0.0960 | train_acc 0.9647 | val_loss 0.1381 | val_acc 0.9600
No improvement (1/15).
Epoch [10/100] | train_loss 0.0981 | train_acc 0.9643 | val_loss 0.1553 | val_acc 0.9433
No improvement (2/15).
Epoch [11/100] | train_loss 0.1051 | train_acc 0.9617 | val_loss 0.1730 | val_acc 0.9433
No improvement (3/15).
Epoch [12/100] | train_loss 0.0856 | train_acc 0.9693 | val_loss 0.1326 | val_acc 0.9600
No improvement (4/15).
Epoch [13/100] | train_loss 0.0877 | train_acc 0.9733 | val_loss 0.1437 | val_acc 0.9550
Epoch [14/100] | train_loss 0.0882 | train_acc 0.9673 | val_loss 0.1443 | val_acc 0.9633
No improvement (1/15).
Epoch [15/100] | train_loss 0.0769 | train_acc 0.9740 | val_loss 0.2075 | val_acc 0.9367
No improvement (2/15).
Epoch [16/100] | train_loss 0.0807 | train_acc 0.9720 | val_loss 0.1970 | val_acc 0.9333
No improvement (3/15).
Epoch [17/100] | train_loss 0.0777 | train_acc 0.9727 | val_loss 0.2007 | val_acc 0.9367
No improvement (4/15).
Epoch [18/100] | train_loss 0.0760 | train_acc 0.9743 | val_loss 0.2061 | val_acc 0.9333
No improvement (5/15).
Epoch [19/100] | train_loss 0.0721 | train_acc 0.9747 | val_loss 0.1841 | val_acc 0.9417
No improvement (6/15).
Epoch [20/100] | train_loss 0.0712 | train_acc 0.9783 | val_loss 0.1684 | val_acc 0.9467
No improvement (7/15).
Epoch [21/100] | train_loss 0.0629 | train_acc 0.9787 | val_loss 0.1473 | val_acc 0.9567
No improvement (8/15).
Epoch [22/100] | train_loss 0.0620 | train_acc 0.9800 | val_loss 0.1500 | val_acc 0.9567
No improvement (9/15).
Epoch [23/100] | train_loss 0.0632 | train_acc 0.9790 | val_loss 0.1647 | val_acc 0.9483
No improvement (10/15).
Epoch [24/100] | train_loss 0.0646 | train_acc 0.9773 | val_loss 0.2021 | val_acc 0.9317
No improvement (11/15).
Epoch [25/100] | train_loss 0.0736 | train_acc 0.9727 | val_loss 0.1692 | val_acc 0.9417
No improvement (12/15).
Epoch [26/100] | train_loss 0.0610 | train_acc 0.9793 | val_loss 0.1436 | val_acc 0.9617
No improvement (13/15).
Epoch [27/100] | train_loss 0.0593 | train_acc 0.9797 | val_loss 0.1499 | val_acc 0.9550
No improvement (14/15).
Epoch [28/100] | train_loss 0.0595 | train_acc 0.9803 | val_loss 0.1519 | val_acc 0.9567
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 93 Finished. Best Val Acc: 96.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6888 | train_acc 0.5557 | val_loss 0.6833 | val_acc 0.5867
Epoch [2/100] | train_loss 0.6535 | train_acc 0.6020 | val_loss 0.6184 | val_acc 0.6217
Epoch [3/100] | train_loss 0.4866 | train_acc 0.7480 | val_loss 0.3165 | val_acc 0.8733
Epoch [4/100] | train_loss 0.2326 | train_acc 0.9070 | val_loss 0.3230 | val_acc 0.8750
Epoch [5/100] | train_loss 0.2660 | train_acc 0.8907 | val_loss 0.2310 | val_acc 0.9067
No improvement (1/15).
Epoch [6/100] | train_loss 0.2206 | train_acc 0.9140 | val_loss 0.2933 | val_acc 0.8983
Epoch [7/100] | train_loss 0.1786 | train_acc 0.9297 | val_loss 0.1916 | val_acc 0.9267
Epoch [8/100] | train_loss 0.1252 | train_acc 0.9547 | val_loss 0.1686 | val_acc 0.9367
Epoch [9/100] | train_loss 0.1277 | train_acc 0.9507 | val_loss 0.1458 | val_acc 0.9483
Epoch [10/100] | train_loss 0.1049 | train_acc 0.9590 | val_loss 0.1268 | val_acc 0.9583
No improvement (1/15).
Epoch [11/100] | train_loss 0.0975 | train_acc 0.9647 | val_loss 0.1296 | val_acc 0.9583
No improvement (2/15).
Epoch [12/100] | train_loss 0.1021 | train_acc 0.9603 | val_loss 0.1396 | val_acc 0.9567
No improvement (3/15).
Epoch [13/100] | train_loss 0.0991 | train_acc 0.9613 | val_loss 0.1396 | val_acc 0.9550
No improvement (4/15).
Epoch [14/100] | train_loss 0.0857 | train_acc 0.9647 | val_loss 0.1715 | val_acc 0.9467
No improvement (5/15).
Epoch [15/100] | train_loss 0.0887 | train_acc 0.9660 | val_loss 0.1592 | val_acc 0.9533
No improvement (6/15).
Epoch [16/100] | train_loss 0.0742 | train_acc 0.9730 | val_loss 0.1575 | val_acc 0.9467wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–
wandb:  train/acc â–â–‚â–„â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–ˆâ–†â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–‚â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–â–â–â–â–â–‚â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95833
wandb:             epoch 24
wandb:         grad/norm 1.0
wandb:                lr 0.0001
wandb:         train/acc 0.97633
wandb:        train/loss 0.06432
wandb: training_time_sec 10478.57634
wandb:           val/acc 0.93833
wandb:          val/loss 0.19254
wandb: 
wandb: ğŸš€ View run Trial94_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zqeinygr
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_045722-zqeinygr/logs
[I 2026-02-10 07:52:02,352] Trial 94 finished with value: 0.9583333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0007780374268674357, 'dropout': 0.1553345997974263}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run fipxoz8i
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_075202-fipxoz8i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial95_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fipxoz8i
wandb: updating run metadata
wandb: uploading config.yaml
wandb: uploading summary, console lines 87-88
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–ƒâ–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–‡â–†â–„â–„â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95333
wandb:             epoch 49
wandb:         grad/norm 1.0
wandb:                lr 0.0
wandb:         train/acc 0.98167
wandb:        train/loss 0.05533
wandb: training_time_sec 20950.78826
wandb:           val/acc 0.95
wandb:          val/loss 0.17601
wandb: 
wandb: ğŸš€ View run Trial95_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fipxoz8i
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_075202-fipxoz8i/logs
[I 2026-02-10 13:41:15,590] Trial 95 finished with value: 0.9533333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0005520848285581941, 'dropout': 0.18597827762259084}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run 2s0ezp4r
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_134116-2s0ezp4r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial96_[CV-Variation]_L4_H8_D16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2s0ezp4r
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run Trial96_[CV-Variation]_L4_H8_D16 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2s0ezp4r
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_134116-2s0ezp4r/logs
[I 2026-02-10 13:47:57,293] Trial 96 finished with value: 0.0 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 16, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0004374897480968102, 'dropout': 0.11211697668169432}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run d5p9g1vl
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_134757-d5p9g1vl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial97_[CV-Variation]_L4_H2_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d5p9g1vl

No improvement (7/15).
Epoch [17/100] | train_loss 0.0717 | train_acc 0.9753 | val_loss 0.1601 | val_acc 0.9517
No improvement (8/15).
Epoch [18/100] | train_loss 0.0703 | train_acc 0.9770 | val_loss 0.1764 | val_acc 0.9483
No improvement (9/15).
Epoch [19/100] | train_loss 0.0759 | train_acc 0.9737 | val_loss 0.1660 | val_acc 0.9500
No improvement (10/15).
Epoch [20/100] | train_loss 0.0813 | train_acc 0.9693 | val_loss 0.1704 | val_acc 0.9483
No improvement (11/15).
Epoch [21/100] | train_loss 0.0669 | train_acc 0.9770 | val_loss 0.1797 | val_acc 0.9433
No improvement (12/15).
Epoch [22/100] | train_loss 0.0637 | train_acc 0.9770 | val_loss 0.1719 | val_acc 0.9500
No improvement (13/15).
Epoch [23/100] | train_loss 0.0596 | train_acc 0.9810 | val_loss 0.1666 | val_acc 0.9517
No improvement (14/15).
Epoch [24/100] | train_loss 0.0643 | train_acc 0.9763 | val_loss 0.1925 | val_acc 0.9383
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 94 Finished. Best Val Acc: 95.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7097 | train_acc 0.5437 | val_loss 0.6922 | val_acc 0.5533
Epoch [2/100] | train_loss 0.6415 | train_acc 0.6423 | val_loss 0.6087 | val_acc 0.6667
Epoch [3/100] | train_loss 0.4939 | train_acc 0.7493 | val_loss 0.5128 | val_acc 0.7650
Epoch [4/100] | train_loss 0.3326 | train_acc 0.8593 | val_loss 0.4125 | val_acc 0.8317
Epoch [5/100] | train_loss 0.2231 | train_acc 0.9123 | val_loss 0.4194 | val_acc 0.8783
Epoch [6/100] | train_loss 0.1951 | train_acc 0.9247 | val_loss 0.3806 | val_acc 0.9067
Epoch [7/100] | train_loss 0.1804 | train_acc 0.9337 | val_loss 0.1836 | val_acc 0.9333
Epoch [8/100] | train_loss 0.1284 | train_acc 0.9527 | val_loss 0.1964 | val_acc 0.9433
Epoch [9/100] | train_loss 0.1131 | train_acc 0.9593 | val_loss 0.1885 | val_acc 0.9450
No improvement (1/15).
Epoch [10/100] | train_loss 0.1134 | train_acc 0.9597 | val_loss 0.1698 | val_acc 0.9433
No improvement (2/15).
Epoch [11/100] | train_loss 0.1126 | train_acc 0.9593 | val_loss 0.1804 | val_acc 0.9433
No improvement (3/15).
Epoch [12/100] | train_loss 0.1042 | train_acc 0.9637 | val_loss 0.1978 | val_acc 0.9383
No improvement (4/15).
Epoch [13/100] | train_loss 0.1056 | train_acc 0.9603 | val_loss 0.2409 | val_acc 0.9317
No improvement (5/15).
Epoch [14/100] | train_loss 0.1021 | train_acc 0.9613 | val_loss 0.1831 | val_acc 0.9450
No improvement (6/15).
Epoch [15/100] | train_loss 0.0932 | train_acc 0.9657 | val_loss 0.1691 | val_acc 0.9450
No improvement (7/15).
Epoch [16/100] | train_loss 0.0928 | train_acc 0.9670 | val_loss 0.1819 | val_acc 0.9433
No improvement (8/15).
Epoch [17/100] | train_loss 0.0919 | train_acc 0.9673 | val_loss 0.1860 | val_acc 0.9450
No improvement (9/15).
Epoch [18/100] | train_loss 0.0879 | train_acc 0.9687 | val_loss 0.1720 | val_acc 0.9450
Epoch [19/100] | train_loss 0.0888 | train_acc 0.9687 | val_loss 0.1865 | val_acc 0.9483
Epoch [20/100] | train_loss 0.0773 | train_acc 0.9713 | val_loss 0.1669 | val_acc 0.9517
No improvement (1/15).
Epoch [21/100] | train_loss 0.0736 | train_acc 0.9740 | val_loss 0.1674 | val_acc 0.9517
No improvement (2/15).
Epoch [22/100] | train_loss 0.0713 | train_acc 0.9747 | val_loss 0.1756 | val_acc 0.9500
No improvement (3/15).
Epoch [23/100] | train_loss 0.0734 | train_acc 0.9747 | val_loss 0.1792 | val_acc 0.9500
No improvement (4/15).
Epoch [24/100] | train_loss 0.0734 | train_acc 0.9747 | val_loss 0.1940 | val_acc 0.9450
No improvement (5/15).
Epoch [25/100] | train_loss 0.0745 | train_acc 0.9717 | val_loss 0.1969 | val_acc 0.9383
No improvement (6/15).
Epoch [26/100] | train_loss 0.0651 | train_acc 0.9767 | val_loss 0.1728 | val_acc 0.9483
No improvement (7/15).
Epoch [27/100] | train_loss 0.0641 | train_acc 0.9777 | val_loss 0.1738 | val_acc 0.9450
No improvement (8/15).
Epoch [28/100] | train_loss 0.0637 | train_acc 0.9770 | val_loss 0.1771 | val_acc 0.9467
No improvement (9/15).
Epoch [29/100] | train_loss 0.0619 | train_acc 0.9790 | val_loss 0.1823 | val_acc 0.9450
No improvement (10/15).
Epoch [30/100] | train_loss 0.0611 | train_acc 0.9793 | val_loss 0.1796 | val_acc 0.9467
No improvement (11/15).
Epoch [31/100] | train_loss 0.0628 | train_acc 0.9783 | val_loss 0.1883 | val_acc 0.9433
No improvement (12/15).
Epoch [32/100] | train_loss 0.0609 | train_acc 0.9783 | val_loss 0.1715 | val_acc 0.9517
No improvement (13/15).
Epoch [33/100] | train_loss 0.0602 | train_acc 0.9813 | val_loss 0.1727 | val_acc 0.9517
No improvement (14/15).
Epoch [34/100] | train_loss 0.0602 | train_acc 0.9793 | val_loss 0.1767 | val_acc 0.9500
Epoch [35/100] | train_loss 0.0593 | train_acc 0.9803 | val_loss 0.1753 | val_acc 0.9533
No improvement (1/15).
Epoch [36/100] | train_loss 0.0607 | train_acc 0.9803 | val_loss 0.1779 | val_acc 0.9517
No improvement (2/15).
Epoch [37/100] | train_loss 0.0586 | train_acc 0.9807 | val_loss 0.1775 | val_acc 0.9517
No improvement (3/15).
Epoch [38/100] | train_loss 0.0604 | train_acc 0.9793 | val_loss 0.1676 | val_acc 0.9500
No improvement (4/15).
Epoch [39/100] | train_loss 0.0586 | train_acc 0.9807 | val_loss 0.1708 | val_acc 0.9500
No improvement (5/15).
Epoch [40/100] | train_loss 0.0576 | train_acc 0.9810 | val_loss 0.1722 | val_acc 0.9500
No improvement (6/15).
Epoch [41/100] | train_loss 0.0610 | train_acc 0.9803 | val_loss 0.1717 | val_acc 0.9500
No improvement (7/15).
Epoch [42/100] | train_loss 0.0581 | train_acc 0.9817 | val_loss 0.1730 | val_acc 0.9500
No improvement (8/15).
Epoch [43/100] | train_loss 0.0583 | train_acc 0.9803 | val_loss 0.1741 | val_acc 0.9500
No improvement (9/15).
Epoch [44/100] | train_loss 0.0565 | train_acc 0.9817 | val_loss 0.1745 | val_acc 0.9500
No improvement (10/15).
Epoch [45/100] | train_loss 0.0569 | train_acc 0.9807 | val_loss 0.1747 | val_acc 0.9500
No improvement (11/15).
Epoch [46/100] | train_loss 0.0558 | train_acc 0.9823 | val_loss 0.1749 | val_acc 0.9500
No improvement (12/15).
Epoch [47/100] | train_loss 0.0566 | train_acc 0.9817 | val_loss 0.1755 | val_acc 0.9500
No improvement (13/15).
Epoch [48/100] | train_loss 0.0565 | train_acc 0.9813 | val_loss 0.1755 | val_acc 0.9500
No improvement (14/15).
Epoch [49/100] | train_loss 0.0553 | train_acc 0.9817 | val_loss 0.1760 | val_acc 0.9500
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 95 Finished. Best Val Acc: 95.33%
ğŸ§¹ Memory Cleared
Starting training...
âŒ Trial 96 Failed: CUDA out of memory. Tried to allocate 47.74 GiB. GPU 0 has a total capacity of 7.78 GiB of which 7.31 GiB is free. Including non-PyTorch memory, this process has 278.00 MiB memory in use. Of the allocated memory 118.96 MiB is allocated by PyTorch, and 3.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.6771 | train_acc 0.5860 | val_loss 0.6376 | val_acc 0.6483
Epoch [2/100] | train_loss 0.5284 | train_acc 0.7380 | val_loss 0.4692 | val_acc 0.7650
Epoch [3/100] | train_loss 0.4333 | train_acc 0.8070 | val_loss 0.4195 | val_acc 0.8217
Epoch [4/100] | train_loss 0.2817 | train_acc 0.8883 | val_loss 0.3127 | val_acc 0.8750
Epoch [5/100] | train_loss 0.1810 | train_acc 0.9347 | val_loss 0.1700 | val_acc 0.9317
Epoch [6/100] | train_loss 0.1568 | train_acc 0.9427 | val_loss 0.1962 | val_acc 0.9350
No improvement (1/15).
Epoch [7/100] | train_loss 0.2169 | train_acc 0.9187 | val_loss 0.2274 | val_acc 0.9200
Epoch [8/100] | train_loss 0.1353 | train_acc 0.9493 | val_loss 0.1656 | val_acc 0.9367
Epoch [9/100] | train_loss 0.1240 | train_acc 0.9523 | val_loss 0.1452 | val_acc 0.9517
No improvement (1/15).
Epoch [10/100] | train_loss 0.1096 | train_acc 0.9617 | val_loss 0.1411 | val_acc 0.9500
No improvement (2/15).
Epoch [11/100] | train_loss 0.1135 | train_acc 0.9590 | val_loss 0.1600 | val_acc 0.9467
No improvement (3/15).
Epoch [12/100] | train_loss 0.1076 | train_acc 0.9613 | val_loss 0.1579 | val_acc 0.9500
No improvement (4/15).
Epoch [13/100] | train_loss 0.1077 | train_acc 0.9620 | val_loss 0.1511 | val_acc 0.9517wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–„â–„â–
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:  train/acc â–â–„â–…â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–†â–…â–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–„â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–†â–…â–ƒâ–â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.95833
wandb:             epoch 31
wandb:         grad/norm 0.51713
wandb:                lr 3e-05
wandb:         train/acc 0.981
wandb:        train/loss 0.05537
wandb: training_time_sec 4061.25641
wandb:           val/acc 0.95167
wandb:          val/loss 0.22648
wandb: 
wandb: ğŸš€ View run Trial97_[CV-Variation]_L4_H2_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d5p9g1vl
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_134757-d5p9g1vl/logs
[I 2026-02-10 14:55:41,076] Trial 97 finished with value: 0.9583333333333334 and parameters: {'nhead': 2, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0009015764999525101, 'dropout': 0.06178039849068542}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run axict513
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_145541-axict513
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial98_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/axict513
wandb: updating run metadata
wandb: uploading wandb-summary.json
wandb: uploading summary, console lines 39-40
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–„â–ˆâ–ˆâ–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–ˆâ–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–…â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆ
wandb:   val/loss â–ˆâ–…â–…â–„â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96333
wandb:             epoch 23
wandb:         grad/norm 1.0
wandb:                lr 0.00013
wandb:         train/acc 0.97133
wandb:        train/loss 0.07768
wandb: training_time_sec 10032.12017
wandb:           val/acc 0.94667
wandb:          val/loss 0.193
wandb: 
wandb: ğŸš€ View run Trial98_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/axict513
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_145541-axict513/logs
[I 2026-02-10 17:42:56,266] Trial 98 finished with value: 0.9633333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0010279453524987354, 'dropout': 0.19958315850566113}. Best is trial 91 with value: 0.9666666666666667.
wandb: setting up run x8aoh9bs
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ianyang/SSA/experiments/EXP-26-IY018/wandb/run-20260210_174256-x8aoh9bs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Trial99_[CV-Variation]_L4_H8_D64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/x8aoh9bs

No improvement (5/15).
Epoch [14/100] | train_loss 0.0866 | train_acc 0.9670 | val_loss 0.1597 | val_acc 0.9500
No improvement (6/15).
Epoch [15/100] | train_loss 0.1052 | train_acc 0.9613 | val_loss 0.1979 | val_acc 0.9367
No improvement (7/15).
Epoch [16/100] | train_loss 0.1007 | train_acc 0.9627 | val_loss 0.1737 | val_acc 0.9483
Epoch [17/100] | train_loss 0.0867 | train_acc 0.9677 | val_loss 0.1664 | val_acc 0.9583
No improvement (1/15).
Epoch [18/100] | train_loss 0.0928 | train_acc 0.9657 | val_loss 0.2071 | val_acc 0.9417
No improvement (2/15).
Epoch [19/100] | train_loss 0.1065 | train_acc 0.9613 | val_loss 0.1819 | val_acc 0.9483
No improvement (3/15).
Epoch [20/100] | train_loss 0.0736 | train_acc 0.9767 | val_loss 0.2241 | val_acc 0.9383
No improvement (4/15).
Epoch [21/100] | train_loss 0.0699 | train_acc 0.9750 | val_loss 0.2149 | val_acc 0.9400
No improvement (5/15).
Epoch [22/100] | train_loss 0.0655 | train_acc 0.9760 | val_loss 0.1897 | val_acc 0.9500
No improvement (6/15).
Epoch [23/100] | train_loss 0.0601 | train_acc 0.9783 | val_loss 0.1974 | val_acc 0.9517
No improvement (7/15).
Epoch [24/100] | train_loss 0.0590 | train_acc 0.9793 | val_loss 0.2066 | val_acc 0.9500
No improvement (8/15).
Epoch [25/100] | train_loss 0.0569 | train_acc 0.9820 | val_loss 0.2036 | val_acc 0.9517
No improvement (9/15).
Epoch [26/100] | train_loss 0.0634 | train_acc 0.9777 | val_loss 0.2054 | val_acc 0.9500
No improvement (10/15).
Epoch [27/100] | train_loss 0.0614 | train_acc 0.9767 | val_loss 0.2139 | val_acc 0.9483
No improvement (11/15).
Epoch [28/100] | train_loss 0.0602 | train_acc 0.9797 | val_loss 0.2126 | val_acc 0.9483
No improvement (12/15).
Epoch [29/100] | train_loss 0.0588 | train_acc 0.9797 | val_loss 0.2186 | val_acc 0.9500
No improvement (13/15).
Epoch [30/100] | train_loss 0.0567 | train_acc 0.9813 | val_loss 0.2211 | val_acc 0.9483
No improvement (14/15).
Epoch [31/100] | train_loss 0.0554 | train_acc 0.9810 | val_loss 0.2265 | val_acc 0.9517
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 97 Finished. Best Val Acc: 95.83%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7263 | train_acc 0.5340 | val_loss 0.6819 | val_acc 0.5983
Epoch [2/100] | train_loss 0.5914 | train_acc 0.6603 | val_loss 0.4351 | val_acc 0.8267
Epoch [3/100] | train_loss 0.2466 | train_acc 0.8983 | val_loss 0.4681 | val_acc 0.8467
Epoch [4/100] | train_loss 0.2261 | train_acc 0.9093 | val_loss 0.3423 | val_acc 0.8533
Epoch [5/100] | train_loss 0.1710 | train_acc 0.9387 | val_loss 0.2115 | val_acc 0.9133
Epoch [6/100] | train_loss 0.1313 | train_acc 0.9513 | val_loss 0.1879 | val_acc 0.9333
Epoch [7/100] | train_loss 0.1050 | train_acc 0.9623 | val_loss 0.1278 | val_acc 0.9517
No improvement (1/15).
Epoch [8/100] | train_loss 0.0894 | train_acc 0.9673 | val_loss 0.1652 | val_acc 0.9500
Epoch [9/100] | train_loss 0.0955 | train_acc 0.9640 | val_loss 0.1352 | val_acc 0.9633
No improvement (1/15).
Epoch [10/100] | train_loss 0.0894 | train_acc 0.9687 | val_loss 0.1427 | val_acc 0.9567
No improvement (2/15).
Epoch [11/100] | train_loss 0.0786 | train_acc 0.9733 | val_loss 0.1457 | val_acc 0.9550
No improvement (3/15).
Epoch [12/100] | train_loss 0.0863 | train_acc 0.9667 | val_loss 0.2049 | val_acc 0.9350
No improvement (4/15).
Epoch [13/100] | train_loss 0.0855 | train_acc 0.9673 | val_loss 0.1332 | val_acc 0.9583
No improvement (5/15).
Epoch [14/100] | train_loss 0.0655 | train_acc 0.9773 | val_loss 0.1630 | val_acc 0.9533
No improvement (6/15).
Epoch [15/100] | train_loss 0.0854 | train_acc 0.9700 | val_loss 0.2115 | val_acc 0.9367
No improvement (7/15).
Epoch [16/100] | train_loss 0.0775 | train_acc 0.9737 | val_loss 0.2423 | val_acc 0.9250
No improvement (8/15).
Epoch [17/100] | train_loss 0.0767 | train_acc 0.9750 | val_loss 0.2167 | val_acc 0.9383
No improvement (9/15).
Epoch [18/100] | train_loss 0.0722 | train_acc 0.9753 | val_loss 0.2136 | val_acc 0.9400
No improvement (10/15).
Epoch [19/100] | train_loss 0.0879 | train_acc 0.9670 | val_loss 0.1808 | val_acc 0.9467
No improvement (11/15).
Epoch [20/100] | train_loss 0.0650 | train_acc 0.9760 | val_loss 0.2103 | val_acc 0.9333
No improvement (12/15).
Epoch [21/100] | train_loss 0.0775 | train_acc 0.9703 | val_loss 0.2141 | val_acc 0.9333
No improvement (13/15).
Epoch [22/100] | train_loss 0.0849 | train_acc 0.9693 | val_loss 0.1852 | val_acc 0.9483
No improvement (14/15).
Epoch [23/100] | train_loss 0.0777 | train_acc 0.9713 | val_loss 0.1930 | val_acc 0.9467
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 98 Finished. Best Val Acc: 96.33%
ğŸ§¹ Memory Cleared
Starting training...
Epoch [1/100] | train_loss 0.7138 | train_acc 0.5373 | val_loss 0.6656 | val_acc 0.5800
Epoch [2/100] | train_loss 0.6252 | train_acc 0.6417 | val_loss 0.5300 | val_acc 0.7250
Epoch [3/100] | train_loss 0.4150 | train_acc 0.8197 | val_loss 0.3485 | val_acc 0.8800
Epoch [4/100] | train_loss 0.2625 | train_acc 0.9057 | val_loss 0.1969 | val_acc 0.9183
Epoch [5/100] | train_loss 0.1616 | train_acc 0.9373 | val_loss 0.1663 | val_acc 0.9400
No improvement (1/15).
Epoch [6/100] | train_loss 0.2057 | train_acc 0.9180 | val_loss 0.1970 | val_acc 0.9317
No improvement (2/15).
Epoch [7/100] | train_loss 0.1797 | train_acc 0.9350 | val_loss 0.4933 | val_acc 0.8133
No improvement (3/15).
Epoch [8/100] | train_loss 0.1204 | train_acc 0.9563 | val_loss 0.2259 | val_acc 0.9333
No improvement (4/15).
Epoch [9/100] | train_loss 0.1201 | train_acc 0.9560 | val_loss 0.1696 | val_acc 0.9367
Epoch [10/100] | train_loss 0.1043 | train_acc 0.9613 | val_loss 0.1603 | val_acc 0.9450
Epoch [11/100] | train_loss 0.0849 | train_acc 0.9697 | val_loss 0.1614 | val_acc 0.9467
Epoch [12/100] | train_loss 0.0785 | train_acc 0.9737 | val_loss 0.1727 | val_acc 0.9533
No improvement (1/15).
Epoch [13/100] | train_loss 0.0794 | train_acc 0.9677 | val_loss 0.1728 | val_acc 0.9467
No improvement (2/15).
Epoch [14/100] | train_loss 0.0745 | train_acc 0.9747 | val_loss 0.1514 | val_acc 0.9450
No improvement (3/15).
Epoch [15/100] | train_loss 0.0749 | train_acc 0.9740 | val_loss 0.1310 | val_acc 0.9533
Epoch [16/100] | train_loss 0.0672 | train_acc 0.9767 | val_loss 0.1609 | val_acc 0.9550
No improvement (1/15).
Epoch [17/100] | train_loss 0.0688 | train_acc 0.9787 | val_loss 0.1851 | val_acc 0.9400
No improvement (2/15).
Epoch [18/100] | train_loss 0.0617 | train_acc 0.9790 | val_loss 0.1702 | val_acc 0.9550
No improvement (3/15).
Epoch [19/100] | train_loss 0.0633 | train_acc 0.9777 | val_loss 0.1701 | val_acc 0.9517
No improvement (4/15).
Epoch [20/100] | train_loss 0.0787 | train_acc 0.9713 | val_loss 0.2410 | val_acc 0.9350
Epoch [21/100] | train_loss 0.0463 | train_acc 0.9840 | val_loss 0.1405 | val_acc 0.9633
No improvement (1/15).
Epoch [22/100] | train_loss 0.0755 | train_acc 0.9730 | val_loss 0.2145 | val_acc 0.9450
No improvement (2/15).
Epoch [23/100] | train_loss 0.0425 | train_acc 0.9877 | val_loss 0.1445 | val_acc 0.9550
No improvement (3/15).
Epoch [24/100] | train_loss 0.0772 | train_acc 0.9723 | val_loss 0.2410 | val_acc 0.9400
No improvement (4/15).
Epoch [25/100] | train_loss 0.0405 | train_acc 0.9873 | val_loss 0.1591 | val_acc 0.9517
No improvement (5/15).
Epoch [26/100] | train_loss 0.0542 | train_acc 0.9807 | val_loss 0.2736 | val_acc 0.9400
No improvement (6/15).
Epoch [27/100] | train_loss 0.0532 | train_acc 0.9800 | val_loss 0.2256 | val_acc 0.9483
No improvement (7/15).
Epoch [28/100] | train_loss 0.0481 | train_acc 0.9827 | val_loss 0.2338 | val_acc 0.9400
No improvement (8/15).
Epoch [29/100] | train_loss 0.0467 | train_acc 0.9837 | val_loss 0.2383 | val_acc 0.9383
No improvement (9/15).
Epoch [30/100] | train_loss 0.0537 | train_acc 0.9800 | val_loss 0.2163 | val_acc 0.9483
No improvement (10/15).
Epoch [31/100] | train_loss 0.0460 | train_acc 0.9843 | val_loss 0.2441 | val_acc 0.9383
No improvement (11/15).
Epoch [32/100] | train_loss 0.0397 | train_acc 0.9877 | val_loss 0.1828 | val_acc 0.9467
No improvement (12/15).
Epoch [33/100] | train_loss 0.0426 | train_acc 0.9873 | val_loss 0.1898 | val_acc 0.9500
No improvement (13/15).
Epoch [34/100] | train_loss 0.0363 | train_acc 0.9890 | val_loss 0.1816 | val_acc 0.9467wandb: updating run metadata
wandb: uploading output.log
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:  grad/norm â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–ˆâ–ƒ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train/acc â–â–ƒâ–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train/loss â–ˆâ–‡â–…â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val/acc â–â–„â–†â–‡â–ˆâ–‡â–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:   val/loss â–ˆâ–†â–„â–‚â–â–‚â–†â–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:      best_val_acc 0.96333
wandb:             epoch 35
wandb:         grad/norm 0.45069
wandb:                lr 3e-05
wandb:         train/acc 0.989
wandb:        train/loss 0.03884
wandb: training_time_sec 15127.53543
wandb:           val/acc 0.94667
wandb:          val/loss 0.18535
wandb: 
wandb: ğŸš€ View run Trial99_[CV-Variation]_L4_H8_D64 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/x8aoh9bs
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260210_174256-x8aoh9bs/logs
[I 2026-02-10 21:55:05,976] Trial 99 finished with value: 0.9633333333333334 and parameters: {'nhead': 8, 'num_layers': 4, 'd_model': 64, 'batch_size': 64, 'use_conv1d': False, 'lr': 0.0010770527420867217, 'dropout': 0.21363685486765255}. Best is trial 91 with value: 0.9666666666666667.

No improvement (14/15).
Epoch [35/100] | train_loss 0.0388 | train_acc 0.9890 | val_loss 0.1854 | val_acc 0.9467
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
âœ… Trial 99 Finished. Best Val Acc: 96.33%
ğŸ§¹ Memory Cleared

==========================================
ğŸ† Sweep Complete.
Best Trial: 91
Best Value: 0.9666666666666667
