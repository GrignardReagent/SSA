wandb: Agent Starting Run: ovyjl4t3 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.283724312464949
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006347924733956054
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Currently logged in as: grignardreagent (grignard-reagent) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260127_235515-ovyjl4t3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-sweep-1
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ovyjl4t3
wandb: updating run metadata
wandb: uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–â–â–â–â–
wandb:  train_acc â–â–ƒâ–„â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train_loss â–ˆâ–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:    val_acc â–â–‚â–…â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:   val_loss â–ˆâ–ˆâ–„â–ƒâ–â–â–â–â–…â–…â–ˆâ–†â–†â–‡â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:      epoch 20
wandb:         lr 0.00159
wandb:  train_acc 0.86333
wandb: train_loss 0.33287
wandb:    val_acc 0.76333
wandb:   val_loss 0.55837
wandb: 
wandb: ğŸš€ View run [Baseline] L2-H8-D64-drop0.283724312464949 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ovyjl4t3
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260127_235515-ovyjl4t3/logs
wandb: Agent Starting Run: 6iskrhpr with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.410198688699405
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.002742913371992445
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_003954-6iskrhpr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run scarlet-sweep-2
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/6iskrhpr
wandb: uploading data; updating run metadata
wandb: uploading output.log; uploading wandb-summary.json
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:  train_acc â–â–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ
wandb: train_loss â–ˆâ–†â–…â–…â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:    val_acc â–‚â–â–„â–‚â–…â–…â–†â–„â–ˆâ–ˆâ–‡â–‡â–ˆâ–…â–‡â–‡â–‡â–‡â–‡â–†â–†â–‡â–†
wandb:   val_loss â–†â–ˆâ–‡â–„â–‚â–ƒâ–‚â–…â–â–â–â–ƒâ–â–„â–‚â–â–â–„â–ƒâ–ƒâ–…â–‚â–…
wandb: 
wandb: Run summary:
wandb:      epoch 23
wandb:         lr 0.00034
wandb:  train_acc 0.835
wandb: train_loss 0.37565
wandb:    val_acc 0.67833
wandb:   val_loss 0.72084
wandb: 
wandb: ğŸš€ View run [Baseline] L3-H2-D128-drop0.410198688699405 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/6iskrhpr
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_003954-6iskrhpr/logs
wandb: Agent Starting Run: fe1blo70 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.157781503261739
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0029763225018562393
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_011629-fe1blo70
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glamorous-sweep-3
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fe1blo70
wandb: updating run metadata
wandb: uploading history steps 0-27, summary, console lines 50-53
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:  train_acc â–â–ƒâ–ƒâ–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train_loss â–ˆâ–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:    val_acc â–â–ƒâ–…â–…â–…â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb:   val_loss â–ˆâ–†â–…â–„â–†â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–…â–…
wandb: 
wandb: Run summary:
wandb:      epoch 28
wandb:         lr 0.00019
wandb:  train_acc 0.87433
wandb: train_loss 0.30358
wandb:    val_acc 0.74
wandb:   val_loss 0.61005
wandb: 
wandb: ğŸš€ View run [Baseline] L1-H2-D128-drop0.157781503261739 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/fe1blo70
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_011629-fe1blo70/logs
wandb: Agent Starting Run: 4atimkxc with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.4527145545847131
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005070068409120767
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_013131-4atimkxc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sunny-sweep-4
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4atimkxc
Create sweep with ID: ui8nb3kf
Sweep URL: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
Loading data for experiment: Baseline
ğŸ“‚ Loading static data from ../EXP-25-IY011/data/IY011_static_train.pt...
ğŸ“‚ Loading static data from ../EXP-25-IY011/data/IY011_static_val.pt...
Starting training...
Epoch [1/100] | train_loss 0.7396 | train_acc 0.5327 | val_loss 0.6767 | val_acc 0.5733
Epoch [2/100] | train_loss 0.6441 | train_acc 0.6200 | val_loss 0.6779 | val_acc 0.5983
Epoch [3/100] | train_loss 0.5865 | train_acc 0.6893 | val_loss 0.6040 | val_acc 0.6817
Epoch [4/100] | train_loss 0.5049 | train_acc 0.7623 | val_loss 0.5656 | val_acc 0.7300
Epoch [5/100] | train_loss 0.4707 | train_acc 0.7823 | val_loss 0.5371 | val_acc 0.7567
Epoch [6/100] | train_loss 0.4592 | train_acc 0.7850 | val_loss 0.5316 | val_acc 0.7733
No improvement (1/15).
Epoch [7/100] | train_loss 0.4402 | train_acc 0.8007 | val_loss 0.5319 | val_acc 0.7433
No improvement (2/15).
Epoch [8/100] | train_loss 0.4306 | train_acc 0.8087 | val_loss 0.5307 | val_acc 0.7633
No improvement (3/15).
Epoch [9/100] | train_loss 0.3845 | train_acc 0.8333 | val_loss 0.6073 | val_acc 0.7733
No improvement (4/15).
Epoch [10/100] | train_loss 0.3807 | train_acc 0.8380 | val_loss 0.6109 | val_acc 0.7633
No improvement (5/15).
Epoch [11/100] | train_loss 0.3703 | train_acc 0.8373 | val_loss 0.6724 | val_acc 0.7650
No improvement (6/15).
Epoch [12/100] | train_loss 0.3676 | train_acc 0.8453 | val_loss 0.6269 | val_acc 0.7650
No improvement (7/15).
Epoch [13/100] | train_loss 0.3628 | train_acc 0.8473 | val_loss 0.6292 | val_acc 0.7600
No improvement (8/15).
Epoch [14/100] | train_loss 0.3526 | train_acc 0.8497 | val_loss 0.6470 | val_acc 0.7667
No improvement (9/15).
Epoch [15/100] | train_loss 0.3389 | train_acc 0.8560 | val_loss 0.5810 | val_acc 0.7533
No improvement (10/15).
Epoch [16/100] | train_loss 0.3318 | train_acc 0.8590 | val_loss 0.5630 | val_acc 0.7467
No improvement (11/15).
Epoch [17/100] | train_loss 0.3277 | train_acc 0.8660 | val_loss 0.5604 | val_acc 0.7450
No improvement (12/15).
Epoch [18/100] | train_loss 0.3316 | train_acc 0.8630 | val_loss 0.5777 | val_acc 0.7400
No improvement (13/15).
Epoch [19/100] | train_loss 0.3311 | train_acc 0.8637 | val_loss 0.5510 | val_acc 0.7467
No improvement (14/15).
Epoch [20/100] | train_loss 0.3329 | train_acc 0.8633 | val_loss 0.5584 | val_acc 0.7633
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 77.33%
Starting training...
Epoch [1/100] | train_loss 0.8322 | train_acc 0.5283 | val_loss 0.7276 | val_acc 0.5300
No improvement (1/15).
Epoch [2/100] | train_loss 0.6692 | train_acc 0.5953 | val_loss 0.7981 | val_acc 0.5050
Epoch [3/100] | train_loss 0.6191 | train_acc 0.6443 | val_loss 0.7644 | val_acc 0.6050
No improvement (1/15).
Epoch [4/100] | train_loss 0.6051 | train_acc 0.6577 | val_loss 0.6824 | val_acc 0.5283
Epoch [5/100] | train_loss 0.6031 | train_acc 0.6753 | val_loss 0.6381 | val_acc 0.6267
Epoch [6/100] | train_loss 0.5822 | train_acc 0.6977 | val_loss 0.6505 | val_acc 0.6467
Epoch [7/100] | train_loss 0.5856 | train_acc 0.6970 | val_loss 0.6196 | val_acc 0.6567
No improvement (1/15).
Epoch [8/100] | train_loss 0.5697 | train_acc 0.7153 | val_loss 0.7139 | val_acc 0.5900
Epoch [9/100] | train_loss 0.5248 | train_acc 0.7273 | val_loss 0.5991 | val_acc 0.7267
No improvement (1/15).
Epoch [10/100] | train_loss 0.4959 | train_acc 0.7553 | val_loss 0.5971 | val_acc 0.7167
No improvement (2/15).
Epoch [11/100] | train_loss 0.4860 | train_acc 0.7613 | val_loss 0.6029 | val_acc 0.7017
No improvement (3/15).
Epoch [12/100] | train_loss 0.4712 | train_acc 0.7697 | val_loss 0.6501 | val_acc 0.7083
No improvement (4/15).
Epoch [13/100] | train_loss 0.4690 | train_acc 0.7740 | val_loss 0.5964 | val_acc 0.7183
No improvement (5/15).
Epoch [14/100] | train_loss 0.4592 | train_acc 0.7840 | val_loss 0.6730 | val_acc 0.6400
No improvement (6/15).
Epoch [15/100] | train_loss 0.4584 | train_acc 0.7883 | val_loss 0.6201 | val_acc 0.6983
No improvement (7/15).
Epoch [16/100] | train_loss 0.4440 | train_acc 0.8020 | val_loss 0.5968 | val_acc 0.7100
No improvement (8/15).
Epoch [17/100] | train_loss 0.4313 | train_acc 0.7977 | val_loss 0.5946 | val_acc 0.6950
No improvement (9/15).
Epoch [18/100] | train_loss 0.4207 | train_acc 0.8097 | val_loss 0.6890 | val_acc 0.6967
No improvement (10/15).
Epoch [19/100] | train_loss 0.4164 | train_acc 0.8140 | val_loss 0.6554 | val_acc 0.6950
No improvement (11/15).
Epoch [20/100] | train_loss 0.4124 | train_acc 0.8193 | val_loss 0.6650 | val_acc 0.6783
No improvement (12/15).
Epoch [21/100] | train_loss 0.3941 | train_acc 0.8283 | val_loss 0.7010 | val_acc 0.6767
No improvement (13/15).
Epoch [22/100] | train_loss 0.3783 | train_acc 0.8363 | val_loss 0.6352 | val_acc 0.7083
No improvement (14/15).
Epoch [23/100] | train_loss 0.3757 | train_acc 0.8350 | val_loss 0.7208 | val_acc 0.6783
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 72.67%
Starting training...
Epoch [1/100] | train_loss 0.7417 | train_acc 0.5250 | val_loss 0.6692 | val_acc 0.5800
Epoch [2/100] | train_loss 0.6475 | train_acc 0.6143 | val_loss 0.6322 | val_acc 0.6417
Epoch [3/100] | train_loss 0.6157 | train_acc 0.6373 | val_loss 0.5922 | val_acc 0.6967
Epoch [4/100] | train_loss 0.5412 | train_acc 0.7150 | val_loss 0.5751 | val_acc 0.7000
No improvement (1/15).
Epoch [5/100] | train_loss 0.4952 | train_acc 0.7510 | val_loss 0.6263 | val_acc 0.6983
Epoch [6/100] | train_loss 0.4649 | train_acc 0.7800 | val_loss 0.5227 | val_acc 0.7433
Epoch [7/100] | train_loss 0.4433 | train_acc 0.7987 | val_loss 0.5118 | val_acc 0.7617
No improvement (1/15).
Epoch [8/100] | train_loss 0.4039 | train_acc 0.8227 | val_loss 0.5450 | val_acc 0.7433
No improvement (2/15).
Epoch [9/100] | train_loss 0.3894 | train_acc 0.8283 | val_loss 0.5454 | val_acc 0.7467
No improvement (3/15).
Epoch [10/100] | train_loss 0.3833 | train_acc 0.8320 | val_loss 0.5446 | val_acc 0.7517
No improvement (4/15).
Epoch [11/100] | train_loss 0.3772 | train_acc 0.8367 | val_loss 0.5398 | val_acc 0.7550
No improvement (5/15).
Epoch [12/100] | train_loss 0.3732 | train_acc 0.8343 | val_loss 0.5365 | val_acc 0.7583
No improvement (6/15).
Epoch [13/100] | train_loss 0.3693 | train_acc 0.8347 | val_loss 0.5537 | val_acc 0.7617
Epoch [14/100] | train_loss 0.3603 | train_acc 0.8430 | val_loss 0.5637 | val_acc 0.7733
No improvement (1/15).
Epoch [15/100] | train_loss 0.3676 | train_acc 0.8440 | val_loss 0.5328 | val_acc 0.7500
No improvement (2/15).
Epoch [16/100] | train_loss 0.3550 | train_acc 0.8497 | val_loss 0.5392 | val_acc 0.7583
No improvement (3/15).
Epoch [17/100] | train_loss 0.3538 | train_acc 0.8473 | val_loss 0.5432 | val_acc 0.7600
No improvement (4/15).
Epoch [18/100] | train_loss 0.3507 | train_acc 0.8497 | val_loss 0.5474 | val_acc 0.7600
No improvement (5/15).
Epoch [19/100] | train_loss 0.3472 | train_acc 0.8510 | val_loss 0.5524 | val_acc 0.7583
No improvement (6/15).
Epoch [20/100] | train_loss 0.3292 | train_acc 0.8567 | val_loss 0.5680 | val_acc 0.7533
No improvement (7/15).
Epoch [21/100] | train_loss 0.3262 | train_acc 0.8577 | val_loss 0.5797 | val_acc 0.7517
No improvement (8/15).
Epoch [22/100] | train_loss 0.3238 | train_acc 0.8597 | val_loss 0.5890 | val_acc 0.7433
No improvement (9/15).
Epoch [23/100] | train_loss 0.3213 | train_acc 0.8613 | val_loss 0.5953 | val_acc 0.7417
No improvement (10/15).
Epoch [24/100] | train_loss 0.3189 | train_acc 0.8640 | val_loss 0.6010 | val_acc 0.7433
No improvement (11/15).
Epoch [25/100] | train_loss 0.3166 | train_acc 0.8640 | val_loss 0.6078 | val_acc 0.7433
No improvement (12/15).
Epoch [26/100] | train_loss 0.3076 | train_acc 0.8713 | val_loss 0.6005 | val_acc 0.7433
No improvement (13/15).
Epoch [27/100] | train_loss 0.3052 | train_acc 0.8720 | val_loss 0.6057 | val_acc 0.7417
No improvement (14/15).
Epoch [28/100] | train_loss 0.3036 | train_acc 0.8743 | val_loss 0.6100 | val_acc 0.7400
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 77.33%
Starting training...wandb: updating run metadata
wandb: uploading output.log; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_acc â–â–‚â–ƒâ–ƒâ–„â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train_loss â–ˆâ–ˆâ–‡â–†â–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    val_acc â–‚â–ƒâ–‚â–ƒâ–â–…â–†â–…â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡
wandb:   val_loss â–‚â–‚â–‚â–‚â–‚â–â–â–‡â–ƒâ–…â–…â–„â–…â–…â–â–ƒâ–ƒâ–„â–†â–„â–ƒâ–„â–„â–…â–…â–†â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:      epoch 36
wandb:         lr 0.00032
wandb:  train_acc 0.87133
wandb: train_loss 0.32493
wandb:    val_acc 0.73
wandb:   val_loss 0.85217
wandb: 
wandb: ğŸš€ View run [Baseline] L2-H2-D32-drop0.4527145545847131 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4atimkxc
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_013131-4atimkxc/logs
wandb: Agent Starting Run: d9kydw8v with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.30015934517527193
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003023302317410163
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run d9kydw8v
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_015239-d9kydw8v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-sweep-5
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d9kydw8v
wandb: updating run metadata
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: 
wandb: Run history:
wandb:      epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:         lr â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_acc â–â–ƒâ–„â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: train_loss â–ˆâ–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:    val_acc â–â–†â–‡â–ˆâ–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–‡â–†â–‡â–†
wandb:   val_loss â–„â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–‡â–†â–…â–†â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:      epoch 33
wandb:         lr 0.00038
wandb:  train_acc 0.91967
wandb: train_loss 0.2084
wandb:    val_acc 0.74
wandb:   val_loss 0.92883
wandb: 
wandb: ğŸš€ View run [Baseline] L3-H4-D128-drop0.30015934517527193 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/d9kydw8v
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_015239-d9kydw8v/logs
wandb: Agent Starting Run: vmmj2rxs with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.07265823765842527
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006822966985400763
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025832-vmmj2rxs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-6
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vmmj2rxs
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 56.00 MiB is free. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 479.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H2-D128-drop0.07265823765842527 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vmmj2rxs
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025832-vmmj2rxs/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 56.00 MiB is free. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 479.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vmmj2rxs errored: CUDA out of memory. Tried to allocate 340.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 56.00 MiB is free. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 6.84 GiB is allocated by PyTorch, and 479.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 4rt5ccpv with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.2007788025177872
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0028747742672957464
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 4rt5ccpv
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025838-4rt5ccpv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-7
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4rt5ccpv
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 73, in train_model
    loss.backward()
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 294.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading console lines 0-0
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H8-D16-drop0.2007788025177872 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/4rt5ccpv
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025838-4rt5ccpv/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 73, in train_model
    loss.backward()
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 294.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 4rt5ccpv errored: CUDA out of memory. Tried to allocate 458.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 54.00 MiB is free. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 7.02 GiB is allocated by PyTorch, and 294.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 3ansu88w with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.4407543103861782
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00025336140294565586
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 3ansu88w
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025843-3ansu88w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-sweep-8
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3ansu88w
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5614, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 42.00 MiB is free. Including non-PyTorch memory, this process has 7.47 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 159.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H4-D128-drop0.4407543103861782-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3ansu88w
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025843-3ansu88w/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5614, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 42.00 MiB is free. Including non-PyTorch memory, this process has 7.47 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 159.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 3ansu88w errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 42.00 MiB is free. Including non-PyTorch memory, this process has 7.47 GiB memory in use. Of the allocated memory 7.16 GiB is allocated by PyTorch, and 159.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: wwb2yirc with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.4278383876689279
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003506225863828784
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run wwb2yirc
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025849-wwb2yirc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-9
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wwb2yirc
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 36.00 MiB is free. Including non-PyTorch memory, this process has 7.48 GiB memory in use. Of the allocated memory 7.17 GiB is allocated by PyTorch, and 156.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L3-H2-D128-drop0.4278383876689279-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wwb2yirc
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025849-wwb2yirc/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 36.00 MiB is free. Including non-PyTorch memory, this process has 7.48 GiB memory in use. Of the allocated memory 7.17 GiB is allocated by PyTorch, and 156.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wwb2yirc errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 36.00 MiB is free. Including non-PyTorch memory, this process has 7.48 GiB memory in use. Of the allocated memory 7.17 GiB is allocated by PyTorch, and 156.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 2ie6re57 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.4325683170145222
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007229805961605378
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025855-2ie6re57
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run copper-sweep-10
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2ie6re57
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 30.00 MiB is free. Including non-PyTorch memory, this process has 7.48 GiB memory in use. Of the allocated memory 7.26 GiB is allocated by PyTorch, and 71.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 0-53
wandb: ğŸš€ View run [Baseline] L4-H4-D16-drop0.4325683170145222-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2ie6re57
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025855-2ie6re57/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 92, in forward
    x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # mask: [B,T] boolean True = ignore this position
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 514, in forward
    output = mod(
             ^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 907, in forward
    x = x + self._sa_block(
            ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 928, in _sa_block
    x = self.self_attn(
        ^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1373, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 6230, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/functional.py", line 5621, in _in_projection_packed
    .contiguous()
     ^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 30.00 MiB is free. Including non-PyTorch memory, this process has 7.48 GiB memory in use. Of the allocated memory 7.26 GiB is allocated by PyTorch, and 71.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2ie6re57 errored: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 30.00 MiB is free. Including non-PyTorch memory, this process has 7.48 GiB memory in use. Of the allocated memory 7.26 GiB is allocated by PyTorch, and 71.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: grakvq91 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.07690538520875606
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005145450230312577
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025900-grakvq91
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-11
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/grakvq91
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 28.00 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 68.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 3-26
wandb: ğŸš€ View run [Baseline] L2-H8-D128-drop0.07690538520875606 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/grakvq91
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025900-grakvq91/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 28.00 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 68.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run grakvq91 errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 28.00 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 68.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: iexpv11b with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.13536187082515144
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009703413604670884
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run iexpv11b
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025906-iexpv11b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-sweep-12
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iexpv11b
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 91, in forward
    x = self.pe(x)                       # add positions â†’ unbounded
        ^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 29, in forward
    return x + pe
           ~~^~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.30 GiB is allocated by PyTorch, and 41.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt; uploading console lines 0-0
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-26
wandb: ğŸš€ View run [Baseline] L2-H4-D32-drop0.13536187082515144-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iexpv11b
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025906-iexpv11b/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 91, in forward
    x = self.pe(x)                       # add positions â†’ unbounded
        ^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 29, in forward
    return x + pe
           ~~^~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.30 GiB is allocated by PyTorch, and 41.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run iexpv11b errored: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 22.00 MiB is free. Including non-PyTorch memory, this process has 7.49 GiB memory in use. Of the allocated memory 7.30 GiB is allocated by PyTorch, and 41.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: pizer587 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.3464228249971076
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00970711310475294
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025912-pizer587
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-13
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pizer587
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 14.00 MiB is free. Including non-PyTorch memory, this process has 7.50 GiB memory in use. Of the allocated memory 7.31 GiB is allocated by PyTorch, and 40.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H2-D128-drop0.3464228249971076-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pizer587
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025912-pizer587/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 14.00 MiB is free. Including non-PyTorch memory, this process has 7.50 GiB memory in use. Of the allocated memory 7.31 GiB is allocated by PyTorch, and 40.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pizer587 errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 14.00 MiB is free. Including non-PyTorch memory, this process has 7.50 GiB memory in use. Of the allocated memory 7.31 GiB is allocated by PyTorch, and 40.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: vkjf2ri5 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.2897867841494235
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004213647276223962
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run vkjf2ri5
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025917-vkjf2ri5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-14
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vkjf2ri5
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 5-26
wandb: ğŸš€ View run [Baseline] L4-H4-D128-drop0.2897867841494235-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vkjf2ri5
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025917-vkjf2ri5/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 90, in forward
    x = self.input_proj(x)               # [B,T,D] â†’ unbounded
        ^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vkjf2ri5 errored: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: qwdi5z50 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.32701708464151497
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006703934121814858
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run qwdi5z50
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025923-qwdi5z50
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-sweep-15
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qwdi5z50
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 87, in forward
    x = self.stem(x)
        ^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H2-D16-drop0.32701708464151497-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qwdi5z50
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025923-qwdi5z50/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 69, in train_model
    outputs = model(X_batch)
              ^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/models/transformer.py", line 87, in forward
    x = self.stem(x)
        ^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 734, in forward
    return F.gelu(input, approximate=self.approximate)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qwdi5z50 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: xw10ee9l with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.18733806288554344
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004227903728142578
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run xw10ee9l
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025929-xw10ee9l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run drawn-sweep-16
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xw10ee9l
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 1-8
wandb: ğŸš€ View run [Baseline] L2-H8-D16-drop0.18733806288554344-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/xw10ee9l
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025929-xw10ee9l/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run xw10ee9l errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 37.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7tbpbkhr with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.31493121572121036
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009140345258263896
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 7tbpbkhr
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025934-7tbpbkhr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-17
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7tbpbkhr
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-8
wandb: ğŸš€ View run [Baseline] L3-H8-D32-drop0.31493121572121036-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7tbpbkhr
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025934-7tbpbkhr/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 116, in run_sweep_agent
    history = train_model(
              ^^^^^^^^^^^^
  File "/home/ianyang/stochastic_simulations/src/training/train.py", line 66, in train_model
    X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                       ^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7tbpbkhr errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 038nyocx with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.4040084676379253
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.001079174627097917
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 038nyocx
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025940-038nyocx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-sweep-18
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/038nyocx
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 3-14
wandb: ğŸš€ View run [Baseline] L4-H2-D32-drop0.4040084676379253 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/038nyocx
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025940-038nyocx/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 038nyocx errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 3szjx6ht with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.22875444915262116
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007617459959424615
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025945-3szjx6ht
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-19
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3szjx6ht
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H8-D32-drop0.22875444915262116-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3szjx6ht
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025945-3szjx6ht/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 3szjx6ht errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: nvntezqy with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.41539362751642794
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00016687536962615155
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run nvntezqy
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025951-nvntezqy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-20
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nvntezqy
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: ğŸš€ View run [Baseline] L1-H8-D64-drop0.41539362751642794 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nvntezqy
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025951-nvntezqy/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nvntezqy errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 1n3erz3n with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.1919970522317454
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008042033652837243
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 1n3erz3n
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_025957-1n3erz3n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-21
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1n3erz3n
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L1-H4-D64-drop0.1919970522317454-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1n3erz3n
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_025957-1n3erz3n/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1n3erz3n errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 07x2pb5k with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.34824089720558166
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006118716339368328
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 07x2pb5k
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030003-07x2pb5k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wandering-sweep-22
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/07x2pb5k
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H2-D32-drop0.34824089720558166 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/07x2pb5k
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030003-07x2pb5k/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 07x2pb5k errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: w8u73omz with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.46817031356571026
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005580295314966393
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030009-w8u73omz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sleek-sweep-23
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/w8u73omz
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H8-D32-drop0.46817031356571026 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/w8u73omz
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030009-w8u73omz/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run w8u73omz errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 36.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: z7vt7v7i with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.029544283705947416
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0027155831791518207
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030014-z7vt7v7i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run generous-sweep-24
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z7vt7v7i
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 34.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log
wandb: uploading summary, console lines 1-19
wandb: ğŸš€ View run [Baseline] L4-H2-D128-drop0.029544283705947416 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z7vt7v7i
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030014-z7vt7v7i/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 34.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run z7vt7v7i errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 34.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: hniczbb4 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.06185795474248801
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008621506463944908
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run hniczbb4
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030020-hniczbb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-25
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hniczbb4
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 32.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-19
wandb: ğŸš€ View run [Baseline] L4-H4-D128-drop0.06185795474248801 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/hniczbb4
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030020-hniczbb4/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 32.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run hniczbb4 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 32.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: c6svwq04 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.07756375181341169
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004374588684675346
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030025-c6svwq04
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-sweep-26
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/c6svwq04
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L2-H2-D128-drop0.07756375181341169-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/c6svwq04
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030025-c6svwq04/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run c6svwq04 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: uzzws7hm with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.4745062822871311
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0023591863474653947
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run uzzws7hm
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030031-uzzws7hm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-27
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uzzws7hm
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L3-H2-D32-drop0.4745062822871311-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/uzzws7hm
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030031-uzzws7hm/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run uzzws7hm errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: t6gfq3ym with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.14096033350120174
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008653378110134871
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030037-t6gfq3ym
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-28
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t6gfq3ym
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading output.log
wandb: ğŸš€ View run [Baseline] L3-H8-D64-drop0.14096033350120174-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t6gfq3ym
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030037-t6gfq3ym/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run t6gfq3ym errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: rxl8sflk with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.09977992276555342
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006627221383596495
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run rxl8sflk
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030042-rxl8sflk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-sweep-29
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rxl8sflk
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L2-H4-D16-drop0.09977992276555342-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/rxl8sflk
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030042-rxl8sflk/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run rxl8sflk errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: stjpxezf with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.17536620743854303
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00017759166617190047
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run stjpxezf
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030048-stjpxezf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-sweep-30
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/stjpxezf
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H4-D32-drop0.17536620743854303-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/stjpxezf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030048-stjpxezf/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run stjpxezf errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 09td8hpt with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.3821160000459013
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007599557076277563
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030054-09td8hpt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run different-sweep-31
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/09td8hpt
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H4-D64-drop0.3821160000459013-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/09td8hpt
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030054-09td8hpt/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 09td8hpt errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 3nq811dr with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.0932461366100959
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0018788764500290345
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 3nq811dr
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030059-3nq811dr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-32
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3nq811dr
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H2-D64-drop0.0932461366100959 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/3nq811dr
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030059-3nq811dr/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 3nq811dr errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 30.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: va4m83u4 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.4239714977091607
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005823159841311065
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030105-va4m83u4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-33
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/va4m83u4
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 28.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading output.log
wandb: ğŸš€ View run [Baseline] L2-H2-D128-drop0.4239714977091607 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/va4m83u4
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030105-va4m83u4/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 28.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run va4m83u4 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 28.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: de2p5jlx with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.4468331469162491
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0017919169251664222
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run de2p5jlx
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030111-de2p5jlx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dark-sweep-34
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/de2p5jlx
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 28.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L3-H2-D32-drop0.4468331469162491-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/de2p5jlx
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030111-de2p5jlx/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 28.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run de2p5jlx errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 28.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 84zwlu62 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.25156662841243216
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008866112918059686
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 84zwlu62
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030117-84zwlu62
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-sweep-35
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/84zwlu62
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading data
wandb: uploading output.log
wandb: ğŸš€ View run [Baseline] L1-H4-D128-drop0.25156662841243216-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/84zwlu62
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030117-84zwlu62/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 84zwlu62 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: z3y3as9x with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.26683511132085214
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004645188710435309
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030122-z3y3as9x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-sweep-36
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z3y3as9x
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading output.log; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H8-D16-drop0.26683511132085214 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z3y3as9x
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030122-z3y3as9x/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run z3y3as9x errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 9ov28oyj with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.05360039180537218
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009444755323824444
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 9ov28oyj
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030128-9ov28oyj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sweep-37
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9ov28oyj
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H8-D32-drop0.05360039180537218-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9ov28oyj
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030128-9ov28oyj/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9ov28oyj errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: vb8zkc0d with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.39571348118173705
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003710821071392219
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run vb8zkc0d
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030133-vb8zkc0d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sweep-38
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vb8zkc0d
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L2-H4-D16-drop0.39571348118173705-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/vb8zkc0d
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030133-vb8zkc0d/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run vb8zkc0d errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 1gvq5w62 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.07005017697496241
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008995822029513519
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030139-1gvq5w62
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-sweep-39
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1gvq5w62
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L3-H8-D64-drop0.07005017697496241 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1gvq5w62
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030139-1gvq5w62/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1gvq5w62 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ydbik4nn with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.4226363925111028
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009899248765761192
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run ydbik4nn
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030145-ydbik4nn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run firm-sweep-40
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ydbik4nn
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H2-D32-drop0.4226363925111028-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ydbik4nn
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030145-ydbik4nn/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ydbik4nn errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: z9ab2dak with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.2747654340906558
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005466895328871953
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run z9ab2dak
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030151-z9ab2dak
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-41
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z9ab2dak
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H2-D32-drop0.2747654340906558-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/z9ab2dak
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030151-z9ab2dak/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run z9ab2dak errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7fmmxu5e with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.38435021190110424
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008851814361548506
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 7fmmxu5e
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030156-7fmmxu5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-42
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7fmmxu5e
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H4-D64-drop0.38435021190110424 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7fmmxu5e
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030156-7fmmxu5e/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7fmmxu5e errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ic3t67zi with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.4592879841980928
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009993768699593472
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030202-ic3t67zi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-43
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ic3t67zi
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H4-D16-drop0.4592879841980928 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ic3t67zi
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030202-ic3t67zi/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ic3t67zi errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 26.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: e8ueudw4 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.3747205793338225
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003099733811422465
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030208-e8ueudw4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-sweep-44
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/e8ueudw4
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading console lines 0-0
wandb: uploading config.yaml
wandb: uploading summary, console lines 1-19
wandb: ğŸš€ View run [Baseline] L4-H8-D128-drop0.3747205793338225 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/e8ueudw4
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030208-e8ueudw4/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 942, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run e8ueudw4 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: peiwcbeh with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.07731157844855546
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007992707118242537
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run peiwcbeh
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030214-peiwcbeh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-45
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/peiwcbeh
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L2-H4-D16-drop0.07731157844855546 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/peiwcbeh
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030214-peiwcbeh/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run peiwcbeh errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: j79rgo71 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.15461304920708996
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005161918869375805
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run j79rgo71
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030219-j79rgo71
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-sweep-46
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j79rgo71
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H4-D32-drop0.15461304920708996-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j79rgo71
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030219-j79rgo71/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run j79rgo71 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: mo9qyz3l with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.03175557518609712
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007960436538701163
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run mo9qyz3l
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030225-mo9qyz3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-sweep-47
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/mo9qyz3l
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading requirements.txt; uploading console lines 0-0; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L3-H2-D32-drop0.03175557518609712 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/mo9qyz3l
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030225-mo9qyz3l/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mo9qyz3l errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ka1d1p9o with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.16927833790232005
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00995856351117944
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run ka1d1p9o
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030230-ka1d1p9o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fiery-sweep-48
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ka1d1p9o
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H4-D128-drop0.16927833790232005-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ka1d1p9o
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030230-ka1d1p9o/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ka1d1p9o errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: j6hpy2r0 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.09113672438608156
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007095552927292549
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030236-j6hpy2r0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-49
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j6hpy2r0
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt
wandb: uploading config.yaml
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H4-D64-drop0.09113672438608156-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j6hpy2r0
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030236-j6hpy2r0/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run j6hpy2r0 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 986nblbw with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.30080072237287614
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0015458610541525324
wandb: 	nhead: 2
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030242-986nblbw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run electric-sweep-50
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/986nblbw
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L1-H2-D64-drop0.30080072237287614-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/986nblbw
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030242-986nblbw/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 986nblbw errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: kpeluyid with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.14554521423890138
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0018387862403094648
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030248-kpeluyid
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rural-sweep-51
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/kpeluyid
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H4-D32-drop0.14554521423890138 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/kpeluyid
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030248-kpeluyid/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run kpeluyid errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 6ja2fb6k with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.21212972154048604
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005007571763854094
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 6ja2fb6k
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030253-6ja2fb6k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-sweep-52
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/6ja2fb6k
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L4-H2-D32-drop0.21212972154048604 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/6ja2fb6k
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030253-6ja2fb6k/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 6ja2fb6k errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: j7qxvb3a with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.27777751839325593
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003028911982779225
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run j7qxvb3a
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030259-j7qxvb3a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-sweep-53
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j7qxvb3a
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L3-H4-D32-drop0.27777751839325593-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/j7qxvb3a
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030259-j7qxvb3a/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run j7qxvb3a errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 0hqnh9ns with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.07780779756521194
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.001583508439546446
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 0hqnh9ns
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030305-0hqnh9ns
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-sweep-54
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0hqnh9ns
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 11-14
wandb: ğŸš€ View run [Baseline] L4-H2-D128-drop0.07780779756521194 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0hqnh9ns
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030305-0hqnh9ns/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0hqnh9ns errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: v8wql1px with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.21343161058306692
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008998531734152068
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run v8wql1px
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030310-v8wql1px
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run driven-sweep-55
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v8wql1px
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H4-D64-drop0.21343161058306692 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v8wql1px
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030310-v8wql1px/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v8wql1px errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: tqv28sa7 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.07473460875549354
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0007658850081471614
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run tqv28sa7
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030316-tqv28sa7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-56
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tqv28sa7
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L4-H8-D128-drop0.07473460875549354 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tqv28sa7
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030316-tqv28sa7/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run tqv28sa7 errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: sqcv3qbl with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.33548706514483556
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009296817369601296
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run sqcv3qbl
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030322-sqcv3qbl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-sweep-57
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sqcv3qbl
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading output.log; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L1-H4-D16-drop0.33548706514483556-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/sqcv3qbl
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030322-sqcv3qbl/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run sqcv3qbl errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: bkgzcf7s with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.1197261318377944
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0099513602592268
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run bkgzcf7s
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030328-bkgzcf7s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polished-sweep-58
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/bkgzcf7s
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H2-D64-drop0.1197261318377944-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/bkgzcf7s
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030328-bkgzcf7s/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run bkgzcf7s errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: pcumkhao with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.348129732579851
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00829378470048197
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run pcumkhao
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030333-pcumkhao
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swept-sweep-59
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pcumkhao
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H2-D64-drop0.348129732579851-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pcumkhao
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030333-pcumkhao/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pcumkhao errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: n94boaum with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.06429435584860538
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.001631799481480901
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030339-n94boaum
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rose-sweep-60
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/n94boaum
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L3-H8-D64-drop0.06429435584860538-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/n94boaum
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030339-n94boaum/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run n94boaum errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: zjwh7syz with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.21247962162239936
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008037789713863435
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030344-zjwh7syz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pleasant-sweep-61
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zjwh7syz
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L3-H8-D128-drop0.21247962162239936-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zjwh7syz
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030344-zjwh7syz/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zjwh7syz errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: wk8d9dd0 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.11130071883459008
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0020995403183527465
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run wk8d9dd0
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030350-wk8d9dd0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-sweep-62
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wk8d9dd0
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt
wandb: uploading output.log
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H2-D32-drop0.11130071883459008 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wk8d9dd0
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030350-wk8d9dd0/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wk8d9dd0 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 7ip5rs42 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.14279586765321306
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.002672196804497731
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030356-7ip5rs42
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-sweep-63
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7ip5rs42
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading requirements.txt; uploading data
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H4-D64-drop0.14279586765321306-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/7ip5rs42
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030356-7ip5rs42/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 7ip5rs42 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: r0xb3ost with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.05685128384796067
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005564074637379483
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030402-r0xb3ost
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run radiant-sweep-64
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/r0xb3ost
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading console lines 0-0; uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H8-D32-drop0.05685128384796067-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/r0xb3ost
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030402-r0xb3ost/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run r0xb3ost errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: up01276y with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.31935331495336744
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00528434087744163
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run up01276y
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030407-up01276y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fancy-sweep-65
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/up01276y
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H4-D64-drop0.31935331495336744 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/up01276y
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030407-up01276y/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run up01276y errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 1tpkt181 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.4067953545520648
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006744450469845544
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 1tpkt181
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030413-1tpkt181
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-66
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1tpkt181
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L4-H8-D64-drop0.4067953545520648 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/1tpkt181
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030413-1tpkt181/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 1tpkt181 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ginqi9tj with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.20772902903943777
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00886683249985202
wandb: 	nhead: 8
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030419-ginqi9tj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-67
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ginqi9tj
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L4-H8-D16-drop0.20772902903943777 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ginqi9tj
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030419-ginqi9tj/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ginqi9tj errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: zloe6p9n with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.31935878724134675
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004749487809253491
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run zloe6p9n
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030425-zloe6p9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run iconic-sweep-68
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zloe6p9n
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L3-H4-D64-drop0.31935878724134675 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/zloe6p9n
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030425-zloe6p9n/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run zloe6p9n errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 05yls6tt with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.46108037436197913
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0003404436846675989
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 05yls6tt
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030430-05yls6tt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run astral-sweep-69
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/05yls6tt
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H4-D64-drop0.46108037436197913 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/05yls6tt
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030430-05yls6tt/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 05yls6tt errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 679oaa0o with config:
wandb: 	batch_size: 64
wandb: 	d_model: 128
wandb: 	dropout: 0.07961086781451171
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00011914684794053922
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run 679oaa0o
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030436-679oaa0o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run noble-sweep-70
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/679oaa0o
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading output.log; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L2-H8-D128-drop0.07961086781451171 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/679oaa0o
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030436-679oaa0o/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 679oaa0o errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: nwn35rgf with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.2753002071801006
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006170681952421544
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030442-nwn35rgf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-sweep-71
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nwn35rgf
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L4-H4-D32-drop0.2753002071801006-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/nwn35rgf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030442-nwn35rgf/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run nwn35rgf errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ey6zzzng with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.19087053946871543
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005405303680453267
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run ey6zzzng
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030447-ey6zzzng
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-72
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ey6zzzng
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L4-H2-D32-drop0.19087053946871543 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ey6zzzng
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030447-ey6zzzng/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ey6zzzng errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 09epll5m with config:
wandb: 	batch_size: 32
wandb: 	d_model: 32
wandb: 	dropout: 0.025485451653033897
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008073777749749536
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 09epll5m
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030453-09epll5m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run balmy-sweep-73
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/09epll5m
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H2-D32-drop0.025485451653033897-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/09epll5m
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030453-09epll5m/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 09epll5m errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 8sbiq59w with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.007473774980535586
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0002451688009707759
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 8sbiq59w
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030459-8sbiq59w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-sweep-74
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8sbiq59w
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading data; uploading requirements.txt; uploading wandb-summary.json; uploading output.log
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L4-H2-D16-drop0.007473774980535586-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8sbiq59w
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030459-8sbiq59w/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8sbiq59w errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: f7v3jrc6 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.4338428065510238
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00292537440476342
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run f7v3jrc6
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030504-f7v3jrc6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-sweep-75
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f7v3jrc6
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L3-H2-D32-drop0.4338428065510238-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f7v3jrc6
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030504-f7v3jrc6/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f7v3jrc6 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: gu0oh410 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.3643764046939739
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0018544026796457889
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run gu0oh410
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030510-gu0oh410
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-76
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/gu0oh410
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: ğŸš€ View run [Baseline] L3-H4-D64-drop0.3643764046939739-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/gu0oh410
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030510-gu0oh410/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run gu0oh410 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: aaozne13 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.35779069142520387
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.002295881392347801
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run aaozne13
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030516-aaozne13
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-sweep-77
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/aaozne13
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt; uploading console lines 0-0
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L2-H4-D32-drop0.35779069142520387 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/aaozne13
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030516-aaozne13/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run aaozne13 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: dneni2mf with config:
wandb: 	batch_size: 32
wandb: 	d_model: 128
wandb: 	dropout: 0.3132565159363104
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0029962071735270285
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030522-dneni2mf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-sweep-78
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/dneni2mf
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H4-D128-drop0.3132565159363104-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/dneni2mf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030522-dneni2mf/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run dneni2mf errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: v0d2w4ui with config:
wandb: 	batch_size: 128
wandb: 	d_model: 16
wandb: 	dropout: 0.06833705396494683
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006796683532157684
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030527-v0d2w4ui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run faithful-sweep-79
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v0d2w4ui
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H2-D16-drop0.06833705396494683-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v0d2w4ui
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030527-v0d2w4ui/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v0d2w4ui errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: btwp4q79 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.05708032963792187
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005288940126720244
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run btwp4q79
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030533-btwp4q79
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sweep-80
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/btwp4q79
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: ğŸš€ View run [Baseline] L3-H2-D32-drop0.05708032963792187 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/btwp4q79
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030533-btwp4q79/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run btwp4q79 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: pliynjzk with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.051303883502389624
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0041465370444534275
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run pliynjzk
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030539-pliynjzk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-sweep-81
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pliynjzk
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading output.log; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L3-H2-D16-drop0.051303883502389624 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/pliynjzk
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030539-pliynjzk/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run pliynjzk errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: u9oi47cy with config:
wandb: 	batch_size: 64
wandb: 	d_model: 32
wandb: 	dropout: 0.3260281586988856
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005829076257243121
wandb: 	nhead: 2
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030544-u9oi47cy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-sweep-82
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u9oi47cy
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading output.log; uploading wandb-summary.json
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L3-H2-D32-drop0.3260281586988856 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/u9oi47cy
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030544-u9oi47cy/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run u9oi47cy errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: ea4ks09h with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.4110040869378058
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0009178288683124758
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030550-ea4ks09h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run clean-sweep-83
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ea4ks09h
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 6-14
wandb: ğŸš€ View run [Baseline] L3-H8-D128-drop0.4110040869378058-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/ea4ks09h
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030550-ea4ks09h/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ea4ks09h errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: iqo8lyqw with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.3153203399858693
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00895134555468894
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030556-iqo8lyqw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-sweep-84
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iqo8lyqw
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H4-D32-drop0.3153203399858693 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/iqo8lyqw
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030556-iqo8lyqw/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run iqo8lyqw errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: js4c5z0o with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.06968779345139609
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003348313713810954
wandb: 	nhead: 4
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run js4c5z0o
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030602-js4c5z0o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run expert-sweep-85
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/js4c5z0o
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H4-D64-drop0.06968779345139609-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/js4c5z0o
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030602-js4c5z0o/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run js4c5z0o errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: swik29x9 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.34085303114260807
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004822419743227321
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run swik29x9
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030607-swik29x9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run usual-sweep-86
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/swik29x9
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H8-D16-drop0.34085303114260807 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/swik29x9
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030607-swik29x9/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run swik29x9 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: v9xkmbsx with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.3219357344369017
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009166236632757622
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030613-v9xkmbsx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-87
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v9xkmbsx
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading config.yaml
wandb: ğŸš€ View run [Baseline] L3-H4-D64-drop0.3219357344369017 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/v9xkmbsx
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030613-v9xkmbsx/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run v9xkmbsx errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: qppj4d62 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.26479912950972867
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.00683008556109244
wandb: 	nhead: 2
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run qppj4d62
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030619-qppj4d62
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crisp-sweep-88
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qppj4d62
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H2-D16-drop0.26479912950972867-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/qppj4d62
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030619-qppj4d62/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run qppj4d62 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: tc0jjtd7 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.019226844517442776
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005496507993869941
wandb: 	nhead: 8
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030624-tc0jjtd7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-89
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tc0jjtd7
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading config.yaml
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L1-H8-D16-drop0.019226844517442776-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/tc0jjtd7
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030624-tc0jjtd7/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run tc0jjtd7 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: wwyc3617 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.4892003730413418
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009260853361907469
wandb: 	nhead: 2
wandb: 	num_layers: 4
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run wwyc3617
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030630-wwyc3617
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-sweep-90
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wwyc3617
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-metadata.json; uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L4-H2-D128-drop0.4892003730413418-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wwyc3617
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030630-wwyc3617/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wwyc3617 errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: f5748414 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.11886116114685524
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006695781175625258
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run f5748414
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030636-f5748414
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sweep-91
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f5748414
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H4-D32-drop0.11886116114685524 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/f5748414
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030636-f5748414/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f5748414 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: wnzxxg5h with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.43802253085420134
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.003687086714740393
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run wnzxxg5h
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030641-wnzxxg5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-sweep-92
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wnzxxg5h
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L3-H8-D64-drop0.43802253085420134-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/wnzxxg5h
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030641-wnzxxg5h/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wnzxxg5h errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 0wwa02v3 with config:
wandb: 	batch_size: 128
wandb: 	d_model: 64
wandb: 	dropout: 0.2585062124741931
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.0018811840947496289
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 0wwa02v3
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030647-0wwa02v3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-sweep-93
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0wwa02v3
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H8-D64-drop0.2585062124741931-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/0wwa02v3
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030647-0wwa02v3/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 0wwa02v3 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 2hc7zu5h with config:
wandb: 	batch_size: 128
wandb: 	d_model: 32
wandb: 	dropout: 0.3456354017661154
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.008186055514797981
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run 2hc7zu5h
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030653-2hc7zu5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jolly-sweep-94
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2hc7zu5h
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: updating run metadata; uploading requirements.txt; uploading console lines 0-0; uploading output.log; uploading wandb-summary.json
wandb: uploading output.log
wandb: ğŸš€ View run [Baseline] L2-H8-D32-drop0.3456354017661154-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/2hc7zu5h
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030653-2hc7zu5h/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 2hc7zu5h errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 8zf3ri3k with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.2698912481830886
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.006562945879057708
wandb: 	nhead: 4
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030658-8zf3ri3k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-sweep-95
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8zf3ri3k
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading wandb-summary.json; uploading output.log
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L2-H4-D64-drop0.2698912481830886-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/8zf3ri3k
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030658-8zf3ri3k/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 8zf3ri3k errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: crev1slb with config:
wandb: 	batch_size: 128
wandb: 	d_model: 128
wandb: 	dropout: 0.14992471748297742
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.009262894549187157
wandb: 	nhead: 8
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030704-crev1slb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rosy-sweep-96
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/crev1slb
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json; uploading requirements.txt; uploading console lines 0-0
wandb: uploading config.yaml
wandb: ğŸš€ View run [Baseline] L3-H8-D128-drop0.14992471748297742-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/crev1slb
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030704-crev1slb/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run crev1slb errored: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: cv6t8zo0 with config:
wandb: 	batch_size: 32
wandb: 	d_model: 16
wandb: 	dropout: 0.4633717477289604
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.007405035121156628
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: setting up run cv6t8zo0
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030710-cv6t8zo0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweepy-sweep-97
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/cv6t8zo0
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: ğŸš€ View run [Baseline] L3-H4-D16-drop0.4633717477289604-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/cv6t8zo0
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030710-cv6t8zo0/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run cv6t8zo0 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: t93wfqru with config:
wandb: 	batch_size: 32
wandb: 	d_model: 64
wandb: 	dropout: 0.14481877526576303
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.000489806250993189
wandb: 	nhead: 4
wandb: 	num_layers: 3
wandb: 	patience: 15
wandb: 	use_conv1d: True
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030715-t93wfqru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fine-sweep-98
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t93wfqru
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata
wandb: uploading requirements.txt; uploading output.log; uploading wandb-summary.json
wandb: uploading summary, console lines 1-14
wandb: ğŸš€ View run [Baseline] L3-H4-D64-drop0.14481877526576303-CNN at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/t93wfqru
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030715-t93wfqru/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run t93wfqru errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: 9imxk8p6 with config:
wandb: 	batch_size: 64
wandb: 	d_model: 16
wandb: 	dropout: 0.11696224600984378
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.005230282935387943
wandb: 	nhead: 8
wandb: 	num_layers: 2
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030721-9imxk8p6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-sweep-99
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9imxk8p6
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading data; uploading requirements.txt
wandb: uploading output.log; uploading config.yaml
wandb: uploading summary, console lines 0-14
wandb: ğŸš€ View run [Baseline] L2-H8-D16-drop0.11696224600984378 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/9imxk8p6
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030721-9imxk8p6/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run 9imxk8p6 errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: Agent Starting Run: i152xaqf with config:
wandb: 	batch_size: 64
wandb: 	d_model: 64
wandb: 	dropout: 0.46257719740146425
wandb: 	epochs: 100
wandb: 	experiment_name: Baseline
wandb: 	lr: 0.004991958836421498
wandb: 	nhead: 4
wandb: 	num_layers: 1
wandb: 	patience: 15
wandb: 	use_conv1d: False
wandb: setting up run i152xaqf
wandb: Tracking run with wandb version 0.22.3
wandb: Run data is saved locally in /home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/wandb/run-20260128_030727-i152xaqf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-sweep-100
wandb: â­ï¸ View project at https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: ğŸ§¹ View sweep at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/sweeps/ui8nb3kf
wandb: ğŸš€ View run at https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i152xaqf
Traceback (most recent call last):
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: updating run metadata; uploading wandb-metadata.json
wandb: uploading output.log; uploading wandb-summary.json
wandb: ğŸš€ View run [Baseline] L1-H4-D64-drop0.46257719740146425 at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep/runs/i152xaqf
wandb: â­ï¸ View project at: https://wandb.ai/grignard-reagent/IY018-baseline-sweep
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260128_030727-i152xaqf/logs
Traceback (most recent call last):
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/wandb/agents/pyagent.py", line 297, in _run_job
    self._function()
  File "/home/ianyang/stochastic_simulations/experiments/EXP-26-IY018/IY018_baseline_sweep.py", line 108, in run_sweep_agent
    model.to(device)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1355, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 915, in _apply
    module._apply(fn)
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1003, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1341, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i152xaqf errored: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 4.00 MiB is free. Including non-PyTorch memory, this process has 7.51 GiB memory in use. Of the allocated memory 7.33 GiB is allocated by PyTorch, and 24.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

Epoch [1/100] | train_loss 0.7020 | train_acc 0.5147 | val_loss 0.6854 | val_acc 0.5883
Epoch [2/100] | train_loss 0.6791 | train_acc 0.5590 | val_loss 0.6686 | val_acc 0.5950
No improvement (1/15).
Epoch [3/100] | train_loss 0.6521 | train_acc 0.5977 | val_loss 0.6669 | val_acc 0.5700
Epoch [4/100] | train_loss 0.6193 | train_acc 0.6367 | val_loss 0.6662 | val_acc 0.6000
No improvement (1/15).
Epoch [5/100] | train_loss 0.5986 | train_acc 0.6633 | val_loss 0.6831 | val_acc 0.5517
Epoch [6/100] | train_loss 0.5641 | train_acc 0.6950 | val_loss 0.6482 | val_acc 0.6550
Epoch [7/100] | train_loss 0.4887 | train_acc 0.7523 | val_loss 0.6558 | val_acc 0.6917
No improvement (1/15).
Epoch [8/100] | train_loss 0.4511 | train_acc 0.7863 | val_loss 0.8114 | val_acc 0.6733
Epoch [9/100] | train_loss 0.4379 | train_acc 0.7990 | val_loss 0.6956 | val_acc 0.6967
Epoch [10/100] | train_loss 0.4292 | train_acc 0.8110 | val_loss 0.7563 | val_acc 0.6983
Epoch [11/100] | train_loss 0.4168 | train_acc 0.8130 | val_loss 0.7727 | val_acc 0.7133
Epoch [12/100] | train_loss 0.4178 | train_acc 0.8153 | val_loss 0.7280 | val_acc 0.7250
No improvement (1/15).
Epoch [13/100] | train_loss 0.4144 | train_acc 0.8117 | val_loss 0.7739 | val_acc 0.7133
No improvement (2/15).
Epoch [14/100] | train_loss 0.4049 | train_acc 0.8157 | val_loss 0.7554 | val_acc 0.7167
Epoch [15/100] | train_loss 0.3892 | train_acc 0.8293 | val_loss 0.6613 | val_acc 0.7350
Epoch [16/100] | train_loss 0.3836 | train_acc 0.8323 | val_loss 0.6993 | val_acc 0.7383
No improvement (1/15).
Epoch [17/100] | train_loss 0.3808 | train_acc 0.8303 | val_loss 0.7047 | val_acc 0.7317
No improvement (2/15).
Epoch [18/100] | train_loss 0.3741 | train_acc 0.8353 | val_loss 0.7367 | val_acc 0.7283
No improvement (3/15).
Epoch [19/100] | train_loss 0.3739 | train_acc 0.8367 | val_loss 0.7821 | val_acc 0.7183
No improvement (4/15).
Epoch [20/100] | train_loss 0.3705 | train_acc 0.8433 | val_loss 0.7456 | val_acc 0.7317
Epoch [21/100] | train_loss 0.3519 | train_acc 0.8453 | val_loss 0.7007 | val_acc 0.7467
Epoch [22/100] | train_loss 0.3512 | train_acc 0.8427 | val_loss 0.7251 | val_acc 0.7517
No improvement (1/15).
Epoch [23/100] | train_loss 0.3461 | train_acc 0.8510 | val_loss 0.7397 | val_acc 0.7467
No improvement (2/15).
Epoch [24/100] | train_loss 0.3458 | train_acc 0.8513 | val_loss 0.7710 | val_acc 0.7383
No improvement (3/15).
Epoch [25/100] | train_loss 0.3455 | train_acc 0.8517 | val_loss 0.7525 | val_acc 0.7417
No improvement (4/15).
Epoch [26/100] | train_loss 0.3393 | train_acc 0.8540 | val_loss 0.7886 | val_acc 0.7383
No improvement (5/15).
Epoch [27/100] | train_loss 0.3340 | train_acc 0.8583 | val_loss 0.7699 | val_acc 0.7450
No improvement (6/15).
Epoch [28/100] | train_loss 0.3338 | train_acc 0.8577 | val_loss 0.7775 | val_acc 0.7383
No improvement (7/15).
Epoch [29/100] | train_loss 0.3305 | train_acc 0.8603 | val_loss 0.7822 | val_acc 0.7433
No improvement (8/15).
Epoch [30/100] | train_loss 0.3276 | train_acc 0.8637 | val_loss 0.7874 | val_acc 0.7417
No improvement (9/15).
Epoch [31/100] | train_loss 0.3289 | train_acc 0.8617 | val_loss 0.7976 | val_acc 0.7317
No improvement (10/15).
Epoch [32/100] | train_loss 0.3258 | train_acc 0.8647 | val_loss 0.8169 | val_acc 0.7283
No improvement (11/15).
Epoch [33/100] | train_loss 0.3263 | train_acc 0.8647 | val_loss 0.8095 | val_acc 0.7300
No improvement (12/15).
Epoch [34/100] | train_loss 0.3246 | train_acc 0.8657 | val_loss 0.8303 | val_acc 0.7300
No improvement (13/15).
Epoch [35/100] | train_loss 0.3241 | train_acc 0.8623 | val_loss 0.8183 | val_acc 0.7300
No improvement (14/15).
Epoch [36/100] | train_loss 0.3249 | train_acc 0.8713 | val_loss 0.8522 | val_acc 0.7300
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 75.17%
Starting training...
Epoch [1/100] | train_loss 0.8467 | train_acc 0.5397 | val_loss 0.6861 | val_acc 0.5567
Epoch [2/100] | train_loss 0.6044 | train_acc 0.6703 | val_loss 0.5744 | val_acc 0.7117
Epoch [3/100] | train_loss 0.5689 | train_acc 0.7263 | val_loss 0.5428 | val_acc 0.7483
Epoch [4/100] | train_loss 0.5060 | train_acc 0.7567 | val_loss 0.4896 | val_acc 0.7800
No improvement (1/15).
Epoch [5/100] | train_loss 0.4500 | train_acc 0.8007 | val_loss 0.5121 | val_acc 0.7717
No improvement (2/15).
Epoch [6/100] | train_loss 0.4235 | train_acc 0.8167 | val_loss 0.5155 | val_acc 0.7717
No improvement (3/15).
Epoch [7/100] | train_loss 0.4139 | train_acc 0.8217 | val_loss 0.5376 | val_acc 0.7450
No improvement (4/15).
Epoch [8/100] | train_loss 0.3819 | train_acc 0.8360 | val_loss 0.5508 | val_acc 0.7683
No improvement (5/15).
Epoch [9/100] | train_loss 0.3613 | train_acc 0.8477 | val_loss 0.5771 | val_acc 0.7667
No improvement (6/15).
Epoch [10/100] | train_loss 0.3489 | train_acc 0.8533 | val_loss 0.5714 | val_acc 0.7800
No improvement (7/15).
Epoch [11/100] | train_loss 0.3431 | train_acc 0.8563 | val_loss 0.5965 | val_acc 0.7583
No improvement (8/15).
Epoch [12/100] | train_loss 0.3434 | train_acc 0.8500 | val_loss 0.5850 | val_acc 0.7733
No improvement (9/15).
Epoch [13/100] | train_loss 0.3278 | train_acc 0.8627 | val_loss 0.5817 | val_acc 0.7733
Epoch [14/100] | train_loss 0.3297 | train_acc 0.8587 | val_loss 0.5510 | val_acc 0.7850
Epoch [15/100] | train_loss 0.3052 | train_acc 0.8723 | val_loss 0.5505 | val_acc 0.7917
Epoch [16/100] | train_loss 0.3039 | train_acc 0.8710 | val_loss 0.5442 | val_acc 0.7933
No improvement (1/15).
Epoch [17/100] | train_loss 0.3001 | train_acc 0.8710 | val_loss 0.5461 | val_acc 0.7850
No improvement (2/15).
Epoch [18/100] | train_loss 0.2959 | train_acc 0.8737 | val_loss 0.5667 | val_acc 0.7750
Epoch [19/100] | train_loss 0.2883 | train_acc 0.8797 | val_loss 0.5663 | val_acc 0.7967
No improvement (1/15).
Epoch [20/100] | train_loss 0.2779 | train_acc 0.8803 | val_loss 0.5756 | val_acc 0.7733
No improvement (2/15).
Epoch [21/100] | train_loss 0.2669 | train_acc 0.8837 | val_loss 0.6144 | val_acc 0.7750
No improvement (3/15).
Epoch [22/100] | train_loss 0.2592 | train_acc 0.8880 | val_loss 0.6895 | val_acc 0.7550
No improvement (4/15).
Epoch [23/100] | train_loss 0.2545 | train_acc 0.8883 | val_loss 0.6774 | val_acc 0.7550
No improvement (5/15).
Epoch [24/100] | train_loss 0.2492 | train_acc 0.8970 | val_loss 0.6846 | val_acc 0.7500
No improvement (6/15).
Epoch [25/100] | train_loss 0.2424 | train_acc 0.9017 | val_loss 0.7425 | val_acc 0.7450
No improvement (7/15).
Epoch [26/100] | train_loss 0.2461 | train_acc 0.8977 | val_loss 0.7277 | val_acc 0.7450
No improvement (8/15).
Epoch [27/100] | train_loss 0.2336 | train_acc 0.9003 | val_loss 0.8412 | val_acc 0.7317
No improvement (9/15).
Epoch [28/100] | train_loss 0.2315 | train_acc 0.9063 | val_loss 0.8311 | val_acc 0.7383
No improvement (10/15).
Epoch [29/100] | train_loss 0.2249 | train_acc 0.9047 | val_loss 0.7704 | val_acc 0.7450
No improvement (11/15).
Epoch [30/100] | train_loss 0.2205 | train_acc 0.9083 | val_loss 0.8143 | val_acc 0.7550
No improvement (12/15).
Epoch [31/100] | train_loss 0.2105 | train_acc 0.9157 | val_loss 0.8940 | val_acc 0.7383
No improvement (13/15).
Epoch [32/100] | train_loss 0.2057 | train_acc 0.9170 | val_loss 0.8550 | val_acc 0.7500
No improvement (14/15).
Epoch [33/100] | train_loss 0.2084 | train_acc 0.9197 | val_loss 0.9288 | val_acc 0.7400
No improvement (15/15).
ğŸ›‘ Early stopping.
Training complete.
Sweep Run Finished. Best Val Acc: 79.67%
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
Starting training...
