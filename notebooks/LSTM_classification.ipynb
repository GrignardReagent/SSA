{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utils.set_seed import set_seed\n",
    "from utils.load_data import load_and_split_data\n",
    "from models.lstm import LSTMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "ðŸ§  Step 1: Understanding LSTM in PyTorch\n",
    "\n",
    "PyTorch's ``nn.LSTM`` expects input of shape:\n",
    "```\n",
    "(seq_len, batch_size, input_size)\n",
    "```\n",
    "It returns:\n",
    "\n",
    "- output: (seq_len, batch_size, hidden_size)\n",
    "\n",
    "- (h_n, c_n): the hidden and cell states (each of shape: num_layers, batch_size, hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3338384/373019092.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ›  Step 2: Creating a Dataset\n",
    "# Example sequence\n",
    "data = np.array([i for i in range(1, 101)], dtype=np.float32)  # [1, 2, ..., 100]\n",
    "\n",
    "# Sequence parameters\n",
    "seq_length = 5\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])\n",
    "    Y.append(data[i+seq_length])\n",
    "\n",
    "X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n",
    "Y = torch.tensor(Y).unsqueeze(-1)  # Shape: (num_samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§± Step 3: Defining the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch, seq_len, hidden)\n",
    "        out = self.fc(out[:, -1, :])     # Take the last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 3088.1760\n",
      "Epoch [20/1000], Loss: 2603.0771\n",
      "Epoch [30/1000], Loss: 2142.0432\n",
      "Epoch [40/1000], Loss: 1760.4187\n",
      "Epoch [50/1000], Loss: 1463.3252\n",
      "Epoch [60/1000], Loss: 1235.0145\n",
      "Epoch [70/1000], Loss: 1062.7549\n",
      "Epoch [80/1000], Loss: 917.2125\n",
      "Epoch [90/1000], Loss: 795.4108\n",
      "Epoch [100/1000], Loss: 629.6179\n",
      "Epoch [110/1000], Loss: 548.2617\n",
      "Epoch [120/1000], Loss: 455.4848\n",
      "Epoch [130/1000], Loss: 342.7028\n",
      "Epoch [140/1000], Loss: 285.5832\n",
      "Epoch [150/1000], Loss: 235.2979\n",
      "Epoch [160/1000], Loss: 190.5910\n",
      "Epoch [170/1000], Loss: 152.8848\n",
      "Epoch [180/1000], Loss: 123.4877\n",
      "Epoch [190/1000], Loss: 99.7712\n",
      "Epoch [200/1000], Loss: 79.0921\n",
      "Epoch [210/1000], Loss: 60.9471\n",
      "Epoch [220/1000], Loss: 47.0691\n",
      "Epoch [230/1000], Loss: 36.5654\n",
      "Epoch [240/1000], Loss: 29.0025\n",
      "Epoch [250/1000], Loss: 23.4070\n",
      "Epoch [260/1000], Loss: 19.1567\n",
      "Epoch [270/1000], Loss: 15.8720\n",
      "Epoch [280/1000], Loss: 13.4180\n",
      "Epoch [290/1000], Loss: 11.3368\n",
      "Epoch [300/1000], Loss: 9.6080\n",
      "Epoch [310/1000], Loss: 8.2048\n",
      "Epoch [320/1000], Loss: 7.0633\n",
      "Epoch [330/1000], Loss: 6.1258\n",
      "Epoch [340/1000], Loss: 5.3342\n",
      "Epoch [350/1000], Loss: 4.6683\n",
      "Epoch [360/1000], Loss: 4.3253\n",
      "Epoch [370/1000], Loss: 3.6837\n",
      "Epoch [380/1000], Loss: 3.2339\n",
      "Epoch [390/1000], Loss: 2.8550\n",
      "Epoch [400/1000], Loss: 2.5467\n",
      "Epoch [410/1000], Loss: 2.2774\n",
      "Epoch [420/1000], Loss: 2.1377\n",
      "Epoch [430/1000], Loss: 1.9041\n",
      "Epoch [440/1000], Loss: 1.7247\n",
      "Epoch [450/1000], Loss: 1.5010\n",
      "Epoch [460/1000], Loss: 1.3653\n",
      "Epoch [470/1000], Loss: 1.2329\n",
      "Epoch [480/1000], Loss: 1.1218\n",
      "Epoch [490/1000], Loss: 1.0216\n",
      "Epoch [500/1000], Loss: 0.9331\n",
      "Epoch [510/1000], Loss: 0.8534\n",
      "Epoch [520/1000], Loss: 0.7915\n",
      "Epoch [530/1000], Loss: 0.7351\n",
      "Epoch [540/1000], Loss: 0.6885\n",
      "Epoch [550/1000], Loss: 0.6360\n",
      "Epoch [560/1000], Loss: 0.5650\n",
      "Epoch [570/1000], Loss: 0.5239\n",
      "Epoch [580/1000], Loss: 0.4834\n",
      "Epoch [590/1000], Loss: 0.4468\n",
      "Epoch [600/1000], Loss: 0.4140\n",
      "Epoch [610/1000], Loss: 0.3845\n",
      "Epoch [620/1000], Loss: 0.3576\n",
      "Epoch [630/1000], Loss: 0.3343\n",
      "Epoch [640/1000], Loss: 0.5361\n",
      "Epoch [650/1000], Loss: 0.2997\n",
      "Epoch [660/1000], Loss: 0.2778\n",
      "Epoch [670/1000], Loss: 0.2667\n",
      "Epoch [680/1000], Loss: 0.2438\n",
      "Epoch [690/1000], Loss: 0.2253\n",
      "Epoch [700/1000], Loss: 0.2104\n",
      "Epoch [710/1000], Loss: 0.1967\n",
      "Epoch [720/1000], Loss: 0.1849\n",
      "Epoch [730/1000], Loss: 0.1738\n",
      "Epoch [740/1000], Loss: 0.1636\n",
      "Epoch [750/1000], Loss: 0.1542\n",
      "Epoch [760/1000], Loss: 0.1491\n",
      "Epoch [770/1000], Loss: 0.5988\n",
      "Epoch [780/1000], Loss: 0.2246\n",
      "Epoch [790/1000], Loss: 0.1613\n",
      "Epoch [800/1000], Loss: 0.1190\n",
      "Epoch [810/1000], Loss: 0.1133\n",
      "Epoch [820/1000], Loss: 0.1068\n",
      "Epoch [830/1000], Loss: 0.1002\n",
      "Epoch [840/1000], Loss: 0.0945\n",
      "Epoch [850/1000], Loss: 0.0895\n",
      "Epoch [860/1000], Loss: 0.0850\n",
      "Epoch [870/1000], Loss: 0.0807\n",
      "Epoch [880/1000], Loss: 0.0767\n",
      "Epoch [890/1000], Loss: 0.0730\n",
      "Epoch [900/1000], Loss: 0.0695\n",
      "Epoch [910/1000], Loss: 0.0662\n",
      "Epoch [920/1000], Loss: 0.0633\n",
      "Epoch [930/1000], Loss: 0.1020\n",
      "Epoch [940/1000], Loss: 0.0936\n",
      "Epoch [950/1000], Loss: 0.0840\n",
      "Epoch [960/1000], Loss: 0.0820\n",
      "Epoch [970/1000], Loss: 0.0565\n",
      "Epoch [980/1000], Loss: 0.0490\n",
      "Epoch [990/1000], Loss: 0.0470\n",
      "Epoch [1000/1000], Loss: 0.0447\n"
     ]
    }
   ],
   "source": [
    "#ðŸ‹ï¸ Step 4: Training the Model\n",
    "# Initialize model, loss, optimizer\n",
    "model = LSTMModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next number: 98.30\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”® Step 5: Making Predictions\n",
    "# Predict the next value for a new sequence\n",
    "with torch.no_grad():\n",
    "    test_seq = torch.tensor([[96, 97, 98, 99, 100]], dtype=torch.float32).unsqueeze(-1)\n",
    "    prediction = model(test_seq)\n",
    "    print(f\"Predicted next number: {prediction.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LSTM using SSA data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“¦ Step 1: Reshape Input for LSTM\n",
    "\n",
    "We need to first standardise the data, then reshape the input.\n",
    "\n",
    "LSTM expects input in the shape ``(batch_size, seq_len, num_features)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.96668072]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [11.26942767]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]]\n",
      "X_train shape: (256, 144, 1)\n",
      "y_train shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2) # we must define split_val_size here to get a validation set\n",
    "# Standardize the data \n",
    "# If your input features are too large (e.g., >1000) or too small (<0.0001), it can cause unstable training, so it's better to standardize the data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Reshape input for LSTM, LSTM expects input in the shape (batch_size, seq_len, num_features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(X_train)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§± Step 2: Convert to PyTorch Tensors and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Step 3: Initialize and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/50], Loss: 0.6941, Train Acc: 0.4883\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/50], Loss: 0.6914, Train Acc: 0.5352\n",
      "Validation Acc: 0.4844\n",
      "No improvement (1/10).\n",
      "Epoch [3/50], Loss: 0.6892, Train Acc: 0.5312\n",
      "Validation Acc: 0.4844\n",
      "No improvement (2/10).\n",
      "Epoch [4/50], Loss: 0.6861, Train Acc: 0.5312\n",
      "Validation Acc: 0.4844\n",
      "No improvement (3/10).\n",
      "Epoch [5/50], Loss: 0.6855, Train Acc: 0.5312\n",
      "Validation Acc: 0.4844\n",
      "No improvement (4/10).\n",
      "Epoch [6/50], Loss: 0.6796, Train Acc: 0.6016\n",
      "Validation Acc: 0.4844\n",
      "No improvement (5/10).\n",
      "Epoch [7/50], Loss: 0.6767, Train Acc: 0.5234\n",
      "Validation Acc: 0.4844\n",
      "No improvement (6/10).\n",
      "Epoch [8/50], Loss: 0.6761, Train Acc: 0.5586\n",
      "Validation Acc: 0.4844\n",
      "No improvement (7/10).\n",
      "Epoch [9/50], Loss: 0.6617, Train Acc: 0.5742\n",
      "Validation Acc: 0.5000\n",
      "No improvement (8/10).\n",
      "Epoch [10/50], Loss: 0.6608, Train Acc: 0.5664\n",
      "Validation Acc: 0.5000\n",
      "No improvement (9/10).\n",
      "Epoch [11/50], Loss: 0.6549, Train Acc: 0.5859\n",
      "Validation Acc: 0.5156\n",
      "Epoch [12/50], Loss: 0.6572, Train Acc: 0.5742\n",
      "Validation Acc: 0.5312\n",
      "Epoch [13/50], Loss: 0.6579, Train Acc: 0.5977\n",
      "Validation Acc: 0.5312\n",
      "No improvement (1/10).\n",
      "Epoch [14/50], Loss: 0.6583, Train Acc: 0.6016\n",
      "Validation Acc: 0.5156\n",
      "No improvement (2/10).\n",
      "Epoch [15/50], Loss: 0.6566, Train Acc: 0.6172\n",
      "Validation Acc: 0.5156\n",
      "No improvement (3/10).\n",
      "Epoch [16/50], Loss: 0.6499, Train Acc: 0.5938\n",
      "Validation Acc: 0.5156\n",
      "No improvement (4/10).\n",
      "Epoch [17/50], Loss: 0.6390, Train Acc: 0.5898\n",
      "Validation Acc: 0.5156\n",
      "No improvement (5/10).\n",
      "Epoch [18/50], Loss: 0.6553, Train Acc: 0.5977\n",
      "Validation Acc: 0.5156\n",
      "No improvement (6/10).\n",
      "Epoch [19/50], Loss: 0.6465, Train Acc: 0.6016\n",
      "Validation Acc: 0.5156\n",
      "No improvement (7/10).\n",
      "Epoch [20/50], Loss: 0.6517, Train Acc: 0.5938\n",
      "Validation Acc: 0.5156\n",
      "No improvement (8/10).\n",
      "Epoch [21/50], Loss: 0.6506, Train Acc: 0.6016\n",
      "Validation Acc: 0.5156\n",
      "No improvement (9/10).\n",
      "Epoch [22/50], Loss: 0.6481, Train Acc: 0.6055\n",
      "Validation Acc: 0.5156\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier\n",
    "\n",
    "input_size = X_train.shape[2]  # each time step is a single value\n",
    "hidden_size = 64\n",
    "num_layers = 2 # number of LSTM layers\n",
    "output_size = len(torch.unique(y_train_tensor))  # number of classes\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMClassifier(input_size=input_size,           \n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers, output_size=output_size,\n",
    "                       dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(train_loader, val_loader=val_loader,\n",
    "                            epochs=50, patience=10,\n",
    "                            # save_path='best_lstm_model.pt'\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”® Step 4: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/100], Loss: 0.6972, Train Acc: 0.5391\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/100], Loss: 0.7149, Train Acc: 0.4883\n",
      "Validation Acc: 0.4531\n",
      "No improvement (1/10).\n",
      "Epoch [3/100], Loss: 0.7070, Train Acc: 0.4766\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [4/100], Loss: 0.7001, Train Acc: 0.4922\n",
      "Validation Acc: 0.5000\n",
      "No improvement (3/10).\n",
      "Epoch [5/100], Loss: 0.6931, Train Acc: 0.5391\n",
      "Validation Acc: 0.5000\n",
      "No improvement (4/10).\n",
      "Epoch [6/100], Loss: 0.6940, Train Acc: 0.5039\n",
      "Validation Acc: 0.5000\n",
      "No improvement (5/10).\n",
      "Epoch [7/100], Loss: 0.6914, Train Acc: 0.5195\n",
      "Validation Acc: 0.5469\n",
      "Epoch [8/100], Loss: 0.6990, Train Acc: 0.5273\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/10).\n",
      "Epoch [9/100], Loss: 0.6968, Train Acc: 0.4922\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [10/100], Loss: 0.6995, Train Acc: 0.4961\n",
      "Validation Acc: 0.5312\n",
      "No improvement (3/10).\n",
      "Epoch [11/100], Loss: 0.6961, Train Acc: 0.4727\n",
      "Validation Acc: 0.5312\n",
      "No improvement (4/10).\n",
      "Epoch [12/100], Loss: 0.6928, Train Acc: 0.5234\n",
      "Validation Acc: 0.5000\n",
      "No improvement (5/10).\n",
      "Epoch [13/100], Loss: 0.6919, Train Acc: 0.5273\n",
      "Validation Acc: 0.5000\n",
      "No improvement (6/10).\n",
      "Epoch [14/100], Loss: 0.6934, Train Acc: 0.4961\n",
      "Validation Acc: 0.5000\n",
      "No improvement (7/10).\n",
      "Epoch [15/100], Loss: 0.6998, Train Acc: 0.4844\n",
      "Validation Acc: 0.5000\n",
      "No improvement (8/10).\n",
      "Epoch [16/100], Loss: 0.6937, Train Acc: 0.4727\n",
      "Validation Acc: 0.5000\n",
      "No improvement (9/10).\n",
      "Epoch [17/100], Loss: 0.6916, Train Acc: 0.4766\n",
      "Validation Acc: 0.5156\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "âœ… Test accuracy: 0.5125\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model using SSA data\n",
    "output_file = '/home/ianyang/stochastic_simulations/notebooks/data/mRNA_trajectories_variance_1211_1200/m_traj_1211.9999999999995_1200.0_0.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2) # we must define split_val_size here to get a validation set\n",
    "# Standardize the data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Reshape input for LSTM, LSTM expects input in the shape (batch_size, seq_len, num_features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "from models.lstm import LSTMClassifier\n",
    "\n",
    "input_size = X_train.shape[2]  # each time step is a single value\n",
    "hidden_size = 64\n",
    "num_layers = 2 # number of LSTM layers\n",
    "output_size = len(torch.unique(y_train_tensor))  # number of classes\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LSTMClassifier(input_size=input_size, hidden_size=hidden_size, \n",
    "                       num_layers=num_layers, output_size=output_size,\n",
    "                       dropout_rate=dropout_rate, learning_rate=learning_rate, bidirectional=True, use_attention=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(train_loader, val_loader=val_loader,\n",
    "                            epochs=100, patience=10,\n",
    "                            # save_path='best_lstm_model.pt'\n",
    "                            )\n",
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick one-liner (all hyperparameters set in ``lstm_classifier`` and may be different from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/50], Loss: 0.6907, Train Acc: 0.5781\n",
      "Validation Acc: 0.6562\n",
      "Epoch [2/50], Loss: 0.6820, Train Acc: 0.6641\n",
      "Validation Acc: 0.6562\n",
      "No improvement (1/10).\n",
      "Epoch [3/50], Loss: 0.6691, Train Acc: 0.6758\n",
      "Validation Acc: 0.6562\n",
      "No improvement (2/10).\n",
      "Epoch [4/50], Loss: 0.6369, Train Acc: 0.7148\n",
      "Validation Acc: 0.6562\n",
      "No improvement (3/10).\n",
      "Epoch [5/50], Loss: 0.5778, Train Acc: 0.7266\n",
      "Validation Acc: 0.6094\n",
      "No improvement (4/10).\n",
      "Epoch [6/50], Loss: 0.5576, Train Acc: 0.7422\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/10).\n",
      "Epoch [7/50], Loss: 0.5515, Train Acc: 0.7461\n",
      "Validation Acc: 0.6094\n",
      "No improvement (6/10).\n",
      "Epoch [8/50], Loss: 0.5389, Train Acc: 0.7383\n",
      "Validation Acc: 0.6094\n",
      "No improvement (7/10).\n",
      "Epoch [9/50], Loss: 0.5365, Train Acc: 0.7383\n",
      "Validation Acc: 0.6406\n",
      "No improvement (8/10).\n",
      "Epoch [10/50], Loss: 0.5461, Train Acc: 0.7422\n",
      "Validation Acc: 0.6406\n",
      "No improvement (9/10).\n",
      "Epoch [11/50], Loss: 0.5428, Train Acc: 0.7422\n",
      "Validation Acc: 0.6406\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "=== Long-Short Term Memory (LSTM) Accuracy: 0.74 ===\n"
     ]
    }
   ],
   "source": [
    "from classifiers.lstm_classifier import lstm_classifier\n",
    "\n",
    "# Train SVM model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2) # we must define split_val_size here to get a validation set\n",
    "lstm_accuracy = lstm_classifier(X_train, X_val, X_test, y_train, y_val, y_test, epochs=50, bidirectional=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
