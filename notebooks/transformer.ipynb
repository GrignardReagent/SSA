{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa20774",
   "metadata": {},
   "source": [
    "# Transformer Classifier for Time Series\n",
    "\n",
    "This notebook demonstrates how to use the `TransformerClassifier` from the transformer.py module for time series classification. Transformers have become a cornerstone of modern deep learning, originally designed for natural language processing but now widely applied to various domains including time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b88752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utils.set_seed import set_seed\n",
    "from utils.load_data import load_and_split_data\n",
    "from models.transformer import TransformerClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25518c4a",
   "metadata": {},
   "source": [
    "## Introduction to Transformers for Time Series\n",
    "\n",
    "Transformers use a self-attention mechanism that allows each point in a sequence to attend to all other points, capturing long-range dependencies more effectively than traditional recurrent models like LSTMs. Here's why transformers are powerful for time series analysis:\n",
    "\n",
    "1. **Parallel Processing**: Unlike RNNs, transformers process the entire sequence in parallel rather than step-by-step, enabling faster training.\n",
    "2. **Global Context**: Self-attention captures relationships between any two points in the sequence regardless of their distance.\n",
    "3. **Positional Encoding**: Since transformers don't inherently understand sequence order, positional encodings are added to maintain temporal information.\n",
    "4. **Multi-Head Attention**: This allows the model to focus on different aspects of the input sequence simultaneously.\n",
    "\n",
    "In this notebook, we'll use the `TransformerClassifier` to predict classes from time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac1f56",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "ðŸ§  Step 1: Understanding Transformers in PyTorch\n",
    "\n",
    "PyTorch's `nn.TransformerEncoder` expects input of shape:\n",
    "```\n",
    "(batch_size, seq_len, d_model)\n",
    "```\n",
    "where `d_model` is the embedding dimension.\n",
    "\n",
    "The transformer processes the entire sequence at once, with self-attention allowing each position to attend to all positions in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9d714f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ›  Step 2: Creating a Simple Dataset\n",
    "# Example sequence\n",
    "data = np.array([i for i in range(1, 101)], dtype=np.float32)  # [1, 2, ..., 100]\n",
    "\n",
    "# Sequence parameters\n",
    "seq_length = 5\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])\n",
    "    Y.append(data[i+seq_length])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n",
    "Y = torch.tensor(Y).unsqueeze(-1)  # Shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "966a1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§± Step 3: Defining a Simple Transformer Model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_size=1, d_model=64, nhead=4, num_layers=2, output_size=1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_model) for _ in range(seq_length)\n",
    "        ])\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=128,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Project input to d_model dimension\n",
    "        x = self.input_proj(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Use the last sequence element for prediction\n",
    "        x = x[:, -1, :]  # [batch_size, d_model]\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_layer(x)  # [batch_size, output_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf28170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2642.2563\n",
      "Epoch [20/100], Loss: 1904.6705\n",
      "Epoch [30/100], Loss: 1227.4397\n",
      "Epoch [40/100], Loss: 823.4940\n",
      "Epoch [50/100], Loss: 753.5926\n",
      "Epoch [60/100], Loss: 728.4941\n",
      "Epoch [70/100], Loss: 753.8328\n",
      "Epoch [80/100], Loss: 753.2046\n",
      "Epoch [90/100], Loss: 755.1202\n",
      "Epoch [100/100], Loss: 754.7064\n"
     ]
    }
   ],
   "source": [
    "# ðŸ‹ï¸ Step 4: Training the Simple Model\n",
    "# Initialize model, loss, optimizer\n",
    "model = SimpleTransformer()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3dc07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next number: 53.93\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”® Step 5: Making Predictions\n",
    "# Predict the next value for a new sequence\n",
    "with torch.no_grad():\n",
    "    test_seq = torch.tensor([[96, 97, 98, 99, 100]], dtype=torch.float32).unsqueeze(-1)\n",
    "    prediction = model(test_seq)\n",
    "    print(f\"Predicted next number: {prediction.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e934b4",
   "metadata": {},
   "source": [
    "## Using TransformerClassifier with SSA Data\n",
    "\n",
    "Now let's apply the TransformerClassifier from our models module to the Stochastic Simulation Algorithm (SSA) time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5f983",
   "metadata": {},
   "source": [
    "ðŸ“¦ Step 1: Data Preprocessing\n",
    "\n",
    "Let's load the mRNA trajectories data, standardize it, and reshape it for the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275a9e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (256, 144, 1)\n",
      "y_train shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "# Load SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2)\n",
    "\n",
    "# Standardize the data (important for transformer models)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for transformer: [batch_size, seq_len, features]\n",
    "# In this case, each time step has a single feature\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cca37e",
   "metadata": {},
   "source": [
    "ðŸ§± Step 2: Convert to PyTorch Tensors and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab6d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590eb14",
   "metadata": {},
   "source": [
    "ðŸ§  Step 3: Initialize and Train TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6046005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = X_train.shape[2]  # Number of features per time step (1 in our case)\n",
    "d_model = 64                   # Embedding dimension\n",
    "nhead = 4                      # Number of attention heads\n",
    "num_layers = 3                 # Number of transformer layers\n",
    "output_size = len(np.unique(y_train))  # Number of classes\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    learning_rate=learning_rate,\n",
    "    use_conv1d=True  # Optional: use Conv1D preprocessing\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=50,\n",
    "    patience=10,\n",
    "    # save_path='best_transformer_model.pt'  # Uncomment to save the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd2a5b",
   "metadata": {},
   "source": [
    "ðŸ”® Step 4: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "073308af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test accuracy: 0.4875\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2457ef",
   "metadata": {},
   "source": [
    "## Complete End-to-End Example\n",
    "\n",
    "Let's put everything together into a single workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1376d8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved at best_transformer_model.pt (Best Val Acc: 0.5000)\n",
      "âœ… Model saved at best_transformer_model.pt (Best Val Acc: 0.5625)\n",
      "âœ… Test accuracy: 0.4875\n"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for transformer: [batch_size, seq_len, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Initialize the transformer model\n",
    "input_size = X_train.shape[2]\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "output_size = len(np.unique(y_train))\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    learning_rate=learning_rate,\n",
    "    use_conv1d=True,\n",
    "    use_auxiliary=True  # Use auxiliary task for better learning\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=100,\n",
    "    patience=15,\n",
    "    save_path='best_transformer_model.pt'\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acfa22",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "The `TransformerClassifier` class has several advanced features that can improve performance:\n",
    "\n",
    "1. **Conv1D Preprocessing**: Setting `use_conv1d=True` adds convolutional layers before the transformer to extract local features.\n",
    "\n",
    "2. **Auxiliary Task Learning**: Setting `use_auxiliary=True` adds an auxiliary regression task that helps the model learn better representations.\n",
    "\n",
    "3. **Different Optimizers**: You can choose between 'Adam', 'SGD', and 'AdamW' optimizers.\n",
    "\n",
    "Let's explore these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6025687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Transformer: 0.5000\n",
      "With Conv1D: 0.5000\n",
      "With Auxiliary: 0.5000\n",
      "With Both: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Example of using different configurations\n",
    "def train_and_evaluate(use_conv1d=False, use_auxiliary=False, optimizer='Adam'):\n",
    "    # Initialize the model with specified configuration\n",
    "    model = TransformerClassifier(\n",
    "        input_size=input_size,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        output_size=output_size,\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001,\n",
    "        optimizer=optimizer,\n",
    "        use_conv1d=use_conv1d,\n",
    "        use_auxiliary=use_auxiliary\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.train_model(\n",
    "        train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=30,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc = model.evaluate(test_loader)\n",
    "    return test_acc\n",
    "\n",
    "# Try different configurations\n",
    "print(f\"Basic Transformer: {train_and_evaluate(use_conv1d=False, use_auxiliary=False):.4f}\")\n",
    "print(f\"With Conv1D: {train_and_evaluate(use_conv1d=True, use_auxiliary=False):.4f}\")\n",
    "print(f\"With Auxiliary: {train_and_evaluate(use_conv1d=False, use_auxiliary=True):.4f}\")\n",
    "print(f\"With Both: {train_and_evaluate(use_conv1d=True, use_auxiliary=True):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbabbe",
   "metadata": {},
   "source": [
    "## Comparing with LSTM Classifier\n",
    "\n",
    "Let's compare the performance of the transformer classifier with the LSTM classifier we used previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1e4e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test accuracy: 0.8125\n",
      "Transformer Test accuracy: 0.4875\n",
      "Comparison: LSTM better by 0.3250\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier\n",
    "\n",
    "# Set up LSTM model\n",
    "lstm_model = LSTMClassifier(\n",
    "    input_size=input_size,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Train LSTM model\n",
    "lstm_history = lstm_model.train_model(\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=50,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_acc = lstm_model.evaluate(test_loader)\n",
    "print(f\"LSTM Test accuracy: {lstm_acc:.4f}\")\n",
    "print(f\"Transformer Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print comparison\n",
    "comparison = \"Transformer better\" if test_acc > lstm_acc else \"LSTM better\"\n",
    "print(f\"Comparison: {comparison} by {abs(test_acc - lstm_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a7a93",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored using the `TransformerClassifier` for time series classification tasks. Transformers offer several advantages for time series analysis:\n",
    "\n",
    "1. They can capture long-range dependencies in the data through self-attention.\n",
    "2. They process sequences in parallel, potentially leading to faster training.\n",
    "3. The multi-head attention mechanism allows the model to focus on different aspects of the input.\n",
    "\n",
    "Key settings that can improve transformer performance for time series:\n",
    "- Using an appropriate number of attention heads (usually 4-8 heads works well)\n",
    "- Adding Conv1D preprocessing to capture local patterns\n",
    "- Using auxiliary tasks for more robust feature learning\n",
    "- Proper standardization of the input data\n",
    "\n",
    "Whether transformers outperform LSTMs depends on the specific dataset and problem, but they're a powerful addition to the time series modeling toolkit.\n",
    "\n",
    "Note: LSTM can be better than a transformer for tasks with smaller datasets or when training time and computational resources are limited. It is also a strong choice for specific scenarios where its recurrent nature is beneficial, such as certain types of time-series forecasting or when the focus is on robust performance on difference sequences, like predicting price movements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
