{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa20774",
   "metadata": {},
   "source": [
    "# Transformer Classifier for Time Series\n",
    "\n",
    "This notebook demonstrates how to use the `TransformerClassifier` from the transformer.py module for time series classification. Transformers have become a cornerstone of modern deep learning, originally designed for natural language processing but now widely applied to various domains including time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b88752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utils.set_seed import set_seed\n",
    "from utils.load_data import load_and_split_data\n",
    "from models.transformer import TransformerClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25518c4a",
   "metadata": {},
   "source": [
    "## Introduction to Transformers for Time Series\n",
    "\n",
    "Transformers use a self-attention mechanism that allows each point in a sequence to attend to all other points, capturing long-range dependencies more effectively than traditional recurrent models like LSTMs. Here's why transformers are powerful for time series analysis:\n",
    "\n",
    "1. **Parallel Processing**: Unlike RNNs, transformers process the entire sequence in parallel rather than step-by-step, enabling faster training.\n",
    "2. **Global Context**: Self-attention captures relationships between any two points in the sequence regardless of their distance.\n",
    "3. **Positional Encoding**: Since transformers don't inherently understand sequence order, positional encodings are added to maintain temporal information.\n",
    "4. **Multi-Head Attention**: This allows the model to focus on different aspects of the input sequence simultaneously.\n",
    "\n",
    "In this notebook, we'll use the `TransformerClassifier` to predict classes from time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac1f56",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "ðŸ§  Step 1: Understanding Transformers in PyTorch\n",
    "\n",
    "PyTorch's `nn.TransformerEncoder` expects input of shape:\n",
    "```\n",
    "(batch_size, seq_len, d_model)\n",
    "```\n",
    "where `d_model` is the embedding dimension.\n",
    "\n",
    "The transformer processes the entire sequence at once, with self-attention allowing each position to attend to all positions in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d714f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3847587/3355746827.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ›  Step 2: Creating a Simple Dataset\n",
    "# Example sequence\n",
    "data = np.array([i for i in range(1, 101)], dtype=np.float32)  # [1, 2, ..., 100]\n",
    "\n",
    "# Sequence parameters\n",
    "seq_length = 5\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])\n",
    "    Y.append(data[i+seq_length])\n",
    "\n",
    "X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n",
    "Y = torch.tensor(Y).unsqueeze(-1)  # Shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966a1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§± Step 3: Defining a Simple Transformer Model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_size=1, d_model=64, nhead=4, num_layers=2, output_size=1):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_model) for _ in range(seq_length)\n",
    "        ])\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=128,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Project input to d_model dimension\n",
    "        x = self.input_proj(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Use the last sequence element for prediction\n",
    "        x = x[:, -1, :]  # [batch_size, d_model]\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_layer(x)  # [batch_size, output_size]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf28170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2642.2563\n",
      "Epoch [20/100], Loss: 1904.6705\n",
      "Epoch [30/100], Loss: 1227.4397\n",
      "Epoch [40/100], Loss: 823.4940\n",
      "Epoch [50/100], Loss: 753.5926\n",
      "Epoch [60/100], Loss: 728.4941\n",
      "Epoch [40/100], Loss: 823.4940\n",
      "Epoch [50/100], Loss: 753.5926\n",
      "Epoch [60/100], Loss: 728.4941\n",
      "Epoch [70/100], Loss: 753.8328\n",
      "Epoch [80/100], Loss: 753.2046\n",
      "Epoch [90/100], Loss: 755.1202\n",
      "Epoch [70/100], Loss: 753.8328\n",
      "Epoch [80/100], Loss: 753.2046\n",
      "Epoch [90/100], Loss: 755.1202\n",
      "Epoch [100/100], Loss: 754.7064\n",
      "Epoch [100/100], Loss: 754.7064\n"
     ]
    }
   ],
   "source": [
    "# ðŸ‹ï¸ Step 4: Training the Simple Model\n",
    "# Initialize model, loss, optimizer\n",
    "model = SimpleTransformer()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3dc07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next number: 53.93\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”® Step 5: Making Predictions\n",
    "# Predict the next value for a new sequence\n",
    "with torch.no_grad():\n",
    "    test_seq = torch.tensor([[96, 97, 98, 99, 100]], dtype=torch.float32).unsqueeze(-1)\n",
    "    prediction = model(test_seq)\n",
    "    print(f\"Predicted next number: {prediction.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e934b4",
   "metadata": {},
   "source": [
    "## Using TransformerClassifier with SSA Data\n",
    "\n",
    "Now let's apply the TransformerClassifier from our models module to the Stochastic Simulation Algorithm (SSA) time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5f983",
   "metadata": {},
   "source": [
    "ðŸ“¦ Step 1: Data Preprocessing\n",
    "\n",
    "Let's load the mRNA trajectories data, standardize it, and reshape it for the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "275a9e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (256, 144, 1)\n",
      "y_train shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "# Load SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2)\n",
    "\n",
    "# Standardize the data (important for transformer models)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for transformer: [batch_size, seq_len, features]\n",
    "# In this case, each time step has a single feature\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cca37e",
   "metadata": {},
   "source": [
    "ðŸ§± Step 2: Convert to PyTorch Tensors and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab6d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590eb14",
   "metadata": {},
   "source": [
    "ðŸ§  Step 3: Initialize and Train TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6046005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.6937, Train Acc: 0.4922\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/50], Loss: 0.6807, Train Acc: 0.5664\n",
      "Validation Acc: 0.6562\n",
      "Epoch [3/50], Loss: 0.6641, Train Acc: 0.6445\n",
      "Validation Acc: 0.6094\n",
      "No improvement (1/10).\n",
      "Epoch [4/50], Loss: 0.6397, Train Acc: 0.7227\n",
      "Validation Acc: 0.6719\n",
      "Epoch [5/50], Loss: 0.6241, Train Acc: 0.7148\n",
      "Validation Acc: 0.6094\n",
      "No improvement (1/10).\n",
      "Epoch [6/50], Loss: 0.6155, Train Acc: 0.7500\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/10).\n",
      "Epoch [7/50], Loss: 0.6121, Train Acc: 0.7344\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/10).\n",
      "Epoch [8/50], Loss: 0.6113, Train Acc: 0.7266\n",
      "Validation Acc: 0.6406\n",
      "No improvement (4/10).\n",
      "Epoch [9/50], Loss: 0.6160, Train Acc: 0.7109\n",
      "Validation Acc: 0.6406\n",
      "No improvement (5/10).\n",
      "Epoch [5/50], Loss: 0.6241, Train Acc: 0.7148\n",
      "Validation Acc: 0.6094\n",
      "No improvement (1/10).\n",
      "Epoch [6/50], Loss: 0.6155, Train Acc: 0.7500\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/10).\n",
      "Epoch [7/50], Loss: 0.6121, Train Acc: 0.7344\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/10).\n",
      "Epoch [8/50], Loss: 0.6113, Train Acc: 0.7266\n",
      "Validation Acc: 0.6406\n",
      "No improvement (4/10).\n",
      "Epoch [9/50], Loss: 0.6160, Train Acc: 0.7109\n",
      "Validation Acc: 0.6406\n",
      "No improvement (5/10).\n",
      "Epoch [10/50], Loss: 0.6165, Train Acc: 0.7148\n",
      "Validation Acc: 0.6562\n",
      "No improvement (6/10).\n",
      "Epoch [11/50], Loss: 0.6235, Train Acc: 0.6797\n",
      "Validation Acc: 0.6719\n",
      "No improvement (7/10).\n",
      "Epoch [12/50], Loss: 0.6041, Train Acc: 0.7344\n",
      "Validation Acc: 0.6719\n",
      "No improvement (8/10).\n",
      "Epoch [13/50], Loss: 0.6109, Train Acc: 0.7188\n",
      "Validation Acc: 0.6719\n",
      "No improvement (9/10).\n",
      "Epoch [14/50], Loss: 0.6168, Train Acc: 0.6914\n",
      "Validation Acc: 0.6719\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "Epoch [10/50], Loss: 0.6165, Train Acc: 0.7148\n",
      "Validation Acc: 0.6562\n",
      "No improvement (6/10).\n",
      "Epoch [11/50], Loss: 0.6235, Train Acc: 0.6797\n",
      "Validation Acc: 0.6719\n",
      "No improvement (7/10).\n",
      "Epoch [12/50], Loss: 0.6041, Train Acc: 0.7344\n",
      "Validation Acc: 0.6719\n",
      "No improvement (8/10).\n",
      "Epoch [13/50], Loss: 0.6109, Train Acc: 0.7188\n",
      "Validation Acc: 0.6719\n",
      "No improvement (9/10).\n",
      "Epoch [14/50], Loss: 0.6168, Train Acc: 0.6914\n",
      "Validation Acc: 0.6719\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "input_size = X_train.shape[2]  # Number of features per time step (1 in our case)\n",
    "d_model = 64                   # Embedding dimension\n",
    "nhead = 4                      # Number of attention heads\n",
    "num_layers = 3                 # Number of transformer layers\n",
    "output_size = len(np.unique(y_train))  # Number of classes\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    learning_rate=learning_rate,\n",
    "    use_conv1d=True  # Optional: use Conv1D preprocessing\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=50,\n",
    "    patience=10,\n",
    "    # save_path='best_transformer_model.pt'  # Uncomment to save the best model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dd2a5b",
   "metadata": {},
   "source": [
    "ðŸ”® Step 4: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "073308af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test accuracy: 0.7750\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2457ef",
   "metadata": {},
   "source": [
    "## Complete End-to-End Example\n",
    "\n",
    "Let's put everything together into a single workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1376d8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/torch/nn/modules/module.py:1751: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 17.9435, Train Acc: 0.5078\n",
      "Validation Acc: 0.5000\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.5000)\n",
      "Epoch [2/100], Loss: 9.1000, Train Acc: 0.5234\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/15).\n",
      "Epoch [3/100], Loss: 7.8625, Train Acc: 0.5273\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/15).\n",
      "Epoch [4/100], Loss: 5.2047, Train Acc: 0.5391\n",
      "Validation Acc: 0.6562\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.6562)\n",
      "Epoch [3/100], Loss: 7.8625, Train Acc: 0.5273\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/15).\n",
      "Epoch [4/100], Loss: 5.2047, Train Acc: 0.5391\n",
      "Validation Acc: 0.6562\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.6562)\n",
      "Epoch [5/100], Loss: 3.5383, Train Acc: 0.5703\n",
      "Validation Acc: 0.6719\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.6719)\n",
      "Epoch [6/100], Loss: 2.7280, Train Acc: 0.6641\n",
      "Validation Acc: 0.6875\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.6875)\n",
      "Epoch [5/100], Loss: 3.5383, Train Acc: 0.5703\n",
      "Validation Acc: 0.6719\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.6719)\n",
      "Epoch [6/100], Loss: 2.7280, Train Acc: 0.6641\n",
      "Validation Acc: 0.6875\n",
      "âœ… Model saved at best_transformer_model.pt (Best Validation Acc: 0.6875)\n",
      "Epoch [7/100], Loss: 2.7103, Train Acc: 0.6641\n",
      "Validation Acc: 0.6875\n",
      "No improvement (1/15).\n",
      "Epoch [8/100], Loss: 2.5973, Train Acc: 0.7188\n",
      "Validation Acc: 0.6875\n",
      "No improvement (2/15).\n",
      "Epoch [7/100], Loss: 2.7103, Train Acc: 0.6641\n",
      "Validation Acc: 0.6875\n",
      "No improvement (1/15).\n",
      "Epoch [8/100], Loss: 2.5973, Train Acc: 0.7188\n",
      "Validation Acc: 0.6875\n",
      "No improvement (2/15).\n",
      "Epoch [9/100], Loss: 2.5639, Train Acc: 0.7109\n",
      "Validation Acc: 0.6094\n",
      "No improvement (3/15).\n",
      "Epoch [10/100], Loss: 2.3979, Train Acc: 0.7109\n",
      "Validation Acc: 0.6250\n",
      "No improvement (4/15).\n",
      "Epoch [9/100], Loss: 2.5639, Train Acc: 0.7109\n",
      "Validation Acc: 0.6094\n",
      "No improvement (3/15).\n",
      "Epoch [10/100], Loss: 2.3979, Train Acc: 0.7109\n",
      "Validation Acc: 0.6250\n",
      "No improvement (4/15).\n",
      "Epoch [11/100], Loss: 2.5263, Train Acc: 0.7109\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/15).\n",
      "Epoch [12/100], Loss: 2.4315, Train Acc: 0.7109\n",
      "Validation Acc: 0.6094\n",
      "No improvement (6/15).\n",
      "Epoch [11/100], Loss: 2.5263, Train Acc: 0.7109\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/15).\n",
      "Epoch [12/100], Loss: 2.4315, Train Acc: 0.7109\n",
      "Validation Acc: 0.6094\n",
      "No improvement (6/15).\n",
      "Epoch [13/100], Loss: 2.6251, Train Acc: 0.7383\n",
      "Validation Acc: 0.6094\n",
      "No improvement (7/15).\n",
      "Epoch [14/100], Loss: 2.5046, Train Acc: 0.6953\n",
      "Validation Acc: 0.6250\n",
      "No improvement (8/15).\n",
      "Epoch [13/100], Loss: 2.6251, Train Acc: 0.7383\n",
      "Validation Acc: 0.6094\n",
      "No improvement (7/15).\n",
      "Epoch [14/100], Loss: 2.5046, Train Acc: 0.6953\n",
      "Validation Acc: 0.6250\n",
      "No improvement (8/15).\n",
      "Epoch [15/100], Loss: 2.3538, Train Acc: 0.6992\n",
      "Validation Acc: 0.6406\n",
      "No improvement (9/15).\n",
      "Epoch [16/100], Loss: 2.2472, Train Acc: 0.7148\n",
      "Validation Acc: 0.6719\n",
      "No improvement (10/15).\n",
      "Epoch [15/100], Loss: 2.3538, Train Acc: 0.6992\n",
      "Validation Acc: 0.6406\n",
      "No improvement (9/15).\n",
      "Epoch [16/100], Loss: 2.2472, Train Acc: 0.7148\n",
      "Validation Acc: 0.6719\n",
      "No improvement (10/15).\n",
      "Epoch [17/100], Loss: 2.4158, Train Acc: 0.6992\n",
      "Validation Acc: 0.6562\n",
      "No improvement (11/15).\n",
      "Epoch [18/100], Loss: 2.1463, Train Acc: 0.7109\n",
      "Validation Acc: 0.6406\n",
      "No improvement (12/15).\n",
      "Epoch [17/100], Loss: 2.4158, Train Acc: 0.6992\n",
      "Validation Acc: 0.6562\n",
      "No improvement (11/15).\n",
      "Epoch [18/100], Loss: 2.1463, Train Acc: 0.7109\n",
      "Validation Acc: 0.6406\n",
      "No improvement (12/15).\n",
      "Epoch [19/100], Loss: 2.0739, Train Acc: 0.7227\n",
      "Validation Acc: 0.6562\n",
      "No improvement (13/15).\n",
      "Epoch [20/100], Loss: 1.8893, Train Acc: 0.6914\n",
      "Validation Acc: 0.6562\n",
      "No improvement (14/15).\n",
      "Epoch [19/100], Loss: 2.0739, Train Acc: 0.7227\n",
      "Validation Acc: 0.6562\n",
      "No improvement (13/15).\n",
      "Epoch [20/100], Loss: 1.8893, Train Acc: 0.6914\n",
      "Validation Acc: 0.6562\n",
      "No improvement (14/15).\n",
      "Epoch [21/100], Loss: 2.0627, Train Acc: 0.6992\n",
      "Validation Acc: 0.6719\n",
      "No improvement (15/15).\n",
      "Stopping early! No improvement for 15 epochs.\n",
      "Training complete!\n",
      "âœ… Test accuracy: 0.7375\n",
      "Epoch [21/100], Loss: 2.0627, Train Acc: 0.6992\n",
      "Validation Acc: 0.6719\n",
      "No improvement (15/15).\n",
      "Stopping early! No improvement for 15 epochs.\n",
      "Training complete!\n",
      "âœ… Test accuracy: 0.7375\n"
     ]
    }
   ],
   "source": [
    "# Load and process data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape input for transformer: [batch_size, seq_len, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Initialize the transformer model\n",
    "input_size = X_train.shape[2]\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "num_layers = 4\n",
    "output_size = len(np.unique(y_train))\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size=input_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=dropout_rate,\n",
    "    learning_rate=learning_rate,\n",
    "    use_conv1d=True,\n",
    "    use_auxiliary=True  # Use auxiliary task for better learning\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=100,\n",
    "    patience=15,\n",
    "    save_path='best_transformer_model.pt'\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62acfa22",
   "metadata": {},
   "source": [
    "## Advanced Features\n",
    "\n",
    "The `TransformerClassifier` class has several advanced features that can improve performance:\n",
    "\n",
    "1. **Conv1D Preprocessing**: Setting `use_conv1d=True` adds convolutional layers before the transformer to extract local features.\n",
    "\n",
    "2. **Auxiliary Task Learning**: Setting `use_auxiliary=True` adds an auxiliary regression task that helps the model learn better representations.\n",
    "\n",
    "3. **Different Optimizers**: You can choose between 'Adam', 'SGD', and 'AdamW' optimizers.\n",
    "\n",
    "Let's explore these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6025687e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 0.6799, Train Acc: 0.6172\n",
      "Validation Acc: 0.6094\n",
      "Epoch [2/30], Loss: 0.6627, Train Acc: 0.7031\n",
      "Validation Acc: 0.6406\n",
      "Epoch [3/30], Loss: 0.6552, Train Acc: 0.7266\n",
      "Validation Acc: 0.6406\n",
      "No improvement (1/5).\n",
      "Epoch [4/30], Loss: 0.6416, Train Acc: 0.7266\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/5).\n",
      "Epoch [5/30], Loss: 0.6327, Train Acc: 0.7461\n",
      "Validation Acc: 0.6875\n",
      "Epoch [6/30], Loss: 0.6259, Train Acc: 0.7656\n",
      "Validation Acc: 0.6719\n",
      "No improvement (1/5).\n",
      "Epoch [7/30], Loss: 0.6147, Train Acc: 0.7500\n",
      "Validation Acc: 0.6719\n",
      "No improvement (2/5).\n",
      "Epoch [8/30], Loss: 0.6056, Train Acc: 0.7383\n",
      "Validation Acc: 0.6719\n",
      "No improvement (3/5).\n",
      "Epoch [9/30], Loss: 0.6047, Train Acc: 0.7695\n",
      "Validation Acc: 0.6719\n",
      "No improvement (4/5).\n",
      "Epoch [10/30], Loss: 0.6083, Train Acc: 0.7578\n",
      "Validation Acc: 0.6719\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "Basic Transformer: 0.7750\n",
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 0.6951, Train Acc: 0.4961\n",
      "Validation Acc: 0.6094\n",
      "Epoch [2/30], Loss: 0.6739, Train Acc: 0.6602\n",
      "Validation Acc: 0.5781\n",
      "No improvement (1/5).\n",
      "Epoch [6/30], Loss: 0.6259, Train Acc: 0.7656\n",
      "Validation Acc: 0.6719\n",
      "No improvement (1/5).\n",
      "Epoch [7/30], Loss: 0.6147, Train Acc: 0.7500\n",
      "Validation Acc: 0.6719\n",
      "No improvement (2/5).\n",
      "Epoch [8/30], Loss: 0.6056, Train Acc: 0.7383\n",
      "Validation Acc: 0.6719\n",
      "No improvement (3/5).\n",
      "Epoch [9/30], Loss: 0.6047, Train Acc: 0.7695\n",
      "Validation Acc: 0.6719\n",
      "No improvement (4/5).\n",
      "Epoch [10/30], Loss: 0.6083, Train Acc: 0.7578\n",
      "Validation Acc: 0.6719\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "Basic Transformer: 0.7750\n",
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 0.6951, Train Acc: 0.4961\n",
      "Validation Acc: 0.6094\n",
      "Epoch [2/30], Loss: 0.6739, Train Acc: 0.6602\n",
      "Validation Acc: 0.5781\n",
      "No improvement (1/5).\n",
      "Epoch [3/30], Loss: 0.6591, Train Acc: 0.6758\n",
      "Validation Acc: 0.6250\n",
      "Epoch [4/30], Loss: 0.6534, Train Acc: 0.6953\n",
      "Validation Acc: 0.6875\n",
      "Epoch [5/30], Loss: 0.6476, Train Acc: 0.6875\n",
      "Validation Acc: 0.6250\n",
      "No improvement (1/5).\n",
      "Epoch [6/30], Loss: 0.6270, Train Acc: 0.7539\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/5).\n",
      "Epoch [7/30], Loss: 0.6315, Train Acc: 0.7148\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/5).\n",
      "Epoch [8/30], Loss: 0.6256, Train Acc: 0.7227\n",
      "Validation Acc: 0.6562\n",
      "No improvement (4/5).\n",
      "Epoch [9/30], Loss: 0.6234, Train Acc: 0.7266\n",
      "Validation Acc: 0.6562\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "Epoch [3/30], Loss: 0.6591, Train Acc: 0.6758\n",
      "Validation Acc: 0.6250\n",
      "Epoch [4/30], Loss: 0.6534, Train Acc: 0.6953\n",
      "Validation Acc: 0.6875\n",
      "Epoch [5/30], Loss: 0.6476, Train Acc: 0.6875\n",
      "Validation Acc: 0.6250\n",
      "No improvement (1/5).\n",
      "Epoch [6/30], Loss: 0.6270, Train Acc: 0.7539\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/5).\n",
      "Epoch [7/30], Loss: 0.6315, Train Acc: 0.7148\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/5).\n",
      "Epoch [8/30], Loss: 0.6256, Train Acc: 0.7227\n",
      "Validation Acc: 0.6562\n",
      "No improvement (4/5).\n",
      "Epoch [9/30], Loss: 0.6234, Train Acc: 0.7266\n",
      "Validation Acc: 0.6562\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "With Conv1D: 0.7625\n",
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 27.3464, Train Acc: 0.4922\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/30], Loss: 18.8500, Train Acc: 0.5039\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/5).\n",
      "Epoch [3/30], Loss: 15.3188, Train Acc: 0.4805\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/5).\n",
      "Epoch [4/30], Loss: 12.5318, Train Acc: 0.4922\n",
      "Validation Acc: 0.6406\n",
      "Epoch [5/30], Loss: 10.5351, Train Acc: 0.5547\n",
      "Validation Acc: 0.6250\n",
      "No improvement (1/5).\n",
      "Epoch [6/30], Loss: 8.4167, Train Acc: 0.5938\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/5).\n",
      "With Conv1D: 0.7625\n",
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 27.3464, Train Acc: 0.4922\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/30], Loss: 18.8500, Train Acc: 0.5039\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/5).\n",
      "Epoch [3/30], Loss: 15.3188, Train Acc: 0.4805\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/5).\n",
      "Epoch [4/30], Loss: 12.5318, Train Acc: 0.4922\n",
      "Validation Acc: 0.6406\n",
      "Epoch [5/30], Loss: 10.5351, Train Acc: 0.5547\n",
      "Validation Acc: 0.6250\n",
      "No improvement (1/5).\n",
      "Epoch [6/30], Loss: 8.4167, Train Acc: 0.5938\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/5).\n",
      "Epoch [7/30], Loss: 7.0040, Train Acc: 0.6523\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/5).\n",
      "Epoch [8/30], Loss: 5.4138, Train Acc: 0.6406\n",
      "Validation Acc: 0.6094\n",
      "No improvement (4/5).\n",
      "Epoch [9/30], Loss: 4.4993, Train Acc: 0.7070\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "With Auxiliary: 0.7500\n",
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 21.4112, Train Acc: 0.5078\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/30], Loss: 13.1738, Train Acc: 0.4805\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/5).\n",
      "Epoch [3/30], Loss: 9.8021, Train Acc: 0.5312\n",
      "Validation Acc: 0.5156\n",
      "Epoch [7/30], Loss: 7.0040, Train Acc: 0.6523\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/5).\n",
      "Epoch [8/30], Loss: 5.4138, Train Acc: 0.6406\n",
      "Validation Acc: 0.6094\n",
      "No improvement (4/5).\n",
      "Epoch [9/30], Loss: 4.4993, Train Acc: 0.7070\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "With Auxiliary: 0.7500\n",
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/30], Loss: 21.4112, Train Acc: 0.5078\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/30], Loss: 13.1738, Train Acc: 0.4805\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/5).\n",
      "Epoch [3/30], Loss: 9.8021, Train Acc: 0.5312\n",
      "Validation Acc: 0.5156\n",
      "Epoch [4/30], Loss: 8.1283, Train Acc: 0.5195\n",
      "Validation Acc: 0.6094\n",
      "Epoch [5/30], Loss: 5.9020, Train Acc: 0.6289\n",
      "Validation Acc: 0.6875\n",
      "Epoch [6/30], Loss: 4.4121, Train Acc: 0.6875\n",
      "Validation Acc: 0.6719\n",
      "No improvement (1/5).\n",
      "Epoch [7/30], Loss: 3.0752, Train Acc: 0.7422\n",
      "Validation Acc: 0.6719\n",
      "No improvement (2/5).\n",
      "Epoch [8/30], Loss: 2.6533, Train Acc: 0.7188\n",
      "Validation Acc: 0.6719\n",
      "No improvement (3/5).\n",
      "Epoch [9/30], Loss: 2.4073, Train Acc: 0.7188\n",
      "Validation Acc: 0.6250\n",
      "No improvement (4/5).\n",
      "Epoch [4/30], Loss: 8.1283, Train Acc: 0.5195\n",
      "Validation Acc: 0.6094\n",
      "Epoch [5/30], Loss: 5.9020, Train Acc: 0.6289\n",
      "Validation Acc: 0.6875\n",
      "Epoch [6/30], Loss: 4.4121, Train Acc: 0.6875\n",
      "Validation Acc: 0.6719\n",
      "No improvement (1/5).\n",
      "Epoch [7/30], Loss: 3.0752, Train Acc: 0.7422\n",
      "Validation Acc: 0.6719\n",
      "No improvement (2/5).\n",
      "Epoch [8/30], Loss: 2.6533, Train Acc: 0.7188\n",
      "Validation Acc: 0.6719\n",
      "No improvement (3/5).\n",
      "Epoch [9/30], Loss: 2.4073, Train Acc: 0.7188\n",
      "Validation Acc: 0.6250\n",
      "No improvement (4/5).\n",
      "Epoch [10/30], Loss: 1.9821, Train Acc: 0.7383\n",
      "Validation Acc: 0.6406\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "With Both: 0.7500\n",
      "Epoch [10/30], Loss: 1.9821, Train Acc: 0.7383\n",
      "Validation Acc: 0.6406\n",
      "No improvement (5/5).\n",
      "Stopping early! No improvement for 5 epochs.\n",
      "Training complete!\n",
      "With Both: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Example of using different configurations\n",
    "def train_and_evaluate(use_conv1d=False, use_auxiliary=False, optimizer='Adam'):\n",
    "    # Initialize the model with specified configuration\n",
    "    model = TransformerClassifier(\n",
    "        input_size=input_size,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_layers=2,\n",
    "        output_size=output_size,\n",
    "        dropout_rate=0.2,\n",
    "        learning_rate=0.001,\n",
    "        optimizer=optimizer,\n",
    "        use_conv1d=use_conv1d,\n",
    "        use_auxiliary=use_auxiliary\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.train_model(\n",
    "        train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=30,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_acc = model.evaluate(test_loader)\n",
    "    return test_acc\n",
    "\n",
    "# Try different configurations\n",
    "print(f\"Basic Transformer: {train_and_evaluate(use_conv1d=False, use_auxiliary=False):.4f}\")\n",
    "print(f\"With Conv1D: {train_and_evaluate(use_conv1d=True, use_auxiliary=False):.4f}\")\n",
    "print(f\"With Auxiliary: {train_and_evaluate(use_conv1d=False, use_auxiliary=True):.4f}\")\n",
    "print(f\"With Both: {train_and_evaluate(use_conv1d=True, use_auxiliary=True):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbabbe",
   "metadata": {},
   "source": [
    "## Comparing with LSTM Classifier\n",
    "\n",
    "Let's compare the performance of the transformer classifier with the LSTM classifier we used previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1e4e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/50], Loss: 0.6934, Train Acc: 0.4961\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/50], Loss: 0.6931, Train Acc: 0.5195\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/10).\n",
      "Epoch [3/50], Loss: 0.6930, Train Acc: 0.5117\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [4/50], Loss: 0.6931, Train Acc: 0.5000\n",
      "Validation Acc: 0.4844\n",
      "No improvement (3/10).\n",
      "Epoch [5/50], Loss: 0.6932, Train Acc: 0.4492\n",
      "Validation Acc: 0.4844\n",
      "No improvement (4/10).\n",
      "Epoch [6/50], Loss: 0.6927, Train Acc: 0.5117\n",
      "Validation Acc: 0.4844\n",
      "No improvement (5/10).\n",
      "Epoch [7/50], Loss: 0.6918, Train Acc: 0.4922\n",
      "Validation Acc: 0.5156\n",
      "Epoch [8/50], Loss: 0.6901, Train Acc: 0.5273\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/10).\n",
      "Epoch [9/50], Loss: 0.6885, Train Acc: 0.5117\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [10/50], Loss: 0.6752, Train Acc: 0.6133\n",
      "Validation Acc: 0.5000\n",
      "No improvement (3/10).\n",
      "Epoch [11/50], Loss: 0.6615, Train Acc: 0.6055\n",
      "Validation Acc: 0.6562\n",
      "Epoch [12/50], Loss: 0.6613, Train Acc: 0.6523\n",
      "Validation Acc: 0.6562\n",
      "No improvement (1/10).\n",
      "Epoch [13/50], Loss: 0.6596, Train Acc: 0.7031\n",
      "Validation Acc: 0.6562\n",
      "No improvement (2/10).\n",
      "Epoch [14/50], Loss: 0.6509, Train Acc: 0.7344\n",
      "Validation Acc: 0.6250\n",
      "No improvement (3/10).\n",
      "Epoch [15/50], Loss: 0.6443, Train Acc: 0.7461\n",
      "Validation Acc: 0.6250\n",
      "No improvement (4/10).\n",
      "Epoch [16/50], Loss: 0.6494, Train Acc: 0.7266\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/10).\n",
      "Epoch [17/50], Loss: 0.6521, Train Acc: 0.7344\n",
      "Validation Acc: 0.6094\n",
      "No improvement (6/10).\n",
      "Epoch [18/50], Loss: 0.6408, Train Acc: 0.7344\n",
      "Validation Acc: 0.6094\n",
      "No improvement (7/10).\n",
      "Epoch [19/50], Loss: 0.6507, Train Acc: 0.7305\n",
      "Validation Acc: 0.6094\n",
      "No improvement (8/10).\n",
      "Epoch [20/50], Loss: 0.6494, Train Acc: 0.7305\n",
      "Validation Acc: 0.6250\n",
      "No improvement (9/10).\n",
      "Epoch [21/50], Loss: 0.6408, Train Acc: 0.7305\n",
      "Validation Acc: 0.6250\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "LSTM Test accuracy: 0.7500\n",
      "Transformer Test accuracy: 0.7375\n",
      "Comparison: LSTM better by 0.0125\n",
      "Epoch [9/50], Loss: 0.6885, Train Acc: 0.5117\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [10/50], Loss: 0.6752, Train Acc: 0.6133\n",
      "Validation Acc: 0.5000\n",
      "No improvement (3/10).\n",
      "Epoch [11/50], Loss: 0.6615, Train Acc: 0.6055\n",
      "Validation Acc: 0.6562\n",
      "Epoch [12/50], Loss: 0.6613, Train Acc: 0.6523\n",
      "Validation Acc: 0.6562\n",
      "No improvement (1/10).\n",
      "Epoch [13/50], Loss: 0.6596, Train Acc: 0.7031\n",
      "Validation Acc: 0.6562\n",
      "No improvement (2/10).\n",
      "Epoch [14/50], Loss: 0.6509, Train Acc: 0.7344\n",
      "Validation Acc: 0.6250\n",
      "No improvement (3/10).\n",
      "Epoch [15/50], Loss: 0.6443, Train Acc: 0.7461\n",
      "Validation Acc: 0.6250\n",
      "No improvement (4/10).\n",
      "Epoch [16/50], Loss: 0.6494, Train Acc: 0.7266\n",
      "Validation Acc: 0.6094\n",
      "No improvement (5/10).\n",
      "Epoch [17/50], Loss: 0.6521, Train Acc: 0.7344\n",
      "Validation Acc: 0.6094\n",
      "No improvement (6/10).\n",
      "Epoch [18/50], Loss: 0.6408, Train Acc: 0.7344\n",
      "Validation Acc: 0.6094\n",
      "No improvement (7/10).\n",
      "Epoch [19/50], Loss: 0.6507, Train Acc: 0.7305\n",
      "Validation Acc: 0.6094\n",
      "No improvement (8/10).\n",
      "Epoch [20/50], Loss: 0.6494, Train Acc: 0.7305\n",
      "Validation Acc: 0.6250\n",
      "No improvement (9/10).\n",
      "Epoch [21/50], Loss: 0.6408, Train Acc: 0.7305\n",
      "Validation Acc: 0.6250\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "LSTM Test accuracy: 0.7500\n",
      "Transformer Test accuracy: 0.7375\n",
      "Comparison: LSTM better by 0.0125\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier\n",
    "\n",
    "# Set up LSTM model\n",
    "lstm_model = LSTMClassifier(\n",
    "    input_size=input_size,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Train LSTM model\n",
    "lstm_history = lstm_model.train_model(\n",
    "    train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=50,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_acc = lstm_model.evaluate(test_loader)\n",
    "print(f\"LSTM Test accuracy: {lstm_acc:.4f}\")\n",
    "print(f\"Transformer Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Print comparison\n",
    "comparison = \"Transformer better\" if test_acc > lstm_acc else \"LSTM better\"\n",
    "print(f\"Comparison: {comparison} by {abs(test_acc - lstm_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a7a93",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored using the `TransformerClassifier` for time series classification tasks. Transformers offer several advantages for time series analysis:\n",
    "\n",
    "1. They can capture long-range dependencies in the data through self-attention.\n",
    "2. They process sequences in parallel, potentially leading to faster training.\n",
    "3. The multi-head attention mechanism allows the model to focus on different aspects of the input.\n",
    "\n",
    "Key settings that can improve transformer performance for time series:\n",
    "- Using an appropriate number of attention heads (usually 4-8 heads works well)\n",
    "- Adding Conv1D preprocessing to capture local patterns\n",
    "- Using auxiliary tasks for more robust feature learning\n",
    "- Proper standardization of the input data\n",
    "\n",
    "Whether transformers outperform LSTMs depends on the specific dataset and problem, but they're a powerful addition to the time series modeling toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
