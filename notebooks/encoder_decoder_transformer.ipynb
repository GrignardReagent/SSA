{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3285a477",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Transformer from Scratch for Time Series Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5712b6",
   "metadata": {},
   "source": [
    "| Component | Description | Purpose |\n",
    "|---|---|---|\n",
    "| Multi-Head Attention | Mechanism to focus on different parts of the input | Captures dependencies across different positions in the sequence |\n",
    "| Feed-Forward Networks | Position-wise fully connected layers | Transforms the attention outputs, adding complexity |\n",
    "| Positional Encoding | Adds positional information to embeddings | Provides sequence order context to the model |\n",
    "| Layer Normalization | Normalizes inputs to each sub-layer | Stabilizes training, improves convergence |\n",
    "| Residual Connections | Shortcuts between layers | Helps in training deeper networks by minimizing gradient issues |\n",
    "| Dropout | Randomly zeroes some network connections | Prevents overfitting by regularizing the model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02335513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917462d6",
   "metadata": {},
   "source": [
    "## Multi-Head Attention \n",
    "The multi-head attention mechanism computes the attention between each pair of positions in a sequence. \n",
    "\n",
    "It consists of multiple “attention heads” that capture different aspects of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec3097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model # Model's dimension\n",
    "        self.num_heads = num_heads # Number of attention heads\n",
    "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
    "        \n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to a single tensor of original shape(batch_size, seq_length, d_model)\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6e442",
   "metadata": {},
   "source": [
    "In summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a6a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60a597",
   "metadata": {},
   "source": [
    "In summary, the PositionWiseFeedForward class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66b9ba",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "Positional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75e3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606de50",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "\n",
    "**Input:**\n",
    "\n",
    "1. x: The input to the encoder layer.\n",
    "2. mask: Optional mask to ignore certain parts of the input.\n",
    "\n",
    "**Processing steps:**\n",
    "\n",
    "1. Self-attention: The input x is passed through the multi-head self-attention mechanism.\n",
    "2. Add and normalize (after attention): The attention output is added to the original input (residual connection), followed by dropout and normalization using norm1.\n",
    "3. Feed-forward network: The output from the previous step is passed through the position-wise feed-forward network.\n",
    "4. Add and normalize (after feed-forward): Similar to step 2, the feed-forward output is added to the input of this stage (residual connection), followed by dropout and normalization using norm2.\n",
    "5. Output: The processed tensor is returned as the output of the encoder layer.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "The EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by the position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. Together, these components allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cb9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f2a66",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f94a9",
   "metadata": {},
   "source": [
    "**Input:**\n",
    "\n",
    "1. x: The input to the decoder layer.\n",
    "2. enc_output: The output from the corresponding encoder (used in the cross-attention step).\n",
    "3. src_mask: Source mask to ignore certain parts of the encoder's output.\n",
    "4. tgt_mask: Target mask to ignore certain parts of the decoder's input.\n",
    "\n",
    "**Processing steps:**\n",
    "\n",
    "1. Self-attention on target sequence: The input x is processed through a self-attention mechanism.\n",
    "2. Add and normalize (after self-attention): The output from self-attention is added to the original x, followed by dropout and normalization using norm1.\n",
    "3. Cross-attention with encoder output: The normalized output from the previous step is processed through a cross-attention mechanism that attends to the encoder's output enc_output.\n",
    "4. Add and normalize (after cross-attention): The output from cross-attention is added to the input of this stage, followed by dropout and normalization using norm2.\n",
    "5. Feed-forward network: The output from the previous step is passed through the feed-forward network.\n",
    "6. Add and normalize (after feed-forward): The feed-forward output is added to the input of this stage, followed by dropout and normalization using norm3.\n",
    "7. Output: The processed tensor is returned as the output of the decoder layer.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "The DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d6c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253a08a1",
   "metadata": {},
   "source": [
    "## Combining the encoder and decoder layers to create the complete Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "672e7990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc649e",
   "metadata": {},
   "source": [
    "## Training the Transformer Model\n",
    "\n",
    "**Loss function and optimizer:**\n",
    "\n",
    "1. criterion = nn.CrossEntropyLoss(ignore_index=0): Defines the loss function as cross-entropy loss. The ignore_index argument is set to 0, meaning the loss will not consider targets with an index of 0 (typically reserved for padding tokens).\n",
    "2. optimizer = optim.Adam(...): Defines the optimizer as Adam with a learning rate of 0.0001 and specific beta values.\n",
    "\n",
    "**Model training mode:**\n",
    "\n",
    "transformer.train(): Sets the transformer model to training mode, enabling behaviors like dropout that only apply during training.\n",
    "\n",
    "**Training loop:**\n",
    "\n",
    "The code snippet trains the model for 100 epochs using a typical training loop:\n",
    "\n",
    "1. for epoch in range(100): Iterates over 100 training epochs.\n",
    "2. optimizer.zero_grad(): Clears the gradients from the previous iteration.\n",
    "3. output = transformer(src_data, tgt_data[:, :-1]): Passes the source data and the target data (excluding the last token in each sequence) through the transformer. This is common in sequence-to-sequence tasks where the target is shifted by one token.\n",
    "4. loss = criterion(...): Computes the loss between the model's predictions and the target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the cross-entropy loss function.\n",
    "5. loss.backward(): Computes the gradients of the loss with respect to the model's parameters.\n",
    "6. optimizer.step(): Updates the model's parameters using the computed gradients.\n",
    "7. print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\"): Prints the current epoch number and the loss value for that epoch.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "This code snippet trains the transformer model on randomly generated source and target sequences for 100 epochs. It uses the Adam optimizer and the cross-entropy loss function. The loss is printed for each epoch, allowing you to monitor the training progress. In a real-world scenario, you would replace the random source and target sequences with actual data from your task, such as machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e97e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb31f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.687600135803223\n",
      "Epoch: 2, Loss: 8.552017211914062\n",
      "Epoch: 3, Loss: 8.48848819732666\n",
      "Epoch: 4, Loss: 8.429305076599121\n",
      "Epoch: 5, Loss: 8.373306274414062\n",
      "Epoch: 6, Loss: 8.307185173034668\n",
      "Epoch: 7, Loss: 8.231369018554688\n",
      "Epoch: 8, Loss: 8.143341064453125\n",
      "Epoch: 9, Loss: 8.06795883178711\n",
      "Epoch: 10, Loss: 7.98965311050415\n",
      "Epoch: 11, Loss: 7.911393165588379\n",
      "Epoch: 12, Loss: 7.826714515686035\n",
      "Epoch: 13, Loss: 7.7469305992126465\n",
      "Epoch: 14, Loss: 7.654513359069824\n",
      "Epoch: 15, Loss: 7.58083438873291\n",
      "Epoch: 16, Loss: 7.492771148681641\n",
      "Epoch: 17, Loss: 7.410205364227295\n",
      "Epoch: 18, Loss: 7.328305721282959\n",
      "Epoch: 19, Loss: 7.2448577880859375\n",
      "Epoch: 20, Loss: 7.17183780670166\n",
      "Epoch: 21, Loss: 7.092879295349121\n",
      "Epoch: 22, Loss: 7.015315055847168\n",
      "Epoch: 23, Loss: 6.9309186935424805\n",
      "Epoch: 24, Loss: 6.857698917388916\n",
      "Epoch: 25, Loss: 6.7838239669799805\n",
      "Epoch: 26, Loss: 6.6970977783203125\n",
      "Epoch: 27, Loss: 6.6360249519348145\n",
      "Epoch: 28, Loss: 6.562252044677734\n",
      "Epoch: 29, Loss: 6.484872341156006\n",
      "Epoch: 30, Loss: 6.416121482849121\n",
      "Epoch: 31, Loss: 6.339635372161865\n",
      "Epoch: 32, Loss: 6.268152236938477\n",
      "Epoch: 33, Loss: 6.2041144371032715\n",
      "Epoch: 34, Loss: 6.134422302246094\n",
      "Epoch: 35, Loss: 6.080063343048096\n",
      "Epoch: 36, Loss: 6.007822513580322\n",
      "Epoch: 37, Loss: 5.943079471588135\n",
      "Epoch: 38, Loss: 5.87852144241333\n",
      "Epoch: 39, Loss: 5.815489292144775\n",
      "Epoch: 40, Loss: 5.748911380767822\n",
      "Epoch: 41, Loss: 5.692944526672363\n",
      "Epoch: 42, Loss: 5.625576496124268\n",
      "Epoch: 43, Loss: 5.564764499664307\n",
      "Epoch: 44, Loss: 5.513116836547852\n",
      "Epoch: 45, Loss: 5.450679779052734\n",
      "Epoch: 46, Loss: 5.390275955200195\n",
      "Epoch: 47, Loss: 5.324213027954102\n",
      "Epoch: 48, Loss: 5.270132541656494\n",
      "Epoch: 49, Loss: 5.211920261383057\n",
      "Epoch: 50, Loss: 5.1551361083984375\n",
      "Epoch: 51, Loss: 5.0922417640686035\n",
      "Epoch: 52, Loss: 5.046037673950195\n",
      "Epoch: 53, Loss: 4.985489368438721\n",
      "Epoch: 54, Loss: 4.935455799102783\n",
      "Epoch: 55, Loss: 4.878218173980713\n",
      "Epoch: 56, Loss: 4.820362567901611\n",
      "Epoch: 57, Loss: 4.774475574493408\n",
      "Epoch: 58, Loss: 4.719005584716797\n",
      "Epoch: 59, Loss: 4.662496566772461\n",
      "Epoch: 60, Loss: 4.609283924102783\n",
      "Epoch: 61, Loss: 4.551872730255127\n",
      "Epoch: 62, Loss: 4.50629186630249\n",
      "Epoch: 63, Loss: 4.457148551940918\n",
      "Epoch: 64, Loss: 4.4086785316467285\n",
      "Epoch: 65, Loss: 4.352556228637695\n",
      "Epoch: 66, Loss: 4.305898189544678\n",
      "Epoch: 67, Loss: 4.252135276794434\n",
      "Epoch: 68, Loss: 4.207470893859863\n",
      "Epoch: 69, Loss: 4.157885551452637\n",
      "Epoch: 70, Loss: 4.099843502044678\n",
      "Epoch: 71, Loss: 4.056782245635986\n",
      "Epoch: 72, Loss: 4.013000011444092\n",
      "Epoch: 73, Loss: 3.9653964042663574\n",
      "Epoch: 74, Loss: 3.9135324954986572\n",
      "Epoch: 75, Loss: 3.866353750228882\n",
      "Epoch: 76, Loss: 3.817808151245117\n",
      "Epoch: 77, Loss: 3.770308256149292\n",
      "Epoch: 78, Loss: 3.717573881149292\n",
      "Epoch: 79, Loss: 3.6738064289093018\n",
      "Epoch: 80, Loss: 3.631211519241333\n",
      "Epoch: 81, Loss: 3.5839619636535645\n",
      "Epoch: 82, Loss: 3.5391294956207275\n",
      "Epoch: 83, Loss: 3.49326491355896\n",
      "Epoch: 84, Loss: 3.4491798877716064\n",
      "Epoch: 85, Loss: 3.411745071411133\n",
      "Epoch: 86, Loss: 3.3543057441711426\n",
      "Epoch: 87, Loss: 3.3079476356506348\n",
      "Epoch: 88, Loss: 3.2723395824432373\n",
      "Epoch: 89, Loss: 3.2253313064575195\n",
      "Epoch: 90, Loss: 3.1829559803009033\n",
      "Epoch: 91, Loss: 3.131946563720703\n",
      "Epoch: 92, Loss: 3.0907604694366455\n",
      "Epoch: 93, Loss: 3.0469436645507812\n",
      "Epoch: 94, Loss: 2.9962821006774902\n",
      "Epoch: 95, Loss: 2.9526023864746094\n",
      "Epoch: 96, Loss: 2.913454532623291\n",
      "Epoch: 97, Loss: 2.87924861907959\n",
      "Epoch: 98, Loss: 2.8361945152282715\n",
      "Epoch: 99, Loss: 2.791872978210449\n",
      "Epoch: 100, Loss: 2.747858762741089\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe110e3",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373fdc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 8.832911491394043\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad(): # disables gradient calculation for validation\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6328c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e672c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stochastic_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
