{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianyang/micromamba/envs/stochastic_sim/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from utils.set_seed import set_seed\n",
    "from utils.load_data import load_and_split_data\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "ðŸ§  Step 1: Understanding LSTM in PyTorch\n",
    "\n",
    "PyTorch's ``nn.LSTM`` expects input of shape:\n",
    "```\n",
    "(seq_len, batch_size, input_size)\n",
    "```\n",
    "It returns:\n",
    "\n",
    "- output: (seq_len, batch_size, hidden_size)\n",
    "\n",
    "- (h_n, c_n): the hidden and cell states (each of shape: num_layers, batch_size, hidden_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2225613/1721303172.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ›  Step 2: Creating a Dataset\n",
    "# Example sequence\n",
    "data = np.array([i for i in range(1, 101)], dtype=np.float32)  # [1, 2, ..., 100]\n",
    "\n",
    "# Sequence parameters\n",
    "seq_length = 5\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in range(len(data) - seq_length):\n",
    "    X.append(data[i:i+seq_length])\n",
    "    Y.append(data[i+seq_length])\n",
    "\n",
    "X = torch.tensor(X).unsqueeze(-1)  # Shape: (num_samples, seq_len, 1)\n",
    "Y = torch.tensor(Y).unsqueeze(-1)  # Shape: (num_samples, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ§± Step 3: Defining the LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "\n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch, seq_len, hidden)\n",
    "        out = self.fc(out[:, -1, :])     # Take the last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 3058.3525\n",
      "Epoch [20/1000], Loss: 2550.8455\n",
      "Epoch [30/1000], Loss: 2107.7151\n",
      "Epoch [40/1000], Loss: 1748.3368\n",
      "Epoch [50/1000], Loss: 1462.8076\n",
      "Epoch [60/1000], Loss: 1241.3734\n",
      "Epoch [70/1000], Loss: 1070.9705\n",
      "Epoch [80/1000], Loss: 924.4969\n",
      "Epoch [90/1000], Loss: 783.4571\n",
      "Epoch [100/1000], Loss: 610.5067\n",
      "Epoch [110/1000], Loss: 508.5166\n",
      "Epoch [120/1000], Loss: 424.8292\n",
      "Epoch [130/1000], Loss: 352.0236\n",
      "Epoch [140/1000], Loss: 286.4334\n",
      "Epoch [150/1000], Loss: 234.0060\n",
      "Epoch [160/1000], Loss: 192.4662\n",
      "Epoch [170/1000], Loss: 159.6376\n",
      "Epoch [180/1000], Loss: 132.0133\n",
      "Epoch [190/1000], Loss: 107.2719\n",
      "Epoch [200/1000], Loss: 86.3053\n",
      "Epoch [210/1000], Loss: 70.4205\n",
      "Epoch [220/1000], Loss: 58.0133\n",
      "Epoch [230/1000], Loss: 48.3615\n",
      "Epoch [240/1000], Loss: 40.6656\n",
      "Epoch [250/1000], Loss: 34.4655\n",
      "Epoch [260/1000], Loss: 29.4186\n",
      "Epoch [270/1000], Loss: 25.2660\n",
      "Epoch [280/1000], Loss: 21.8175\n",
      "Epoch [290/1000], Loss: 18.9309\n",
      "Epoch [300/1000], Loss: 16.3271\n",
      "Epoch [310/1000], Loss: 13.7929\n",
      "Epoch [320/1000], Loss: 11.5856\n",
      "Epoch [330/1000], Loss: 9.7936\n",
      "Epoch [340/1000], Loss: 8.3314\n",
      "Epoch [350/1000], Loss: 7.1412\n",
      "Epoch [360/1000], Loss: 6.9192\n",
      "Epoch [370/1000], Loss: 5.4593\n",
      "Epoch [380/1000], Loss: 4.7527\n",
      "Epoch [390/1000], Loss: 4.1205\n",
      "Epoch [400/1000], Loss: 3.6435\n",
      "Epoch [410/1000], Loss: 3.2197\n",
      "Epoch [420/1000], Loss: 2.8618\n",
      "Epoch [430/1000], Loss: 2.5531\n",
      "Epoch [440/1000], Loss: 2.2836\n",
      "Epoch [450/1000], Loss: 2.0482\n",
      "Epoch [460/1000], Loss: 1.9489\n",
      "Epoch [470/1000], Loss: 1.6268\n",
      "Epoch [480/1000], Loss: 1.4551\n",
      "Epoch [490/1000], Loss: 1.2386\n",
      "Epoch [500/1000], Loss: 0.9782\n",
      "Epoch [510/1000], Loss: 0.8347\n",
      "Epoch [520/1000], Loss: 0.7202\n",
      "Epoch [530/1000], Loss: 0.6241\n",
      "Epoch [540/1000], Loss: 0.5447\n",
      "Epoch [550/1000], Loss: 0.4788\n",
      "Epoch [560/1000], Loss: 0.4229\n",
      "Epoch [570/1000], Loss: 0.3753\n",
      "Epoch [580/1000], Loss: 0.3345\n",
      "Epoch [590/1000], Loss: 0.2993\n",
      "Epoch [600/1000], Loss: 0.2688\n",
      "Epoch [610/1000], Loss: 0.2422\n",
      "Epoch [620/1000], Loss: 0.2189\n",
      "Epoch [630/1000], Loss: 0.1985\n",
      "Epoch [640/1000], Loss: 0.1804\n",
      "Epoch [650/1000], Loss: 0.1644\n",
      "Epoch [660/1000], Loss: 0.1502\n",
      "Epoch [670/1000], Loss: 0.1375\n",
      "Epoch [680/1000], Loss: 0.1262\n",
      "Epoch [690/1000], Loss: 0.1277\n",
      "Epoch [700/1000], Loss: 0.1428\n",
      "Epoch [710/1000], Loss: 0.3701\n",
      "Epoch [720/1000], Loss: 0.1716\n",
      "Epoch [730/1000], Loss: 0.1112\n",
      "Epoch [740/1000], Loss: 0.0883\n",
      "Epoch [750/1000], Loss: 0.0788\n",
      "Epoch [760/1000], Loss: 0.0728\n",
      "Epoch [770/1000], Loss: 0.0677\n",
      "Epoch [780/1000], Loss: 0.0632\n",
      "Epoch [790/1000], Loss: 0.0591\n",
      "Epoch [800/1000], Loss: 0.0555\n",
      "Epoch [810/1000], Loss: 0.0522\n",
      "Epoch [820/1000], Loss: 0.0491\n",
      "Epoch [830/1000], Loss: 0.0462\n",
      "Epoch [840/1000], Loss: 0.0435\n",
      "Epoch [850/1000], Loss: 0.0411\n",
      "Epoch [860/1000], Loss: 0.0387\n",
      "Epoch [870/1000], Loss: 0.0366\n",
      "Epoch [880/1000], Loss: 0.0345\n",
      "Epoch [890/1000], Loss: 0.0326\n",
      "Epoch [900/1000], Loss: 0.0307\n",
      "Epoch [910/1000], Loss: 0.0290\n",
      "Epoch [920/1000], Loss: 0.0273\n",
      "Epoch [930/1000], Loss: 0.0257\n",
      "Epoch [940/1000], Loss: 0.0242\n",
      "Epoch [950/1000], Loss: 0.0228\n",
      "Epoch [960/1000], Loss: 0.0324\n",
      "Epoch [970/1000], Loss: 0.1071\n",
      "Epoch [980/1000], Loss: 0.1197\n",
      "Epoch [990/1000], Loss: 0.0552\n",
      "Epoch [1000/1000], Loss: 0.0312\n"
     ]
    }
   ],
   "source": [
    "#ðŸ‹ï¸ Step 4: Training the Model\n",
    "# Initialize model, loss, optimizer\n",
    "model = LSTMModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted next number: 98.88\n"
     ]
    }
   ],
   "source": [
    "# ðŸ”® Step 5: Making Predictions\n",
    "# Predict the next value for a new sequence\n",
    "with torch.no_grad():\n",
    "    test_seq = torch.tensor([[96, 97, 98, 99, 100]], dtype=torch.float32).unsqueeze(-1)\n",
    "    prediction = model(test_seq)\n",
    "    print(f\"Predicted next number: {prediction.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the LSTM using SSA data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ“¦ Step 1: Reshape Input for LSTM\n",
    "\n",
    "We need to first standardise the data, then reshape the input.\n",
    "\n",
    "LSTM expects input in the shape ``(batch_size, seq_len, num_features)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.96668072]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [11.26942767]\n",
      "  [-0.08873565]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [-0.40815408]\n",
      "  [-0.27320156]\n",
      "  ...\n",
      "  [-0.06262243]\n",
      "  [-0.08873565]\n",
      "  [-0.08873565]]]\n",
      "X_train shape: (256, 144, 1)\n",
      "y_train shape: (256,)\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2) # we must define split_val_size here to get a validation set\n",
    "# Standardize the data \n",
    "# If your input features are too large (e.g., >1000) or too small (<0.0001), it can cause unstable training, so it's better to standardize the data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Reshape input for LSTM, LSTM expects input in the shape (batch_size, seq_len, num_features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(X_train)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§± Step 2: Convert to PyTorch Tensors and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§  Step 3: Initialize and Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/50], Loss: 0.6901, Train Acc: 0.5234\n",
      "Validation Acc: 0.4844\n",
      "Epoch [2/50], Loss: 0.6916, Train Acc: 0.5312\n",
      "Validation Acc: 0.5000\n",
      "Epoch [3/50], Loss: 0.6877, Train Acc: 0.5352\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/10).\n",
      "Epoch [4/50], Loss: 0.6932, Train Acc: 0.4805\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [5/50], Loss: 0.6909, Train Acc: 0.5195\n",
      "Validation Acc: 0.5000\n",
      "No improvement (3/10).\n",
      "Epoch [6/50], Loss: 0.6837, Train Acc: 0.5391\n",
      "Validation Acc: 0.4844\n",
      "No improvement (4/10).\n",
      "Epoch [7/50], Loss: 0.6831, Train Acc: 0.5312\n",
      "Validation Acc: 0.4844\n",
      "No improvement (5/10).\n",
      "Epoch [8/50], Loss: 0.6751, Train Acc: 0.5547\n",
      "Validation Acc: 0.4844\n",
      "No improvement (6/10).\n",
      "Epoch [9/50], Loss: 0.6639, Train Acc: 0.5625\n",
      "Validation Acc: 0.5312\n",
      "Epoch [10/50], Loss: 0.6495, Train Acc: 0.6055\n",
      "Validation Acc: 0.5312\n",
      "No improvement (1/10).\n",
      "Epoch [11/50], Loss: 0.6707, Train Acc: 0.6289\n",
      "Validation Acc: 0.5625\n",
      "Epoch [12/50], Loss: 0.6536, Train Acc: 0.6250\n",
      "Validation Acc: 0.5312\n",
      "No improvement (1/10).\n",
      "Epoch [13/50], Loss: 0.6334, Train Acc: 0.6328\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [14/50], Loss: 0.6422, Train Acc: 0.6328\n",
      "Validation Acc: 0.5312\n",
      "No improvement (3/10).\n",
      "Epoch [15/50], Loss: 0.6668, Train Acc: 0.5742\n",
      "Validation Acc: 0.5312\n",
      "No improvement (4/10).\n",
      "Epoch [16/50], Loss: 0.6618, Train Acc: 0.5664\n",
      "Validation Acc: 0.5312\n",
      "No improvement (5/10).\n",
      "Epoch [17/50], Loss: 0.6591, Train Acc: 0.5742\n",
      "Validation Acc: 0.5312\n",
      "No improvement (6/10).\n",
      "Epoch [18/50], Loss: 0.6572, Train Acc: 0.5742\n",
      "Validation Acc: 0.5000\n",
      "No improvement (7/10).\n",
      "Epoch [19/50], Loss: 0.6553, Train Acc: 0.5703\n",
      "Validation Acc: 0.5156\n",
      "No improvement (8/10).\n",
      "Epoch [20/50], Loss: 0.6556, Train Acc: 0.5781\n",
      "Validation Acc: 0.5312\n",
      "No improvement (9/10).\n",
      "Epoch [21/50], Loss: 0.6499, Train Acc: 0.6094\n",
      "Validation Acc: 0.5156\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMClassifier\n",
    "\n",
    "input_size = X_train.shape[2]  # each time step is a single value\n",
    "hidden_size = 64\n",
    "num_layers = 2 # number of LSTM layers\n",
    "output_size = len(torch.unique(y_train_tensor))  # number of classes\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMClassifier(input_size=input_size,           \n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers, output_size=output_size,\n",
    "                       dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(train_loader, val_loader=val_loader,\n",
    "                            epochs=50, patience=10,\n",
    "                            # save_path='best_lstm_model.pt'\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”® Step 4: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/50], Loss: 0.7000, Train Acc: 0.4922\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/50], Loss: 0.6967, Train Acc: 0.4805\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/10).\n",
      "Epoch [3/50], Loss: 0.6948, Train Acc: 0.4609\n",
      "Validation Acc: 0.5000\n",
      "No improvement (2/10).\n",
      "Epoch [4/50], Loss: 0.6906, Train Acc: 0.5000\n",
      "Validation Acc: 0.5000\n",
      "No improvement (3/10).\n",
      "Epoch [5/50], Loss: 0.6889, Train Acc: 0.4961\n",
      "Validation Acc: 0.5000\n",
      "No improvement (4/10).\n",
      "Epoch [6/50], Loss: 0.6873, Train Acc: 0.4961\n",
      "Validation Acc: 0.4844\n",
      "No improvement (5/10).\n",
      "Epoch [7/50], Loss: 0.6837, Train Acc: 0.5352\n",
      "Validation Acc: 0.4844\n",
      "No improvement (6/10).\n",
      "Epoch [8/50], Loss: 0.6762, Train Acc: 0.5234\n",
      "Validation Acc: 0.4844\n",
      "No improvement (7/10).\n",
      "Epoch [9/50], Loss: 0.6700, Train Acc: 0.5508\n",
      "Validation Acc: 0.4844\n",
      "No improvement (8/10).\n",
      "Epoch [10/50], Loss: 0.6626, Train Acc: 0.5508\n",
      "Validation Acc: 0.5312\n",
      "Epoch [11/50], Loss: 0.6482, Train Acc: 0.6055\n",
      "Validation Acc: 0.5156\n",
      "No improvement (1/10).\n",
      "Epoch [12/50], Loss: 0.6415, Train Acc: 0.6406\n",
      "Validation Acc: 0.5156\n",
      "No improvement (2/10).\n",
      "Epoch [13/50], Loss: 0.6184, Train Acc: 0.6680\n",
      "Validation Acc: 0.5625\n",
      "Epoch [14/50], Loss: 0.5915, Train Acc: 0.6914\n",
      "Validation Acc: 0.5938\n",
      "Epoch [15/50], Loss: 0.5593, Train Acc: 0.7461\n",
      "Validation Acc: 0.6250\n",
      "Epoch [16/50], Loss: 0.5585, Train Acc: 0.7500\n",
      "Validation Acc: 0.6562\n",
      "Epoch [17/50], Loss: 0.6537, Train Acc: 0.6992\n",
      "Validation Acc: 0.7344\n",
      "Epoch [18/50], Loss: 0.5527, Train Acc: 0.7617\n",
      "Validation Acc: 0.6719\n",
      "No improvement (1/10).\n",
      "Epoch [19/50], Loss: 0.5464, Train Acc: 0.7305\n",
      "Validation Acc: 0.6406\n",
      "No improvement (2/10).\n",
      "Epoch [20/50], Loss: 0.5345, Train Acc: 0.7617\n",
      "Validation Acc: 0.6406\n",
      "No improvement (3/10).\n",
      "Epoch [21/50], Loss: 0.5762, Train Acc: 0.7383\n",
      "Validation Acc: 0.6406\n",
      "No improvement (4/10).\n",
      "Epoch [22/50], Loss: 0.5809, Train Acc: 0.7422\n",
      "Validation Acc: 0.6406\n",
      "No improvement (5/10).\n",
      "Epoch [23/50], Loss: 0.5662, Train Acc: 0.7383\n",
      "Validation Acc: 0.6562\n",
      "No improvement (6/10).\n",
      "Epoch [24/50], Loss: 0.5613, Train Acc: 0.7344\n",
      "Validation Acc: 0.6562\n",
      "No improvement (7/10).\n",
      "Epoch [25/50], Loss: 0.5679, Train Acc: 0.7266\n",
      "Validation Acc: 0.6719\n",
      "No improvement (8/10).\n",
      "Epoch [26/50], Loss: 0.5833, Train Acc: 0.7266\n",
      "Validation Acc: 0.6562\n",
      "No improvement (9/10).\n",
      "Epoch [27/50], Loss: 0.5637, Train Acc: 0.7383\n",
      "Validation Acc: 0.6406\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "âœ… Test accuracy: 0.7375\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2) # we must define split_val_size here to get a validation set\n",
    "# Standardize the data \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Reshape input for LSTM, LSTM expects input in the shape (batch_size, seq_len, num_features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "from models.lstm import LSTMClassifier\n",
    "\n",
    "input_size = X_train.shape[2]  # each time step is a single value\n",
    "hidden_size = 64\n",
    "num_layers = 2 # number of LSTM layers\n",
    "output_size = len(torch.unique(y_train_tensor))  # number of classes\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = LSTMClassifier(input_size=input_size, hidden_size=hidden_size, \n",
    "                       num_layers=num_layers, output_size=output_size,\n",
    "                       dropout_rate=dropout_rate, learning_rate=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "history = model.train_model(train_loader, val_loader=val_loader,\n",
    "                            epochs=50, patience=10,\n",
    "                            # save_path='best_lstm_model.pt'\n",
    "                            )\n",
    "# Prepare test data\n",
    "X_test_tensor = torch.tensor(X_test.reshape(X_test.shape[0], X_test.shape[1], 1), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = model.evaluate(test_loader)\n",
    "print(f\"âœ… Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/50], Loss: 0.6920, Train Acc: 0.5156\n",
      "Validation Acc: 0.5000\n",
      "Epoch [2/50], Loss: 0.6928, Train Acc: 0.5156\n",
      "Validation Acc: 0.5000\n",
      "No improvement (1/10).\n",
      "Epoch [3/50], Loss: 0.6926, Train Acc: 0.4727\n",
      "Validation Acc: 0.4844\n",
      "No improvement (2/10).\n",
      "Epoch [4/50], Loss: 0.6932, Train Acc: 0.5039\n",
      "Validation Acc: 0.5000\n",
      "No improvement (3/10).\n",
      "Epoch [5/50], Loss: 0.6894, Train Acc: 0.5352\n",
      "Validation Acc: 0.4844\n",
      "No improvement (4/10).\n",
      "Epoch [6/50], Loss: 0.6900, Train Acc: 0.4961\n",
      "Validation Acc: 0.5000\n",
      "No improvement (5/10).\n",
      "Epoch [7/50], Loss: 0.6862, Train Acc: 0.4688\n",
      "Validation Acc: 0.5000\n",
      "No improvement (6/10).\n",
      "Epoch [8/50], Loss: 0.6770, Train Acc: 0.5195\n",
      "Validation Acc: 0.4844\n",
      "No improvement (7/10).\n",
      "Epoch [9/50], Loss: 0.6704, Train Acc: 0.5625\n",
      "Validation Acc: 0.4844\n",
      "No improvement (8/10).\n",
      "Epoch [10/50], Loss: 0.6578, Train Acc: 0.5703\n",
      "Validation Acc: 0.5312\n",
      "Epoch [11/50], Loss: 0.6541, Train Acc: 0.5977\n",
      "Validation Acc: 0.5156\n",
      "No improvement (1/10).\n",
      "Epoch [12/50], Loss: 0.6834, Train Acc: 0.6680\n",
      "Validation Acc: 0.6250\n",
      "Epoch [13/50], Loss: 0.6672, Train Acc: 0.6914\n",
      "Validation Acc: 0.6406\n",
      "Epoch [14/50], Loss: 0.6308, Train Acc: 0.6992\n",
      "Validation Acc: 0.6406\n",
      "No improvement (1/10).\n",
      "Epoch [15/50], Loss: 0.5662, Train Acc: 0.7383\n",
      "Validation Acc: 0.6719\n",
      "Epoch [16/50], Loss: 0.5777, Train Acc: 0.7422\n",
      "Validation Acc: 0.6875\n",
      "Epoch [17/50], Loss: 0.5328, Train Acc: 0.7773\n",
      "Validation Acc: 0.7500\n",
      "Epoch [18/50], Loss: 0.5531, Train Acc: 0.7578\n",
      "Validation Acc: 0.7500\n",
      "No improvement (1/10).\n",
      "Epoch [19/50], Loss: 0.5461, Train Acc: 0.7578\n",
      "Validation Acc: 0.7500\n",
      "No improvement (2/10).\n",
      "Epoch [20/50], Loss: 0.5644, Train Acc: 0.7539\n",
      "Validation Acc: 0.7344\n",
      "No improvement (3/10).\n",
      "Epoch [21/50], Loss: 0.5751, Train Acc: 0.7266\n",
      "Validation Acc: 0.7344\n",
      "No improvement (4/10).\n",
      "Epoch [22/50], Loss: 0.5680, Train Acc: 0.7461\n",
      "Validation Acc: 0.7500\n",
      "No improvement (5/10).\n",
      "Epoch [23/50], Loss: 0.5482, Train Acc: 0.7578\n",
      "Validation Acc: 0.6719\n",
      "No improvement (6/10).\n",
      "Epoch [24/50], Loss: 0.5437, Train Acc: 0.7617\n",
      "Validation Acc: 0.6719\n",
      "No improvement (7/10).\n",
      "Epoch [25/50], Loss: 0.5478, Train Acc: 0.7656\n",
      "Validation Acc: 0.6719\n",
      "No improvement (8/10).\n",
      "Epoch [26/50], Loss: 0.5572, Train Acc: 0.7578\n",
      "Validation Acc: 0.6719\n",
      "No improvement (9/10).\n",
      "Epoch [27/50], Loss: 0.5543, Train Acc: 0.7578\n",
      "Validation Acc: 0.6719\n",
      "No improvement (10/10).\n",
      "Stopping early! No improvement for 10 epochs.\n",
      "Training complete!\n",
      "=== Long-Short Term Memory (LSTM) Accuracy: 0.75 ===\n"
     ]
    }
   ],
   "source": [
    "from classifiers.lstm_classifer import lstm_classifier\n",
    "\n",
    "# Train SVM model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(output_file, split_val_size=0.2) # we must define split_val_size here to get a validation set\n",
    "lstm_accuracy = lstm_classifier(X_train, X_val, X_test, y_train, y_val, y_test, epochs=50, bidirectional=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
