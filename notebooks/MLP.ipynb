{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)\n",
    "This notebook builts an MLP for classification, same way as described in [Cepeda Humerez et al. (2019)](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007290)\n",
    "\n",
    "Hyperparameters to use:\n",
    "\n",
    "````python\n",
    "input_size = 200  # Adjust based on dataset\n",
    "hidden_size = [300, 200] # 300 and 200 LTUs\n",
    "output_size = 2  # Number of classes\n",
    "dropout_rate = 0.5 # This wasn't specified in the paper, but choose any\n",
    "learning_rate = 0.001 # Not specified in the paper\n",
    "epochs = 100 # Not specified in the paper\n",
    "batch_size = 16 # Not specified in the paper\n",
    "````\n",
    "\n",
    "The model architecture is in ``MLP.py``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MLP model codes from ``src``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tqdm\n",
    "from sympy import sqrt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Import all the functions from the 'src' directory, we import all the functions from each module so we can use them straight away\n",
    "from ssa_simulation import *\n",
    "from ssa_analysis import *\n",
    "from ssa_classification import *\n",
    "from models.MLP import MLP \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage to train an MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/100], Loss: 107.0392, Train Acc: 0.5150\n",
      "Validation Acc: 0.4900\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.4900)\n",
      "Epoch [2/100], Loss: 73.6615, Train Acc: 0.5730\n",
      "Validation Acc: 0.4450\n",
      "Epoch [3/100], Loss: 64.6390, Train Acc: 0.5790\n",
      "Validation Acc: 0.4850\n",
      "Epoch [4/100], Loss: 55.5709, Train Acc: 0.6180\n",
      "Validation Acc: 0.5000\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5000)\n",
      "Epoch [5/100], Loss: 51.5685, Train Acc: 0.6100\n",
      "Validation Acc: 0.5050\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5050)\n",
      "Epoch [6/100], Loss: 46.3775, Train Acc: 0.6460\n",
      "Validation Acc: 0.5050\n",
      "Epoch [7/100], Loss: 42.6653, Train Acc: 0.6640\n",
      "Validation Acc: 0.5200\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5200)\n",
      "Epoch [8/100], Loss: 39.5790, Train Acc: 0.6890\n",
      "Validation Acc: 0.5150\n",
      "Epoch [9/100], Loss: 37.8717, Train Acc: 0.7020\n",
      "Validation Acc: 0.5250\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5250)\n",
      "Epoch [10/100], Loss: 39.0586, Train Acc: 0.6750\n",
      "Validation Acc: 0.5300\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5300)\n",
      "Epoch [11/100], Loss: 36.4473, Train Acc: 0.7020\n",
      "Validation Acc: 0.5150\n",
      "Epoch [12/100], Loss: 34.6931, Train Acc: 0.7250\n",
      "Validation Acc: 0.5000\n",
      "Epoch [13/100], Loss: 33.3116, Train Acc: 0.7480\n",
      "Validation Acc: 0.5150\n",
      "Epoch [14/100], Loss: 32.0265, Train Acc: 0.7470\n",
      "Validation Acc: 0.5050\n",
      "Epoch [15/100], Loss: 30.9832, Train Acc: 0.7640\n",
      "Validation Acc: 0.4800\n",
      "Epoch [16/100], Loss: 30.7144, Train Acc: 0.7600\n",
      "Validation Acc: 0.4800\n",
      "Epoch [17/100], Loss: 29.4015, Train Acc: 0.7730\n",
      "Validation Acc: 0.4800\n",
      "Epoch [18/100], Loss: 30.0931, Train Acc: 0.7680\n",
      "Validation Acc: 0.5150\n",
      "Epoch [19/100], Loss: 26.9268, Train Acc: 0.7970\n",
      "Validation Acc: 0.4700\n",
      "Epoch [20/100], Loss: 24.4480, Train Acc: 0.8330\n",
      "Validation Acc: 0.4650\n",
      "Epoch [21/100], Loss: 24.4932, Train Acc: 0.8150\n",
      "Validation Acc: 0.4950\n",
      "Epoch [22/100], Loss: 25.1413, Train Acc: 0.8090\n",
      "Validation Acc: 0.5000\n",
      "Epoch [23/100], Loss: 24.2090, Train Acc: 0.8270\n",
      "Validation Acc: 0.4750\n",
      "Epoch [24/100], Loss: 21.0383, Train Acc: 0.8440\n",
      "Validation Acc: 0.5050\n",
      "Epoch [25/100], Loss: 20.9737, Train Acc: 0.8460\n",
      "Validation Acc: 0.5300\n",
      "Epoch [26/100], Loss: 18.8622, Train Acc: 0.8670\n",
      "Validation Acc: 0.5350\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5350)\n",
      "Epoch [27/100], Loss: 19.8796, Train Acc: 0.8590\n",
      "Validation Acc: 0.5050\n",
      "Epoch [28/100], Loss: 18.6057, Train Acc: 0.8710\n",
      "Validation Acc: 0.5200\n",
      "Epoch [29/100], Loss: 16.7614, Train Acc: 0.8890\n",
      "Validation Acc: 0.5300\n",
      "Epoch [30/100], Loss: 16.9807, Train Acc: 0.8870\n",
      "Validation Acc: 0.5050\n",
      "Epoch [31/100], Loss: 14.6251, Train Acc: 0.9110\n",
      "Validation Acc: 0.5100\n",
      "Epoch [32/100], Loss: 14.4448, Train Acc: 0.8960\n",
      "Validation Acc: 0.5150\n",
      "Epoch [33/100], Loss: 14.2927, Train Acc: 0.9100\n",
      "Validation Acc: 0.5200\n",
      "Epoch [34/100], Loss: 14.4845, Train Acc: 0.9020\n",
      "Validation Acc: 0.5050\n",
      "Epoch [35/100], Loss: 12.4742, Train Acc: 0.9200\n",
      "Validation Acc: 0.5150\n",
      "Epoch [36/100], Loss: 10.8482, Train Acc: 0.9320\n",
      "Validation Acc: 0.5200\n",
      "Epoch [37/100], Loss: 12.0533, Train Acc: 0.9270\n",
      "Validation Acc: 0.5150\n",
      "Epoch [38/100], Loss: 12.5470, Train Acc: 0.9220\n",
      "Validation Acc: 0.4950\n",
      "Epoch [39/100], Loss: 8.7010, Train Acc: 0.9490\n",
      "Validation Acc: 0.5050\n",
      "Epoch [40/100], Loss: 10.9794, Train Acc: 0.9350\n",
      "Validation Acc: 0.4750\n",
      "Epoch [41/100], Loss: 10.2493, Train Acc: 0.9380\n",
      "Validation Acc: 0.5200\n",
      "Epoch [42/100], Loss: 11.1448, Train Acc: 0.9350\n",
      "Validation Acc: 0.5000\n",
      "Epoch [43/100], Loss: 10.0140, Train Acc: 0.9400\n",
      "Validation Acc: 0.5300\n",
      "Epoch [44/100], Loss: 10.1827, Train Acc: 0.9370\n",
      "Validation Acc: 0.5250\n",
      "Epoch [45/100], Loss: 7.4697, Train Acc: 0.9510\n",
      "Validation Acc: 0.5250\n",
      "Epoch [46/100], Loss: 7.3183, Train Acc: 0.9610\n",
      "Validation Acc: 0.5250\n",
      "Epoch [47/100], Loss: 6.5670, Train Acc: 0.9610\n",
      "Validation Acc: 0.5100\n",
      "Epoch [48/100], Loss: 6.8341, Train Acc: 0.9580\n",
      "Validation Acc: 0.5050\n",
      "Epoch [49/100], Loss: 5.0633, Train Acc: 0.9700\n",
      "Validation Acc: 0.5100\n",
      "Epoch [50/100], Loss: 7.4021, Train Acc: 0.9490\n",
      "Validation Acc: 0.5050\n",
      "Epoch [51/100], Loss: 7.0981, Train Acc: 0.9590\n",
      "Validation Acc: 0.4950\n",
      "Epoch [52/100], Loss: 5.8042, Train Acc: 0.9660\n",
      "Validation Acc: 0.4750\n",
      "Epoch [53/100], Loss: 5.0748, Train Acc: 0.9710\n",
      "Validation Acc: 0.5250\n",
      "Epoch [54/100], Loss: 6.9696, Train Acc: 0.9530\n",
      "Validation Acc: 0.5200\n",
      "Epoch [55/100], Loss: 5.5672, Train Acc: 0.9700\n",
      "Validation Acc: 0.5200\n",
      "Epoch [56/100], Loss: 5.2852, Train Acc: 0.9670\n",
      "Validation Acc: 0.5050\n",
      "Epoch [57/100], Loss: 6.7451, Train Acc: 0.9630\n",
      "Validation Acc: 0.5150\n",
      "Epoch [58/100], Loss: 3.9699, Train Acc: 0.9750\n",
      "Validation Acc: 0.5050\n",
      "Epoch [59/100], Loss: 5.0183, Train Acc: 0.9710\n",
      "Validation Acc: 0.4950\n",
      "Epoch [60/100], Loss: 7.3786, Train Acc: 0.9490\n",
      "Validation Acc: 0.5150\n",
      "Epoch [61/100], Loss: 5.4685, Train Acc: 0.9620\n",
      "Validation Acc: 0.5200\n",
      "Epoch [62/100], Loss: 4.6255, Train Acc: 0.9720\n",
      "Validation Acc: 0.5200\n",
      "Epoch [63/100], Loss: 3.6262, Train Acc: 0.9790\n",
      "Validation Acc: 0.4950\n",
      "Epoch [64/100], Loss: 3.9739, Train Acc: 0.9740\n",
      "Validation Acc: 0.5200\n",
      "Epoch [65/100], Loss: 4.1906, Train Acc: 0.9770\n",
      "Validation Acc: 0.5050\n",
      "Epoch [66/100], Loss: 3.4607, Train Acc: 0.9810\n",
      "Validation Acc: 0.5050\n",
      "Epoch [67/100], Loss: 4.2205, Train Acc: 0.9750\n",
      "Validation Acc: 0.5350\n",
      "Epoch [68/100], Loss: 5.1773, Train Acc: 0.9700\n",
      "Validation Acc: 0.5400\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5400)\n",
      "Epoch [69/100], Loss: 4.2349, Train Acc: 0.9720\n",
      "Validation Acc: 0.5300\n",
      "Epoch [70/100], Loss: 3.8255, Train Acc: 0.9760\n",
      "Validation Acc: 0.5000\n",
      "Epoch [71/100], Loss: 4.6926, Train Acc: 0.9750\n",
      "Validation Acc: 0.5100\n",
      "Epoch [72/100], Loss: 3.6377, Train Acc: 0.9760\n",
      "Validation Acc: 0.5050\n",
      "Epoch [73/100], Loss: 3.7004, Train Acc: 0.9750\n",
      "Validation Acc: 0.5100\n",
      "Epoch [74/100], Loss: 4.3944, Train Acc: 0.9750\n",
      "Validation Acc: 0.5150\n",
      "Epoch [75/100], Loss: 3.3407, Train Acc: 0.9790\n",
      "Validation Acc: 0.5150\n",
      "Epoch [76/100], Loss: 4.4395, Train Acc: 0.9690\n",
      "Validation Acc: 0.5200\n",
      "Epoch [77/100], Loss: 3.7270, Train Acc: 0.9750\n",
      "Validation Acc: 0.5100\n",
      "Epoch [78/100], Loss: 4.4881, Train Acc: 0.9670\n",
      "Validation Acc: 0.5150\n",
      "Epoch [79/100], Loss: 3.4778, Train Acc: 0.9800\n",
      "Validation Acc: 0.5250\n",
      "Epoch [80/100], Loss: 4.9737, Train Acc: 0.9740\n",
      "Validation Acc: 0.4950\n",
      "Epoch [81/100], Loss: 3.9902, Train Acc: 0.9820\n",
      "Validation Acc: 0.5150\n",
      "Epoch [82/100], Loss: 4.2097, Train Acc: 0.9790\n",
      "Validation Acc: 0.5250\n",
      "Epoch [83/100], Loss: 2.6032, Train Acc: 0.9860\n",
      "Validation Acc: 0.5200\n",
      "Epoch [84/100], Loss: 3.8238, Train Acc: 0.9830\n",
      "Validation Acc: 0.5050\n",
      "Epoch [85/100], Loss: 3.1995, Train Acc: 0.9830\n",
      "Validation Acc: 0.5300\n",
      "Epoch [86/100], Loss: 2.5207, Train Acc: 0.9880\n",
      "Validation Acc: 0.5500\n",
      "âœ… Model saved at mlp_model.pth (Best Validation Acc: 0.5500)\n",
      "Epoch [87/100], Loss: 2.4619, Train Acc: 0.9830\n",
      "Validation Acc: 0.5500\n",
      "Epoch [88/100], Loss: 3.0557, Train Acc: 0.9860\n",
      "Validation Acc: 0.5400\n",
      "Epoch [89/100], Loss: 2.0025, Train Acc: 0.9890\n",
      "Validation Acc: 0.5450\n",
      "Epoch [90/100], Loss: 2.9807, Train Acc: 0.9830\n",
      "Validation Acc: 0.5300\n",
      "Epoch [91/100], Loss: 2.4889, Train Acc: 0.9790\n",
      "Validation Acc: 0.5300\n",
      "Epoch [92/100], Loss: 3.5552, Train Acc: 0.9820\n",
      "Validation Acc: 0.5150\n",
      "Epoch [93/100], Loss: 2.8103, Train Acc: 0.9850\n",
      "Validation Acc: 0.5500\n",
      "Epoch [94/100], Loss: 2.7147, Train Acc: 0.9880\n",
      "Validation Acc: 0.5400\n",
      "Epoch [95/100], Loss: 3.2479, Train Acc: 0.9840\n",
      "Validation Acc: 0.5200\n",
      "Epoch [96/100], Loss: 2.6289, Train Acc: 0.9870\n",
      "Validation Acc: 0.5400\n",
      "Epoch [97/100], Loss: 5.1999, Train Acc: 0.9690\n",
      "Validation Acc: 0.5250\n",
      "Epoch [98/100], Loss: 2.9839, Train Acc: 0.9810\n",
      "Validation Acc: 0.4950\n",
      "Epoch [99/100], Loss: 3.5411, Train Acc: 0.9760\n",
      "Validation Acc: 0.5150\n",
      "Epoch [100/100], Loss: 2.0575, Train Acc: 0.9890\n",
      "Validation Acc: 0.5200\n",
      "Training complete!\n",
      "ðŸ”„ Model loaded from mlp_model.pth\n",
      "Final Test Accuracy: 0.5500\n",
      "Predicted classes: tensor([1, 1, 1, 1, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 200  # Adjust based on dataset\n",
    "hidden_size = [300, 200]\n",
    "output_size = 2  # Number of classes\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Generate synthetic data\n",
    "X_train = torch.randn(1000, input_size)\n",
    "y_train = torch.randint(0, output_size, (1000,))\n",
    "X_val = torch.randn(200, input_size)\n",
    "y_val = torch.randint(0, output_size, (200,))\n",
    "\n",
    "# Convert to DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize and train model\n",
    "file_path = \"mlp_model.pth\"\n",
    "model = MLP(input_size, hidden_size, output_size, dropout_rate, learning_rate)\n",
    "model.train_model(train_loader, val_loader, epochs, save_path=file_path)\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_model(file_path)\n",
    "test_acc = model.evaluate(val_loader)\n",
    "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "X_test = torch.randn(5, input_size)\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Predicted classes:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the MLP using SSA data, we need to first standardise the data. If we don't, the loss will be showing as nan, which is incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Train MLP model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_test, y_train, y_test = load_and_split_data(output_file)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.4307749  -0.30161953 ... -0.05598925 -0.09728167\n",
      "  -0.07930516]\n",
      " [ 0.         -0.4307749  -0.30161953 ... -0.05598925 -0.09728167\n",
      "  -0.07930516]\n",
      " [ 0.         -0.4307749  -0.30161953 ... -0.05598925 -0.09728167\n",
      "  -0.07930516]\n",
      " ...\n",
      " [ 0.          1.00514142 -0.30161953 ... -0.05598925 -0.09728167\n",
      "  -0.07930516]\n",
      " [ 0.          2.44105774 -0.30161953 ... -0.05598925 -0.09728167\n",
      "  -0.07930516]\n",
      " [ 0.         -0.4307749  -0.30161953 ... -0.05598925 -0.09728167\n",
      "  -0.07930516]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data \n",
    "# If your input features are too large (e.g., >1000) or too small (<0.0001), it can cause unstable training, so it's better to standardize the data.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/1000], Loss: 16.7488, Train Acc: 0.6031\n",
      "Epoch [2/1000], Loss: 8.7939, Train Acc: 0.7125\n",
      "Epoch [3/1000], Loss: 7.7633, Train Acc: 0.7188\n",
      "Epoch [4/1000], Loss: 7.1606, Train Acc: 0.7469\n",
      "Epoch [5/1000], Loss: 6.3700, Train Acc: 0.7469\n",
      "Epoch [6/1000], Loss: 6.0609, Train Acc: 0.7344\n",
      "Epoch [7/1000], Loss: 6.4363, Train Acc: 0.7469\n",
      "Epoch [8/1000], Loss: 5.0182, Train Acc: 0.7844\n",
      "Epoch [9/1000], Loss: 5.2613, Train Acc: 0.7562\n",
      "Epoch [10/1000], Loss: 5.6556, Train Acc: 0.7781\n",
      "Epoch [11/1000], Loss: 5.8697, Train Acc: 0.7500\n",
      "Epoch [12/1000], Loss: 5.4959, Train Acc: 0.7750\n",
      "Epoch [13/1000], Loss: 5.4651, Train Acc: 0.7562\n",
      "Epoch [14/1000], Loss: 5.8432, Train Acc: 0.7750\n",
      "Epoch [15/1000], Loss: 4.6044, Train Acc: 0.7906\n",
      "Epoch [16/1000], Loss: 5.4678, Train Acc: 0.7656\n",
      "Epoch [17/1000], Loss: 4.7093, Train Acc: 0.8031\n",
      "Epoch [18/1000], Loss: 5.1844, Train Acc: 0.7875\n",
      "Epoch [19/1000], Loss: 4.9215, Train Acc: 0.7875\n",
      "Epoch [20/1000], Loss: 4.9662, Train Acc: 0.7688\n",
      "Epoch [21/1000], Loss: 5.1186, Train Acc: 0.7750\n",
      "Epoch [22/1000], Loss: 4.6871, Train Acc: 0.8063\n",
      "Epoch [23/1000], Loss: 4.5676, Train Acc: 0.7812\n",
      "Epoch [24/1000], Loss: 4.8924, Train Acc: 0.7937\n",
      "Epoch [25/1000], Loss: 4.6998, Train Acc: 0.7719\n",
      "Epoch [26/1000], Loss: 4.0994, Train Acc: 0.8125\n",
      "Epoch [27/1000], Loss: 4.9028, Train Acc: 0.7875\n",
      "Epoch [28/1000], Loss: 4.9542, Train Acc: 0.7812\n",
      "Epoch [29/1000], Loss: 4.2584, Train Acc: 0.8187\n",
      "Epoch [30/1000], Loss: 4.1060, Train Acc: 0.8094\n",
      "Epoch [31/1000], Loss: 3.9050, Train Acc: 0.8250\n",
      "Epoch [32/1000], Loss: 4.3625, Train Acc: 0.8031\n",
      "Epoch [33/1000], Loss: 4.7961, Train Acc: 0.8063\n",
      "Epoch [34/1000], Loss: 4.7181, Train Acc: 0.7719\n",
      "Epoch [35/1000], Loss: 4.6836, Train Acc: 0.8063\n",
      "Epoch [36/1000], Loss: 4.2414, Train Acc: 0.8000\n",
      "Epoch [37/1000], Loss: 4.0239, Train Acc: 0.8125\n",
      "Epoch [38/1000], Loss: 4.5454, Train Acc: 0.7844\n",
      "Epoch [39/1000], Loss: 4.4501, Train Acc: 0.8000\n",
      "Epoch [40/1000], Loss: 4.1566, Train Acc: 0.8031\n",
      "Epoch [41/1000], Loss: 4.0916, Train Acc: 0.8031\n",
      "Epoch [42/1000], Loss: 4.2730, Train Acc: 0.8094\n",
      "Epoch [43/1000], Loss: 3.9643, Train Acc: 0.8187\n",
      "Epoch [44/1000], Loss: 3.9176, Train Acc: 0.8094\n",
      "Epoch [45/1000], Loss: 3.7886, Train Acc: 0.8250\n",
      "Epoch [46/1000], Loss: 4.1026, Train Acc: 0.7969\n",
      "Epoch [47/1000], Loss: 4.0366, Train Acc: 0.8187\n",
      "Epoch [48/1000], Loss: 3.9278, Train Acc: 0.8094\n",
      "Epoch [49/1000], Loss: 4.2127, Train Acc: 0.8063\n",
      "Epoch [50/1000], Loss: 4.1935, Train Acc: 0.7969\n",
      "Epoch [51/1000], Loss: 4.3822, Train Acc: 0.8156\n",
      "Epoch [52/1000], Loss: 4.1247, Train Acc: 0.8094\n",
      "Epoch [53/1000], Loss: 4.2330, Train Acc: 0.8063\n",
      "Epoch [54/1000], Loss: 3.9778, Train Acc: 0.8250\n",
      "Epoch [55/1000], Loss: 3.9186, Train Acc: 0.8219\n",
      "Epoch [56/1000], Loss: 4.3546, Train Acc: 0.8000\n",
      "Epoch [57/1000], Loss: 4.0056, Train Acc: 0.8219\n",
      "Epoch [58/1000], Loss: 4.2022, Train Acc: 0.8187\n",
      "Epoch [59/1000], Loss: 4.3843, Train Acc: 0.8000\n",
      "Epoch [60/1000], Loss: 4.2049, Train Acc: 0.8063\n",
      "Epoch [61/1000], Loss: 4.1405, Train Acc: 0.8094\n",
      "Epoch [62/1000], Loss: 3.8603, Train Acc: 0.8187\n",
      "Epoch [63/1000], Loss: 4.1775, Train Acc: 0.8187\n",
      "Epoch [64/1000], Loss: 3.8547, Train Acc: 0.8063\n",
      "Epoch [65/1000], Loss: 4.0605, Train Acc: 0.8094\n",
      "Epoch [66/1000], Loss: 4.1356, Train Acc: 0.8031\n",
      "Epoch [67/1000], Loss: 3.9950, Train Acc: 0.8281\n",
      "Epoch [68/1000], Loss: 4.0364, Train Acc: 0.8063\n",
      "Epoch [69/1000], Loss: 3.9573, Train Acc: 0.8125\n",
      "Epoch [70/1000], Loss: 4.0715, Train Acc: 0.7875\n",
      "Epoch [71/1000], Loss: 4.0162, Train Acc: 0.8156\n",
      "Epoch [72/1000], Loss: 3.9849, Train Acc: 0.8219\n",
      "Epoch [73/1000], Loss: 4.4040, Train Acc: 0.7906\n",
      "Epoch [74/1000], Loss: 3.8636, Train Acc: 0.8219\n",
      "Epoch [75/1000], Loss: 4.1420, Train Acc: 0.8125\n",
      "Epoch [76/1000], Loss: 3.8387, Train Acc: 0.8219\n",
      "Epoch [77/1000], Loss: 3.7239, Train Acc: 0.8344\n",
      "Epoch [78/1000], Loss: 4.2471, Train Acc: 0.8000\n",
      "Epoch [79/1000], Loss: 4.0617, Train Acc: 0.8094\n",
      "Epoch [80/1000], Loss: 3.7799, Train Acc: 0.8250\n",
      "Epoch [81/1000], Loss: 4.2484, Train Acc: 0.8000\n",
      "Epoch [82/1000], Loss: 4.0232, Train Acc: 0.8187\n",
      "Epoch [83/1000], Loss: 3.9789, Train Acc: 0.8000\n",
      "Epoch [84/1000], Loss: 4.0787, Train Acc: 0.8125\n",
      "Epoch [85/1000], Loss: 4.1630, Train Acc: 0.8063\n",
      "Epoch [86/1000], Loss: 3.7168, Train Acc: 0.8281\n",
      "Epoch [87/1000], Loss: 4.1499, Train Acc: 0.8000\n",
      "Epoch [88/1000], Loss: 4.1377, Train Acc: 0.7906\n",
      "Epoch [89/1000], Loss: 3.9323, Train Acc: 0.8031\n",
      "Epoch [90/1000], Loss: 4.0384, Train Acc: 0.8125\n",
      "Epoch [91/1000], Loss: 3.9063, Train Acc: 0.8094\n",
      "Epoch [92/1000], Loss: 4.1355, Train Acc: 0.8000\n",
      "Epoch [93/1000], Loss: 4.0070, Train Acc: 0.8063\n",
      "Epoch [94/1000], Loss: 3.7434, Train Acc: 0.8125\n",
      "Epoch [95/1000], Loss: 3.8310, Train Acc: 0.8219\n",
      "Epoch [96/1000], Loss: 3.6849, Train Acc: 0.8406\n",
      "Epoch [97/1000], Loss: 4.1623, Train Acc: 0.8063\n",
      "Epoch [98/1000], Loss: 3.8145, Train Acc: 0.8031\n",
      "Epoch [99/1000], Loss: 3.9519, Train Acc: 0.8156\n",
      "Epoch [100/1000], Loss: 4.1018, Train Acc: 0.8000\n",
      "Epoch [101/1000], Loss: 3.9129, Train Acc: 0.8187\n",
      "Epoch [102/1000], Loss: 4.0028, Train Acc: 0.8187\n",
      "Epoch [103/1000], Loss: 3.7297, Train Acc: 0.8250\n",
      "Epoch [104/1000], Loss: 3.9499, Train Acc: 0.8125\n",
      "Epoch [105/1000], Loss: 3.9138, Train Acc: 0.8219\n",
      "Epoch [106/1000], Loss: 3.8824, Train Acc: 0.8125\n",
      "Epoch [107/1000], Loss: 3.8491, Train Acc: 0.8187\n",
      "Epoch [108/1000], Loss: 3.8564, Train Acc: 0.8031\n",
      "Epoch [109/1000], Loss: 3.9541, Train Acc: 0.8000\n",
      "Epoch [110/1000], Loss: 3.5872, Train Acc: 0.8313\n",
      "Epoch [111/1000], Loss: 3.8481, Train Acc: 0.8063\n",
      "Epoch [112/1000], Loss: 3.9422, Train Acc: 0.8063\n",
      "Epoch [113/1000], Loss: 3.7763, Train Acc: 0.8250\n",
      "Epoch [114/1000], Loss: 3.7720, Train Acc: 0.8281\n",
      "Epoch [115/1000], Loss: 3.6566, Train Acc: 0.8125\n",
      "Epoch [116/1000], Loss: 3.6598, Train Acc: 0.8250\n",
      "Epoch [117/1000], Loss: 3.8327, Train Acc: 0.8187\n",
      "Epoch [118/1000], Loss: 3.8459, Train Acc: 0.8125\n",
      "Epoch [119/1000], Loss: 3.7700, Train Acc: 0.8250\n",
      "Epoch [120/1000], Loss: 3.8860, Train Acc: 0.8156\n",
      "Epoch [121/1000], Loss: 3.8762, Train Acc: 0.8219\n",
      "Epoch [122/1000], Loss: 3.6867, Train Acc: 0.8250\n",
      "Epoch [123/1000], Loss: 3.7199, Train Acc: 0.8281\n",
      "Epoch [124/1000], Loss: 3.7818, Train Acc: 0.8250\n",
      "Epoch [125/1000], Loss: 3.9029, Train Acc: 0.8187\n",
      "Epoch [126/1000], Loss: 3.6345, Train Acc: 0.8313\n",
      "Epoch [127/1000], Loss: 3.6597, Train Acc: 0.8281\n",
      "Epoch [128/1000], Loss: 3.7009, Train Acc: 0.8281\n",
      "Epoch [129/1000], Loss: 3.8495, Train Acc: 0.8000\n",
      "Epoch [130/1000], Loss: 3.8172, Train Acc: 0.8187\n",
      "Epoch [131/1000], Loss: 3.8527, Train Acc: 0.8125\n",
      "Epoch [132/1000], Loss: 3.6461, Train Acc: 0.8469\n",
      "Epoch [133/1000], Loss: 3.6847, Train Acc: 0.8250\n",
      "Epoch [134/1000], Loss: 3.9061, Train Acc: 0.8250\n",
      "Epoch [135/1000], Loss: 3.9172, Train Acc: 0.8250\n",
      "Epoch [136/1000], Loss: 3.7625, Train Acc: 0.8344\n",
      "Epoch [137/1000], Loss: 3.7866, Train Acc: 0.8094\n",
      "Epoch [138/1000], Loss: 3.8509, Train Acc: 0.8063\n",
      "Epoch [139/1000], Loss: 3.9043, Train Acc: 0.8375\n",
      "Epoch [140/1000], Loss: 3.7151, Train Acc: 0.8281\n",
      "Epoch [141/1000], Loss: 3.9890, Train Acc: 0.8094\n",
      "Epoch [142/1000], Loss: 3.5425, Train Acc: 0.8313\n",
      "Epoch [143/1000], Loss: 3.7736, Train Acc: 0.8125\n",
      "Epoch [144/1000], Loss: 3.7163, Train Acc: 0.8250\n",
      "Epoch [145/1000], Loss: 3.8658, Train Acc: 0.8219\n",
      "Epoch [146/1000], Loss: 3.8072, Train Acc: 0.8187\n",
      "Epoch [147/1000], Loss: 3.6878, Train Acc: 0.8219\n",
      "Epoch [148/1000], Loss: 3.9503, Train Acc: 0.8281\n",
      "Epoch [149/1000], Loss: 3.8397, Train Acc: 0.8313\n",
      "Epoch [150/1000], Loss: 3.9777, Train Acc: 0.8094\n",
      "Epoch [151/1000], Loss: 3.6190, Train Acc: 0.8281\n",
      "Epoch [152/1000], Loss: 3.7822, Train Acc: 0.8156\n",
      "Epoch [153/1000], Loss: 3.5877, Train Acc: 0.8250\n",
      "Epoch [154/1000], Loss: 3.7085, Train Acc: 0.8156\n",
      "Epoch [155/1000], Loss: 3.7770, Train Acc: 0.8250\n",
      "Epoch [156/1000], Loss: 3.7008, Train Acc: 0.8281\n",
      "Epoch [157/1000], Loss: 3.7149, Train Acc: 0.8156\n",
      "Epoch [158/1000], Loss: 3.7126, Train Acc: 0.8219\n",
      "Epoch [159/1000], Loss: 3.7696, Train Acc: 0.8094\n",
      "Epoch [160/1000], Loss: 3.9149, Train Acc: 0.8219\n",
      "Epoch [161/1000], Loss: 3.7786, Train Acc: 0.8125\n",
      "Epoch [162/1000], Loss: 3.9253, Train Acc: 0.8156\n",
      "Epoch [163/1000], Loss: 3.8880, Train Acc: 0.8156\n",
      "Epoch [164/1000], Loss: 3.7533, Train Acc: 0.8250\n",
      "Epoch [165/1000], Loss: 3.9600, Train Acc: 0.8187\n",
      "Epoch [166/1000], Loss: 3.6624, Train Acc: 0.8375\n",
      "Epoch [167/1000], Loss: 3.6853, Train Acc: 0.8313\n",
      "Epoch [168/1000], Loss: 3.6824, Train Acc: 0.8219\n",
      "Epoch [169/1000], Loss: 3.8985, Train Acc: 0.8219\n",
      "Epoch [170/1000], Loss: 3.6946, Train Acc: 0.8219\n",
      "Epoch [171/1000], Loss: 3.8966, Train Acc: 0.8125\n",
      "Epoch [172/1000], Loss: 3.6373, Train Acc: 0.8281\n",
      "Epoch [173/1000], Loss: 3.6810, Train Acc: 0.8219\n",
      "Epoch [174/1000], Loss: 3.7209, Train Acc: 0.8375\n",
      "Epoch [175/1000], Loss: 3.7887, Train Acc: 0.8000\n",
      "Epoch [176/1000], Loss: 3.7015, Train Acc: 0.8187\n",
      "Epoch [177/1000], Loss: 3.8509, Train Acc: 0.8219\n",
      "Epoch [178/1000], Loss: 3.7251, Train Acc: 0.8281\n",
      "Epoch [179/1000], Loss: 3.8419, Train Acc: 0.8219\n",
      "Epoch [180/1000], Loss: 3.6634, Train Acc: 0.8406\n",
      "Epoch [181/1000], Loss: 3.6277, Train Acc: 0.8375\n",
      "Epoch [182/1000], Loss: 3.6201, Train Acc: 0.8219\n",
      "Epoch [183/1000], Loss: 3.7108, Train Acc: 0.8250\n",
      "Epoch [184/1000], Loss: 3.6357, Train Acc: 0.8375\n",
      "Epoch [185/1000], Loss: 3.6601, Train Acc: 0.8094\n",
      "Epoch [186/1000], Loss: 3.7168, Train Acc: 0.8281\n",
      "Epoch [187/1000], Loss: 3.7216, Train Acc: 0.8375\n",
      "Epoch [188/1000], Loss: 3.7157, Train Acc: 0.8281\n",
      "Epoch [189/1000], Loss: 3.7682, Train Acc: 0.8250\n",
      "Epoch [190/1000], Loss: 3.7809, Train Acc: 0.8250\n",
      "Epoch [191/1000], Loss: 3.7259, Train Acc: 0.8313\n",
      "Epoch [192/1000], Loss: 3.6501, Train Acc: 0.8375\n",
      "Epoch [193/1000], Loss: 3.6179, Train Acc: 0.8125\n",
      "Epoch [194/1000], Loss: 3.7526, Train Acc: 0.8156\n",
      "Epoch [195/1000], Loss: 3.6500, Train Acc: 0.8250\n",
      "Epoch [196/1000], Loss: 3.6845, Train Acc: 0.8313\n",
      "Epoch [197/1000], Loss: 3.7366, Train Acc: 0.8281\n",
      "Epoch [198/1000], Loss: 3.7410, Train Acc: 0.8438\n",
      "Epoch [199/1000], Loss: 3.5865, Train Acc: 0.8250\n",
      "Epoch [200/1000], Loss: 3.6361, Train Acc: 0.8344\n",
      "Epoch [201/1000], Loss: 3.6757, Train Acc: 0.8344\n",
      "Epoch [202/1000], Loss: 3.6117, Train Acc: 0.8250\n",
      "Epoch [203/1000], Loss: 3.6050, Train Acc: 0.8313\n",
      "Epoch [204/1000], Loss: 3.5405, Train Acc: 0.8219\n",
      "Epoch [205/1000], Loss: 3.7639, Train Acc: 0.8281\n",
      "Epoch [206/1000], Loss: 3.7071, Train Acc: 0.8281\n",
      "Epoch [207/1000], Loss: 3.5235, Train Acc: 0.8250\n",
      "Epoch [208/1000], Loss: 3.7149, Train Acc: 0.8281\n",
      "Epoch [209/1000], Loss: 3.6668, Train Acc: 0.8187\n",
      "Epoch [210/1000], Loss: 3.7246, Train Acc: 0.8219\n",
      "Epoch [211/1000], Loss: 3.5943, Train Acc: 0.8281\n",
      "Epoch [212/1000], Loss: 3.8058, Train Acc: 0.8219\n",
      "Epoch [213/1000], Loss: 3.6855, Train Acc: 0.8281\n",
      "Epoch [214/1000], Loss: 3.6571, Train Acc: 0.8313\n",
      "Epoch [215/1000], Loss: 3.5814, Train Acc: 0.8281\n",
      "Epoch [216/1000], Loss: 3.6636, Train Acc: 0.8219\n",
      "Epoch [217/1000], Loss: 3.8304, Train Acc: 0.8250\n",
      "Epoch [218/1000], Loss: 3.5994, Train Acc: 0.8187\n",
      "Epoch [219/1000], Loss: 3.8796, Train Acc: 0.8219\n",
      "Epoch [220/1000], Loss: 3.6960, Train Acc: 0.8281\n",
      "Epoch [221/1000], Loss: 3.6185, Train Acc: 0.8250\n",
      "Epoch [222/1000], Loss: 3.9120, Train Acc: 0.8094\n",
      "Epoch [223/1000], Loss: 3.7412, Train Acc: 0.8125\n",
      "Epoch [224/1000], Loss: 3.7948, Train Acc: 0.8156\n",
      "Epoch [225/1000], Loss: 3.5597, Train Acc: 0.8406\n",
      "Epoch [226/1000], Loss: 3.6657, Train Acc: 0.8344\n",
      "Epoch [227/1000], Loss: 3.6625, Train Acc: 0.8250\n",
      "Epoch [228/1000], Loss: 3.6724, Train Acc: 0.8219\n",
      "Epoch [229/1000], Loss: 3.4957, Train Acc: 0.8375\n",
      "Epoch [230/1000], Loss: 3.6093, Train Acc: 0.8125\n",
      "Epoch [231/1000], Loss: 3.8279, Train Acc: 0.8187\n",
      "Epoch [232/1000], Loss: 3.5693, Train Acc: 0.8375\n",
      "Epoch [233/1000], Loss: 3.7002, Train Acc: 0.8375\n",
      "Epoch [234/1000], Loss: 3.5937, Train Acc: 0.8344\n",
      "Epoch [235/1000], Loss: 3.7070, Train Acc: 0.8250\n",
      "Epoch [236/1000], Loss: 3.6358, Train Acc: 0.8281\n",
      "Epoch [237/1000], Loss: 3.6031, Train Acc: 0.8375\n",
      "Epoch [238/1000], Loss: 3.5858, Train Acc: 0.8281\n",
      "Epoch [239/1000], Loss: 3.7832, Train Acc: 0.8250\n",
      "Epoch [240/1000], Loss: 3.8387, Train Acc: 0.8250\n",
      "Epoch [241/1000], Loss: 3.7308, Train Acc: 0.8406\n",
      "Epoch [242/1000], Loss: 3.5474, Train Acc: 0.8344\n",
      "Epoch [243/1000], Loss: 3.6305, Train Acc: 0.8219\n",
      "Epoch [244/1000], Loss: 3.7410, Train Acc: 0.8281\n",
      "Epoch [245/1000], Loss: 3.9107, Train Acc: 0.8125\n",
      "Epoch [246/1000], Loss: 3.7653, Train Acc: 0.8219\n",
      "Epoch [247/1000], Loss: 3.6486, Train Acc: 0.8281\n",
      "Epoch [248/1000], Loss: 3.6975, Train Acc: 0.8125\n",
      "Epoch [249/1000], Loss: 3.7258, Train Acc: 0.8313\n",
      "Epoch [250/1000], Loss: 3.5424, Train Acc: 0.8219\n",
      "Epoch [251/1000], Loss: 3.7512, Train Acc: 0.8250\n",
      "Epoch [252/1000], Loss: 3.8466, Train Acc: 0.8156\n",
      "Epoch [253/1000], Loss: 3.6900, Train Acc: 0.8250\n",
      "Epoch [254/1000], Loss: 3.6229, Train Acc: 0.8313\n",
      "Epoch [255/1000], Loss: 3.6474, Train Acc: 0.8281\n",
      "Epoch [256/1000], Loss: 3.6785, Train Acc: 0.8156\n",
      "Epoch [257/1000], Loss: 3.6124, Train Acc: 0.8156\n",
      "Epoch [258/1000], Loss: 3.6242, Train Acc: 0.8281\n",
      "Epoch [259/1000], Loss: 3.6299, Train Acc: 0.8281\n",
      "Epoch [260/1000], Loss: 3.6559, Train Acc: 0.8187\n",
      "Epoch [261/1000], Loss: 3.7121, Train Acc: 0.8281\n",
      "Epoch [262/1000], Loss: 3.6564, Train Acc: 0.8187\n",
      "Epoch [263/1000], Loss: 3.5855, Train Acc: 0.8313\n",
      "Epoch [264/1000], Loss: 3.5701, Train Acc: 0.8281\n",
      "Epoch [265/1000], Loss: 3.7908, Train Acc: 0.8313\n",
      "Epoch [266/1000], Loss: 3.6149, Train Acc: 0.8406\n",
      "Epoch [267/1000], Loss: 3.6379, Train Acc: 0.8281\n",
      "Epoch [268/1000], Loss: 3.6528, Train Acc: 0.8156\n",
      "Epoch [269/1000], Loss: 3.6686, Train Acc: 0.8187\n",
      "Epoch [270/1000], Loss: 3.6505, Train Acc: 0.8313\n",
      "Epoch [271/1000], Loss: 3.8263, Train Acc: 0.8125\n",
      "Epoch [272/1000], Loss: 3.4926, Train Acc: 0.8375\n",
      "Epoch [273/1000], Loss: 3.6314, Train Acc: 0.8187\n",
      "Epoch [274/1000], Loss: 3.7061, Train Acc: 0.8063\n",
      "Epoch [275/1000], Loss: 3.7054, Train Acc: 0.8219\n",
      "Epoch [276/1000], Loss: 3.7793, Train Acc: 0.8313\n",
      "Epoch [277/1000], Loss: 3.6276, Train Acc: 0.8250\n",
      "Epoch [278/1000], Loss: 3.7170, Train Acc: 0.8219\n",
      "Epoch [279/1000], Loss: 3.6922, Train Acc: 0.8219\n",
      "Epoch [280/1000], Loss: 3.7272, Train Acc: 0.8219\n",
      "Epoch [281/1000], Loss: 3.5973, Train Acc: 0.8156\n",
      "Epoch [282/1000], Loss: 3.6706, Train Acc: 0.8313\n",
      "Epoch [283/1000], Loss: 3.7103, Train Acc: 0.8219\n",
      "Epoch [284/1000], Loss: 3.6427, Train Acc: 0.8125\n",
      "Epoch [285/1000], Loss: 3.6978, Train Acc: 0.8344\n",
      "Epoch [286/1000], Loss: 3.7844, Train Acc: 0.8250\n",
      "Epoch [287/1000], Loss: 3.6263, Train Acc: 0.8281\n",
      "Epoch [288/1000], Loss: 3.7283, Train Acc: 0.8156\n",
      "Epoch [289/1000], Loss: 3.7102, Train Acc: 0.8281\n",
      "Epoch [290/1000], Loss: 3.7503, Train Acc: 0.8156\n",
      "Epoch [291/1000], Loss: 3.7594, Train Acc: 0.8156\n",
      "Epoch [292/1000], Loss: 3.7028, Train Acc: 0.8219\n",
      "Epoch [293/1000], Loss: 3.6176, Train Acc: 0.8375\n",
      "Epoch [294/1000], Loss: 3.6901, Train Acc: 0.8375\n",
      "Epoch [295/1000], Loss: 3.7097, Train Acc: 0.8219\n",
      "Epoch [296/1000], Loss: 3.7610, Train Acc: 0.8219\n",
      "Epoch [297/1000], Loss: 3.5984, Train Acc: 0.8281\n",
      "Epoch [298/1000], Loss: 3.6643, Train Acc: 0.8344\n",
      "Epoch [299/1000], Loss: 3.6717, Train Acc: 0.8281\n",
      "Epoch [300/1000], Loss: 3.6126, Train Acc: 0.8344\n",
      "Epoch [301/1000], Loss: 3.6278, Train Acc: 0.8187\n",
      "Epoch [302/1000], Loss: 3.6497, Train Acc: 0.8313\n",
      "Epoch [303/1000], Loss: 3.6602, Train Acc: 0.8156\n",
      "Epoch [304/1000], Loss: 3.7097, Train Acc: 0.8125\n",
      "Epoch [305/1000], Loss: 3.6234, Train Acc: 0.8250\n",
      "Epoch [306/1000], Loss: 3.6044, Train Acc: 0.8375\n",
      "Epoch [307/1000], Loss: 3.7070, Train Acc: 0.8156\n",
      "Epoch [308/1000], Loss: 3.6798, Train Acc: 0.8187\n",
      "Epoch [309/1000], Loss: 3.6660, Train Acc: 0.8187\n",
      "Epoch [310/1000], Loss: 3.5780, Train Acc: 0.8156\n",
      "Epoch [311/1000], Loss: 3.6827, Train Acc: 0.8281\n",
      "Epoch [312/1000], Loss: 3.5474, Train Acc: 0.8469\n",
      "Epoch [313/1000], Loss: 3.6076, Train Acc: 0.8156\n",
      "Epoch [314/1000], Loss: 3.5992, Train Acc: 0.8219\n",
      "Epoch [315/1000], Loss: 3.6854, Train Acc: 0.8219\n",
      "Epoch [316/1000], Loss: 3.7498, Train Acc: 0.8187\n",
      "Epoch [317/1000], Loss: 3.7736, Train Acc: 0.8094\n",
      "Epoch [318/1000], Loss: 3.7160, Train Acc: 0.8344\n",
      "Epoch [319/1000], Loss: 3.8041, Train Acc: 0.8094\n",
      "Epoch [320/1000], Loss: 3.7063, Train Acc: 0.8313\n",
      "Epoch [321/1000], Loss: 3.6723, Train Acc: 0.8125\n",
      "Epoch [322/1000], Loss: 3.5684, Train Acc: 0.8219\n",
      "Epoch [323/1000], Loss: 3.5669, Train Acc: 0.8250\n",
      "Epoch [324/1000], Loss: 3.6892, Train Acc: 0.8219\n",
      "Epoch [325/1000], Loss: 3.6625, Train Acc: 0.8281\n",
      "Epoch [326/1000], Loss: 3.6168, Train Acc: 0.8250\n",
      "Epoch [327/1000], Loss: 3.6904, Train Acc: 0.8187\n",
      "Epoch [328/1000], Loss: 3.8441, Train Acc: 0.8125\n",
      "Epoch [329/1000], Loss: 3.7625, Train Acc: 0.8125\n",
      "Epoch [330/1000], Loss: 3.5815, Train Acc: 0.8344\n",
      "Epoch [331/1000], Loss: 3.4823, Train Acc: 0.8344\n",
      "Epoch [332/1000], Loss: 3.5038, Train Acc: 0.8313\n",
      "Epoch [333/1000], Loss: 3.5272, Train Acc: 0.8250\n",
      "Epoch [334/1000], Loss: 3.7008, Train Acc: 0.8187\n",
      "Epoch [335/1000], Loss: 3.5830, Train Acc: 0.8313\n",
      "Epoch [336/1000], Loss: 3.5555, Train Acc: 0.8219\n",
      "Epoch [337/1000], Loss: 3.6456, Train Acc: 0.8250\n",
      "Epoch [338/1000], Loss: 3.6166, Train Acc: 0.8094\n",
      "Epoch [339/1000], Loss: 3.6927, Train Acc: 0.8125\n",
      "Epoch [340/1000], Loss: 3.4685, Train Acc: 0.8281\n",
      "Epoch [341/1000], Loss: 3.6436, Train Acc: 0.8313\n",
      "Epoch [342/1000], Loss: 3.6296, Train Acc: 0.8313\n",
      "Epoch [343/1000], Loss: 3.5771, Train Acc: 0.8281\n",
      "Epoch [344/1000], Loss: 3.6382, Train Acc: 0.8250\n",
      "Epoch [345/1000], Loss: 3.6209, Train Acc: 0.8219\n",
      "Epoch [346/1000], Loss: 3.6367, Train Acc: 0.8344\n",
      "Epoch [347/1000], Loss: 3.8171, Train Acc: 0.8313\n",
      "Epoch [348/1000], Loss: 3.6115, Train Acc: 0.8125\n",
      "Epoch [349/1000], Loss: 3.6409, Train Acc: 0.8344\n",
      "Epoch [350/1000], Loss: 3.6215, Train Acc: 0.8281\n",
      "Epoch [351/1000], Loss: 3.7641, Train Acc: 0.8125\n",
      "Epoch [352/1000], Loss: 3.5438, Train Acc: 0.8313\n",
      "Epoch [353/1000], Loss: 3.6724, Train Acc: 0.8250\n",
      "Epoch [354/1000], Loss: 3.6152, Train Acc: 0.8250\n",
      "Epoch [355/1000], Loss: 3.6088, Train Acc: 0.8250\n",
      "Epoch [356/1000], Loss: 3.5516, Train Acc: 0.8219\n",
      "Epoch [357/1000], Loss: 3.7288, Train Acc: 0.8187\n",
      "Epoch [358/1000], Loss: 3.5757, Train Acc: 0.8187\n",
      "Epoch [359/1000], Loss: 3.6024, Train Acc: 0.8313\n",
      "Epoch [360/1000], Loss: 3.6147, Train Acc: 0.8250\n",
      "Epoch [361/1000], Loss: 3.6358, Train Acc: 0.8281\n",
      "Epoch [362/1000], Loss: 3.6950, Train Acc: 0.8250\n",
      "Epoch [363/1000], Loss: 3.6344, Train Acc: 0.8156\n",
      "Epoch [364/1000], Loss: 3.4999, Train Acc: 0.8281\n",
      "Epoch [365/1000], Loss: 3.5269, Train Acc: 0.8250\n",
      "Epoch [366/1000], Loss: 3.5774, Train Acc: 0.8313\n",
      "Epoch [367/1000], Loss: 3.5971, Train Acc: 0.8250\n",
      "Epoch [368/1000], Loss: 3.5891, Train Acc: 0.8156\n",
      "Epoch [369/1000], Loss: 3.6699, Train Acc: 0.8219\n",
      "Epoch [370/1000], Loss: 3.7166, Train Acc: 0.8187\n",
      "Epoch [371/1000], Loss: 3.6432, Train Acc: 0.8313\n",
      "Epoch [372/1000], Loss: 3.6578, Train Acc: 0.8406\n",
      "Epoch [373/1000], Loss: 3.7478, Train Acc: 0.8219\n",
      "Epoch [374/1000], Loss: 3.6781, Train Acc: 0.8281\n",
      "Epoch [375/1000], Loss: 3.6161, Train Acc: 0.8281\n",
      "Epoch [376/1000], Loss: 3.6872, Train Acc: 0.8187\n",
      "Epoch [377/1000], Loss: 3.7271, Train Acc: 0.8219\n",
      "Epoch [378/1000], Loss: 3.6757, Train Acc: 0.8281\n",
      "Epoch [379/1000], Loss: 3.6439, Train Acc: 0.8187\n",
      "Epoch [380/1000], Loss: 3.6367, Train Acc: 0.8281\n",
      "Epoch [381/1000], Loss: 3.6064, Train Acc: 0.8375\n",
      "Epoch [382/1000], Loss: 3.5997, Train Acc: 0.8281\n",
      "Epoch [383/1000], Loss: 3.7074, Train Acc: 0.8250\n",
      "Epoch [384/1000], Loss: 3.6482, Train Acc: 0.8281\n",
      "Epoch [385/1000], Loss: 3.5796, Train Acc: 0.8313\n",
      "Epoch [386/1000], Loss: 3.6604, Train Acc: 0.8281\n",
      "Epoch [387/1000], Loss: 3.6453, Train Acc: 0.8344\n",
      "Epoch [388/1000], Loss: 3.7772, Train Acc: 0.8125\n",
      "Epoch [389/1000], Loss: 3.6959, Train Acc: 0.8375\n",
      "Epoch [390/1000], Loss: 3.6141, Train Acc: 0.8344\n",
      "Epoch [391/1000], Loss: 3.6435, Train Acc: 0.8344\n",
      "Epoch [392/1000], Loss: 3.6119, Train Acc: 0.8313\n",
      "Epoch [393/1000], Loss: 3.7382, Train Acc: 0.8250\n",
      "Epoch [394/1000], Loss: 3.6369, Train Acc: 0.8313\n",
      "Epoch [395/1000], Loss: 3.6138, Train Acc: 0.8250\n",
      "Epoch [396/1000], Loss: 3.6433, Train Acc: 0.8344\n",
      "Epoch [397/1000], Loss: 3.5791, Train Acc: 0.8313\n",
      "Epoch [398/1000], Loss: 3.7397, Train Acc: 0.8250\n",
      "Epoch [399/1000], Loss: 3.7346, Train Acc: 0.8094\n",
      "Epoch [400/1000], Loss: 3.6432, Train Acc: 0.8250\n",
      "Epoch [401/1000], Loss: 3.6650, Train Acc: 0.8344\n",
      "Epoch [402/1000], Loss: 3.6063, Train Acc: 0.8281\n",
      "Epoch [403/1000], Loss: 3.6643, Train Acc: 0.8219\n",
      "Epoch [404/1000], Loss: 3.5515, Train Acc: 0.8187\n",
      "Epoch [405/1000], Loss: 3.6427, Train Acc: 0.8156\n",
      "Epoch [406/1000], Loss: 3.6303, Train Acc: 0.8187\n",
      "Epoch [407/1000], Loss: 3.7429, Train Acc: 0.8250\n",
      "Epoch [408/1000], Loss: 3.6617, Train Acc: 0.8281\n",
      "Epoch [409/1000], Loss: 3.6347, Train Acc: 0.8281\n",
      "Epoch [410/1000], Loss: 3.7123, Train Acc: 0.8281\n",
      "Epoch [411/1000], Loss: 3.6279, Train Acc: 0.8313\n",
      "Epoch [412/1000], Loss: 3.6511, Train Acc: 0.8219\n",
      "Epoch [413/1000], Loss: 3.6710, Train Acc: 0.8187\n",
      "Epoch [414/1000], Loss: 3.5707, Train Acc: 0.8219\n",
      "Epoch [415/1000], Loss: 3.5293, Train Acc: 0.8313\n",
      "Epoch [416/1000], Loss: 3.6145, Train Acc: 0.8250\n",
      "Epoch [417/1000], Loss: 3.4658, Train Acc: 0.8344\n",
      "Epoch [418/1000], Loss: 3.6443, Train Acc: 0.8313\n",
      "Epoch [419/1000], Loss: 3.5936, Train Acc: 0.8281\n",
      "Epoch [420/1000], Loss: 3.6905, Train Acc: 0.8375\n",
      "Epoch [421/1000], Loss: 3.5455, Train Acc: 0.8250\n",
      "Epoch [422/1000], Loss: 3.6544, Train Acc: 0.8438\n",
      "Epoch [423/1000], Loss: 3.6409, Train Acc: 0.8094\n",
      "Epoch [424/1000], Loss: 3.6612, Train Acc: 0.8281\n",
      "Epoch [425/1000], Loss: 3.7508, Train Acc: 0.8250\n",
      "Epoch [426/1000], Loss: 3.6209, Train Acc: 0.8156\n",
      "Epoch [427/1000], Loss: 3.4773, Train Acc: 0.8250\n",
      "Epoch [428/1000], Loss: 3.7247, Train Acc: 0.8219\n",
      "Epoch [429/1000], Loss: 3.6355, Train Acc: 0.8219\n",
      "Epoch [430/1000], Loss: 3.4945, Train Acc: 0.8281\n",
      "Epoch [431/1000], Loss: 3.6504, Train Acc: 0.8250\n",
      "Epoch [432/1000], Loss: 3.7114, Train Acc: 0.8156\n",
      "Epoch [433/1000], Loss: 3.5272, Train Acc: 0.8250\n",
      "Epoch [434/1000], Loss: 3.7605, Train Acc: 0.8094\n",
      "Epoch [435/1000], Loss: 3.5954, Train Acc: 0.8375\n",
      "Epoch [436/1000], Loss: 3.5697, Train Acc: 0.8187\n",
      "Epoch [437/1000], Loss: 3.5049, Train Acc: 0.8438\n",
      "Epoch [438/1000], Loss: 3.6479, Train Acc: 0.8344\n",
      "Epoch [439/1000], Loss: 3.5429, Train Acc: 0.8344\n",
      "Epoch [440/1000], Loss: 3.6274, Train Acc: 0.8313\n",
      "Epoch [441/1000], Loss: 3.5897, Train Acc: 0.8375\n",
      "Epoch [442/1000], Loss: 3.7805, Train Acc: 0.8156\n",
      "Epoch [443/1000], Loss: 3.6270, Train Acc: 0.8313\n",
      "Epoch [444/1000], Loss: 3.6630, Train Acc: 0.8281\n",
      "Epoch [445/1000], Loss: 3.6661, Train Acc: 0.8219\n",
      "Epoch [446/1000], Loss: 3.7154, Train Acc: 0.8125\n",
      "Epoch [447/1000], Loss: 3.6488, Train Acc: 0.8219\n",
      "Epoch [448/1000], Loss: 3.5973, Train Acc: 0.8187\n",
      "Epoch [449/1000], Loss: 3.6560, Train Acc: 0.8250\n",
      "Epoch [450/1000], Loss: 3.5566, Train Acc: 0.8250\n",
      "Epoch [451/1000], Loss: 3.6115, Train Acc: 0.8156\n",
      "Epoch [452/1000], Loss: 3.6666, Train Acc: 0.8250\n",
      "Epoch [453/1000], Loss: 3.6102, Train Acc: 0.8313\n",
      "Epoch [454/1000], Loss: 3.6467, Train Acc: 0.8313\n",
      "Epoch [455/1000], Loss: 3.5556, Train Acc: 0.8313\n",
      "Epoch [456/1000], Loss: 3.4974, Train Acc: 0.8313\n",
      "Epoch [457/1000], Loss: 3.4793, Train Acc: 0.8375\n",
      "Epoch [458/1000], Loss: 3.6219, Train Acc: 0.8063\n",
      "Epoch [459/1000], Loss: 3.6648, Train Acc: 0.8156\n",
      "Epoch [460/1000], Loss: 3.6336, Train Acc: 0.8156\n",
      "Epoch [461/1000], Loss: 3.5688, Train Acc: 0.8344\n",
      "Epoch [462/1000], Loss: 3.5869, Train Acc: 0.8344\n",
      "Epoch [463/1000], Loss: 3.6764, Train Acc: 0.8313\n",
      "Epoch [464/1000], Loss: 3.6122, Train Acc: 0.8281\n",
      "Epoch [465/1000], Loss: 3.6664, Train Acc: 0.8219\n",
      "Epoch [466/1000], Loss: 3.5897, Train Acc: 0.8063\n",
      "Epoch [467/1000], Loss: 3.7043, Train Acc: 0.8187\n",
      "Epoch [468/1000], Loss: 3.5339, Train Acc: 0.8344\n",
      "Epoch [469/1000], Loss: 3.6627, Train Acc: 0.8313\n",
      "Epoch [470/1000], Loss: 3.7157, Train Acc: 0.8187\n",
      "Epoch [471/1000], Loss: 3.6605, Train Acc: 0.8219\n",
      "Epoch [472/1000], Loss: 3.6124, Train Acc: 0.8344\n",
      "Epoch [473/1000], Loss: 3.6139, Train Acc: 0.8219\n",
      "Epoch [474/1000], Loss: 3.6433, Train Acc: 0.8031\n",
      "Epoch [475/1000], Loss: 3.6826, Train Acc: 0.8344\n",
      "Epoch [476/1000], Loss: 3.6335, Train Acc: 0.8469\n",
      "Epoch [477/1000], Loss: 3.7139, Train Acc: 0.8250\n",
      "Epoch [478/1000], Loss: 3.6671, Train Acc: 0.8250\n",
      "Epoch [479/1000], Loss: 3.6441, Train Acc: 0.8187\n",
      "Epoch [480/1000], Loss: 3.6531, Train Acc: 0.8281\n",
      "Epoch [481/1000], Loss: 3.6634, Train Acc: 0.8219\n",
      "Epoch [482/1000], Loss: 3.5994, Train Acc: 0.8250\n",
      "Epoch [483/1000], Loss: 3.5506, Train Acc: 0.8219\n",
      "Epoch [484/1000], Loss: 3.5449, Train Acc: 0.8313\n",
      "Epoch [485/1000], Loss: 3.5963, Train Acc: 0.8250\n",
      "Epoch [486/1000], Loss: 3.5695, Train Acc: 0.8250\n",
      "Epoch [487/1000], Loss: 3.5940, Train Acc: 0.8281\n",
      "Epoch [488/1000], Loss: 3.4980, Train Acc: 0.8187\n",
      "Epoch [489/1000], Loss: 3.5911, Train Acc: 0.8375\n",
      "Epoch [490/1000], Loss: 3.4642, Train Acc: 0.8344\n",
      "Epoch [491/1000], Loss: 3.6583, Train Acc: 0.8344\n",
      "Epoch [492/1000], Loss: 3.5471, Train Acc: 0.8219\n",
      "Epoch [493/1000], Loss: 3.6633, Train Acc: 0.8250\n",
      "Epoch [494/1000], Loss: 3.6998, Train Acc: 0.8187\n",
      "Epoch [495/1000], Loss: 3.5587, Train Acc: 0.8344\n",
      "Epoch [496/1000], Loss: 3.6797, Train Acc: 0.8250\n",
      "Epoch [497/1000], Loss: 3.5367, Train Acc: 0.8313\n",
      "Epoch [498/1000], Loss: 3.5520, Train Acc: 0.8281\n",
      "Epoch [499/1000], Loss: 3.6311, Train Acc: 0.8250\n",
      "Epoch [500/1000], Loss: 3.4972, Train Acc: 0.8219\n",
      "Epoch [501/1000], Loss: 3.5230, Train Acc: 0.8344\n",
      "Epoch [502/1000], Loss: 3.6789, Train Acc: 0.8187\n",
      "Epoch [503/1000], Loss: 3.7126, Train Acc: 0.8156\n",
      "Epoch [504/1000], Loss: 3.6700, Train Acc: 0.8313\n",
      "Epoch [505/1000], Loss: 3.6645, Train Acc: 0.8313\n",
      "Epoch [506/1000], Loss: 3.7108, Train Acc: 0.8281\n",
      "Epoch [507/1000], Loss: 3.6621, Train Acc: 0.8219\n",
      "Epoch [508/1000], Loss: 3.7144, Train Acc: 0.8281\n",
      "Epoch [509/1000], Loss: 3.6281, Train Acc: 0.8250\n",
      "Epoch [510/1000], Loss: 3.5879, Train Acc: 0.8313\n",
      "Epoch [511/1000], Loss: 3.6280, Train Acc: 0.8344\n",
      "Epoch [512/1000], Loss: 3.6218, Train Acc: 0.8313\n",
      "Epoch [513/1000], Loss: 3.6246, Train Acc: 0.8125\n",
      "Epoch [514/1000], Loss: 3.5272, Train Acc: 0.8313\n",
      "Epoch [515/1000], Loss: 3.6343, Train Acc: 0.8281\n",
      "Epoch [516/1000], Loss: 3.5725, Train Acc: 0.8156\n",
      "Epoch [517/1000], Loss: 3.6463, Train Acc: 0.8156\n",
      "Epoch [518/1000], Loss: 3.5504, Train Acc: 0.8313\n",
      "Epoch [519/1000], Loss: 3.5331, Train Acc: 0.8313\n",
      "Epoch [520/1000], Loss: 3.5222, Train Acc: 0.8313\n",
      "Epoch [521/1000], Loss: 3.6644, Train Acc: 0.8125\n",
      "Epoch [522/1000], Loss: 3.6054, Train Acc: 0.8281\n",
      "Epoch [523/1000], Loss: 3.6042, Train Acc: 0.8156\n",
      "Epoch [524/1000], Loss: 3.7333, Train Acc: 0.8156\n",
      "Epoch [525/1000], Loss: 3.6277, Train Acc: 0.8156\n",
      "Epoch [526/1000], Loss: 3.4599, Train Acc: 0.8406\n",
      "Epoch [527/1000], Loss: 3.6099, Train Acc: 0.8344\n",
      "Epoch [528/1000], Loss: 3.6126, Train Acc: 0.8219\n",
      "Epoch [529/1000], Loss: 3.5784, Train Acc: 0.8344\n",
      "Epoch [530/1000], Loss: 3.4761, Train Acc: 0.8313\n",
      "Epoch [531/1000], Loss: 3.6063, Train Acc: 0.8250\n",
      "Epoch [532/1000], Loss: 3.6772, Train Acc: 0.8344\n",
      "Epoch [533/1000], Loss: 3.5301, Train Acc: 0.8219\n",
      "Epoch [534/1000], Loss: 3.5820, Train Acc: 0.8313\n",
      "Epoch [535/1000], Loss: 3.5440, Train Acc: 0.8250\n",
      "Epoch [536/1000], Loss: 3.7436, Train Acc: 0.8187\n",
      "Epoch [537/1000], Loss: 3.6304, Train Acc: 0.8375\n",
      "Epoch [538/1000], Loss: 3.6167, Train Acc: 0.8219\n",
      "Epoch [539/1000], Loss: 3.5570, Train Acc: 0.8219\n",
      "Epoch [540/1000], Loss: 3.5904, Train Acc: 0.8375\n",
      "Epoch [541/1000], Loss: 3.5782, Train Acc: 0.8219\n",
      "Epoch [542/1000], Loss: 3.6345, Train Acc: 0.8375\n",
      "Epoch [543/1000], Loss: 3.5425, Train Acc: 0.8219\n",
      "Epoch [544/1000], Loss: 3.6859, Train Acc: 0.8375\n",
      "Epoch [545/1000], Loss: 3.5132, Train Acc: 0.8281\n",
      "Epoch [546/1000], Loss: 3.5264, Train Acc: 0.8438\n",
      "Epoch [547/1000], Loss: 3.5203, Train Acc: 0.8250\n",
      "Epoch [548/1000], Loss: 3.6792, Train Acc: 0.8313\n",
      "Epoch [549/1000], Loss: 3.5838, Train Acc: 0.8250\n",
      "Epoch [550/1000], Loss: 3.5050, Train Acc: 0.8344\n",
      "Epoch [551/1000], Loss: 3.5709, Train Acc: 0.8281\n",
      "Epoch [552/1000], Loss: 3.6656, Train Acc: 0.8187\n",
      "Epoch [553/1000], Loss: 3.6286, Train Acc: 0.8187\n",
      "Epoch [554/1000], Loss: 3.5486, Train Acc: 0.8313\n",
      "Epoch [555/1000], Loss: 3.5953, Train Acc: 0.8438\n",
      "Epoch [556/1000], Loss: 3.6254, Train Acc: 0.8219\n",
      "Epoch [557/1000], Loss: 3.7176, Train Acc: 0.8187\n",
      "Epoch [558/1000], Loss: 3.6566, Train Acc: 0.8250\n",
      "Epoch [559/1000], Loss: 3.6616, Train Acc: 0.8219\n",
      "Epoch [560/1000], Loss: 3.6854, Train Acc: 0.8094\n",
      "Epoch [561/1000], Loss: 3.6047, Train Acc: 0.8187\n",
      "Epoch [562/1000], Loss: 3.4700, Train Acc: 0.8219\n",
      "Epoch [563/1000], Loss: 3.7094, Train Acc: 0.8281\n",
      "Epoch [564/1000], Loss: 3.6673, Train Acc: 0.8187\n",
      "Epoch [565/1000], Loss: 3.5412, Train Acc: 0.8187\n",
      "Epoch [566/1000], Loss: 3.5451, Train Acc: 0.8344\n",
      "Epoch [567/1000], Loss: 3.5789, Train Acc: 0.8281\n",
      "Epoch [568/1000], Loss: 3.5367, Train Acc: 0.8187\n",
      "Epoch [569/1000], Loss: 3.5215, Train Acc: 0.8125\n",
      "Epoch [570/1000], Loss: 3.5452, Train Acc: 0.8250\n",
      "Epoch [571/1000], Loss: 3.5234, Train Acc: 0.8406\n",
      "Epoch [572/1000], Loss: 3.5844, Train Acc: 0.8250\n",
      "Epoch [573/1000], Loss: 3.5527, Train Acc: 0.8281\n",
      "Epoch [574/1000], Loss: 3.6062, Train Acc: 0.8344\n",
      "Epoch [575/1000], Loss: 3.7305, Train Acc: 0.8187\n",
      "Epoch [576/1000], Loss: 3.5797, Train Acc: 0.8187\n",
      "Epoch [577/1000], Loss: 3.5728, Train Acc: 0.8281\n",
      "Epoch [578/1000], Loss: 3.6486, Train Acc: 0.8187\n",
      "Epoch [579/1000], Loss: 3.7548, Train Acc: 0.8125\n",
      "Epoch [580/1000], Loss: 3.6781, Train Acc: 0.8250\n",
      "Epoch [581/1000], Loss: 3.6316, Train Acc: 0.8375\n",
      "Epoch [582/1000], Loss: 3.6243, Train Acc: 0.8281\n",
      "Epoch [583/1000], Loss: 3.5392, Train Acc: 0.8187\n",
      "Epoch [584/1000], Loss: 3.6545, Train Acc: 0.8313\n",
      "Epoch [585/1000], Loss: 3.5019, Train Acc: 0.8344\n",
      "Epoch [586/1000], Loss: 3.4120, Train Acc: 0.8469\n",
      "Epoch [587/1000], Loss: 3.5708, Train Acc: 0.8219\n",
      "Epoch [588/1000], Loss: 3.5736, Train Acc: 0.8250\n",
      "Epoch [589/1000], Loss: 3.5737, Train Acc: 0.8375\n",
      "Epoch [590/1000], Loss: 3.6694, Train Acc: 0.8281\n",
      "Epoch [591/1000], Loss: 3.6828, Train Acc: 0.8375\n",
      "Epoch [592/1000], Loss: 3.5221, Train Acc: 0.8406\n",
      "Epoch [593/1000], Loss: 3.5369, Train Acc: 0.8406\n",
      "Epoch [594/1000], Loss: 3.6165, Train Acc: 0.8281\n",
      "Epoch [595/1000], Loss: 3.4439, Train Acc: 0.8438\n",
      "Epoch [596/1000], Loss: nan, Train Acc: 0.5750\n",
      "Epoch [597/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [598/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [599/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [600/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [601/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [602/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [603/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [604/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [605/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [606/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [607/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [608/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [609/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [610/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [611/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [612/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [613/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [614/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [615/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [616/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [617/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [618/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [619/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [620/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [621/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [622/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [623/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [624/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [625/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [626/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [627/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [628/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [629/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [630/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [631/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [632/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [633/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [634/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [635/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [636/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [637/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [638/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [639/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [640/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [641/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [642/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [643/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [644/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [645/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [646/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [647/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [648/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [649/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [650/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [651/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [652/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [653/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [654/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [655/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [656/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [657/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [658/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [659/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [660/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [661/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [662/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [663/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [664/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [665/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [666/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [667/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [668/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [669/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [670/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [671/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [672/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [673/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [674/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [675/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [676/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [677/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [678/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [679/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [680/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [681/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [682/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [683/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [684/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [685/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [686/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [687/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [688/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [689/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [690/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [691/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [692/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [693/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [694/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [695/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [696/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [697/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [698/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [699/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [700/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [701/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [702/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [703/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [704/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [705/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [706/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [707/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [708/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [709/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [710/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [711/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [712/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [713/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [714/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [715/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [716/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [717/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [718/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [719/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [720/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [721/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [722/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [723/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [724/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [725/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [726/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [727/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [728/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [729/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [730/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [731/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [732/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [733/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [734/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [735/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [736/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [737/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [738/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [739/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [740/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [741/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [742/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [743/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [744/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [745/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [746/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [747/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [748/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [749/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [750/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [751/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [752/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [753/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [754/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [755/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [756/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [757/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [758/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [759/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [760/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [761/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [762/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [763/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [764/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [765/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [766/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [767/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [768/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [769/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [770/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [771/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [772/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [773/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [774/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [775/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [776/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [777/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [778/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [779/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [780/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [781/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [782/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [783/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [784/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [785/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [786/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [787/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [788/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [789/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [790/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [791/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [792/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [793/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [794/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [795/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [796/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [797/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [798/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [799/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [800/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [801/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [802/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [803/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [804/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [805/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [806/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [807/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [808/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [809/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [810/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [811/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [812/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [813/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [814/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [815/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [816/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [817/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [818/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [819/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [820/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [821/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [822/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [823/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [824/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [825/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [826/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [827/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [828/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [829/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [830/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [831/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [832/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [833/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [834/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [835/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [836/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [837/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [838/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [839/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [840/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [841/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [842/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [843/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [844/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [845/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [846/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [847/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [848/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [849/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [850/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [851/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [852/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [853/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [854/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [855/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [856/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [857/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [858/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [859/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [860/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [861/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [862/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [863/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [864/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [865/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [866/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [867/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [868/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [869/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [870/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [871/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [872/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [873/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [874/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [875/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [876/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [877/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [878/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [879/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [880/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [881/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [882/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [883/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [884/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [885/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [886/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [887/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [888/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [889/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [890/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [891/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [892/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [893/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [894/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [895/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [896/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [897/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [898/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [899/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [900/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [901/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [902/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [903/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [904/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [905/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [906/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [907/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [908/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [909/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [910/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [911/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [912/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [913/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [914/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [915/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [916/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [917/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [918/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [919/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [920/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [921/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [922/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [923/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [924/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [925/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [926/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [927/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [928/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [929/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [930/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [931/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [932/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [933/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [934/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [935/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [936/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [937/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [938/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [939/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [940/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [941/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [942/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [943/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [944/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [945/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [946/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [947/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [948/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [949/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [950/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [951/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [952/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [953/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [954/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [955/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [956/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [957/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [958/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [959/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [960/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [961/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [962/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [963/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [964/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [965/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [966/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [967/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [968/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [969/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [970/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [971/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [972/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [973/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [974/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [975/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [976/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [977/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [978/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [979/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [980/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [981/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [982/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [983/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [984/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [985/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [986/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [987/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [988/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [989/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [990/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [991/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [992/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [993/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [994/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [995/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [996/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [997/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [998/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [999/1000], Loss: nan, Train Acc: 0.4875\n",
      "Epoch [1000/1000], Loss: nan, Train Acc: 0.4875\n",
      "Training complete!\n",
      "MLP Test Accuracy: 0.5500\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "input_size = X_train.shape[1]\n",
    "output_size = len(set(y_train))  # Number of classes\n",
    "hidden_size = [300, 200]\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.long)),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.long)),\n",
    "    batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size, dropout_rate, learning_rate)\n",
    "model.train_model(train_loader, epochs=epochs)\n",
    "\n",
    "# Evaluate MLP model\n",
    "mlp_accuracy = model.evaluate(test_loader)\n",
    "print(f\"MLP Test Accuracy: {mlp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but getting the accuracy in a one-liner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Using device: cuda (1 GPUs available)\n",
      "DEBUG: Optimizer initialized? True\n",
      "âœ… Running on CUDA!\n",
      "Epoch [1/100], Loss: 9.7973, Train Acc: 0.6531\n",
      "Epoch [2/100], Loss: 6.6727, Train Acc: 0.7406\n",
      "Epoch [3/100], Loss: 5.7438, Train Acc: 0.7688\n",
      "Epoch [4/100], Loss: 4.8472, Train Acc: 0.7937\n",
      "Epoch [5/100], Loss: 4.9642, Train Acc: 0.7656\n",
      "Epoch [6/100], Loss: 4.6204, Train Acc: 0.7906\n",
      "Epoch [7/100], Loss: 4.7605, Train Acc: 0.7812\n",
      "Epoch [8/100], Loss: 4.7469, Train Acc: 0.7844\n",
      "Epoch [9/100], Loss: 4.5166, Train Acc: 0.8125\n",
      "Epoch [10/100], Loss: 4.3355, Train Acc: 0.7937\n",
      "Epoch [11/100], Loss: 4.0849, Train Acc: 0.8219\n",
      "Epoch [12/100], Loss: 4.1167, Train Acc: 0.8031\n",
      "Epoch [13/100], Loss: 4.2674, Train Acc: 0.8063\n",
      "Epoch [14/100], Loss: 4.0178, Train Acc: 0.8187\n",
      "Epoch [15/100], Loss: 4.2022, Train Acc: 0.7906\n",
      "Epoch [16/100], Loss: 3.9901, Train Acc: 0.8156\n",
      "Epoch [17/100], Loss: 4.2762, Train Acc: 0.8031\n",
      "Epoch [18/100], Loss: 4.0971, Train Acc: 0.7875\n",
      "Epoch [19/100], Loss: 4.1427, Train Acc: 0.8156\n",
      "Epoch [20/100], Loss: 4.2476, Train Acc: 0.8125\n",
      "Epoch [21/100], Loss: 4.1096, Train Acc: 0.8063\n",
      "Epoch [22/100], Loss: 3.7034, Train Acc: 0.8281\n",
      "Epoch [23/100], Loss: 3.7386, Train Acc: 0.8313\n",
      "Epoch [24/100], Loss: 3.9638, Train Acc: 0.8187\n",
      "Epoch [25/100], Loss: 4.0819, Train Acc: 0.8156\n",
      "Epoch [26/100], Loss: 4.1943, Train Acc: 0.8125\n",
      "Epoch [27/100], Loss: 4.4897, Train Acc: 0.7937\n",
      "Epoch [28/100], Loss: 4.1385, Train Acc: 0.8125\n",
      "Epoch [29/100], Loss: 4.1159, Train Acc: 0.8094\n",
      "Epoch [30/100], Loss: 3.9350, Train Acc: 0.8250\n",
      "Epoch [31/100], Loss: 4.0647, Train Acc: 0.8125\n",
      "Epoch [32/100], Loss: 3.8569, Train Acc: 0.8063\n",
      "Epoch [33/100], Loss: 3.9347, Train Acc: 0.8187\n",
      "Epoch [34/100], Loss: 4.0095, Train Acc: 0.7969\n",
      "Epoch [35/100], Loss: 3.9903, Train Acc: 0.8063\n",
      "Epoch [36/100], Loss: 3.9847, Train Acc: 0.8125\n",
      "Epoch [37/100], Loss: 3.8665, Train Acc: 0.8219\n",
      "Epoch [38/100], Loss: 4.0604, Train Acc: 0.8281\n",
      "Epoch [39/100], Loss: 3.8218, Train Acc: 0.8250\n",
      "Epoch [40/100], Loss: 3.9913, Train Acc: 0.8031\n",
      "Epoch [41/100], Loss: 3.9801, Train Acc: 0.8125\n",
      "Epoch [42/100], Loss: 3.8545, Train Acc: 0.8125\n",
      "Epoch [43/100], Loss: 4.0934, Train Acc: 0.8031\n",
      "Epoch [44/100], Loss: 3.9936, Train Acc: 0.8063\n",
      "Epoch [45/100], Loss: 3.7093, Train Acc: 0.8156\n",
      "Epoch [46/100], Loss: 3.8552, Train Acc: 0.8219\n",
      "Epoch [47/100], Loss: 3.7309, Train Acc: 0.8250\n",
      "Epoch [48/100], Loss: 3.7900, Train Acc: 0.8187\n",
      "Epoch [49/100], Loss: 3.7742, Train Acc: 0.8156\n",
      "Epoch [50/100], Loss: 3.7565, Train Acc: 0.8219\n",
      "Epoch [51/100], Loss: 3.7575, Train Acc: 0.8344\n",
      "Epoch [52/100], Loss: 3.6881, Train Acc: 0.8313\n",
      "Epoch [53/100], Loss: 3.7978, Train Acc: 0.8250\n",
      "Epoch [54/100], Loss: 3.7752, Train Acc: 0.8156\n",
      "Epoch [55/100], Loss: 3.8195, Train Acc: 0.8156\n",
      "Epoch [56/100], Loss: 3.8150, Train Acc: 0.8281\n",
      "Epoch [57/100], Loss: 3.7241, Train Acc: 0.8156\n",
      "Epoch [58/100], Loss: 3.8854, Train Acc: 0.8156\n",
      "Epoch [59/100], Loss: 3.9993, Train Acc: 0.8250\n",
      "Epoch [60/100], Loss: 3.7980, Train Acc: 0.8125\n",
      "Epoch [61/100], Loss: 3.7868, Train Acc: 0.8250\n",
      "Epoch [62/100], Loss: 3.8454, Train Acc: 0.8000\n",
      "Epoch [63/100], Loss: 3.7716, Train Acc: 0.8313\n",
      "Epoch [64/100], Loss: 3.9250, Train Acc: 0.8187\n",
      "Epoch [65/100], Loss: 3.7683, Train Acc: 0.8313\n",
      "Epoch [66/100], Loss: 3.7925, Train Acc: 0.8344\n",
      "Epoch [67/100], Loss: 3.6167, Train Acc: 0.8344\n",
      "Epoch [68/100], Loss: 3.8353, Train Acc: 0.8250\n",
      "Epoch [69/100], Loss: 3.6983, Train Acc: 0.8375\n",
      "Epoch [70/100], Loss: 3.8139, Train Acc: 0.8125\n",
      "Epoch [71/100], Loss: 3.7889, Train Acc: 0.8344\n",
      "Epoch [72/100], Loss: 3.8002, Train Acc: 0.8063\n",
      "Epoch [73/100], Loss: 3.9114, Train Acc: 0.8125\n",
      "Epoch [74/100], Loss: 3.6053, Train Acc: 0.8250\n",
      "Epoch [75/100], Loss: 3.6642, Train Acc: 0.8406\n",
      "Epoch [76/100], Loss: 3.7599, Train Acc: 0.8187\n",
      "Epoch [77/100], Loss: 3.5522, Train Acc: 0.8313\n",
      "Epoch [78/100], Loss: 3.6213, Train Acc: 0.8281\n",
      "Epoch [79/100], Loss: 3.7473, Train Acc: 0.8187\n",
      "Epoch [80/100], Loss: 3.7659, Train Acc: 0.8125\n",
      "Epoch [81/100], Loss: 3.7030, Train Acc: 0.8094\n",
      "Epoch [82/100], Loss: 3.6041, Train Acc: 0.8250\n",
      "Epoch [83/100], Loss: 3.7799, Train Acc: 0.8313\n",
      "Epoch [84/100], Loss: 3.6020, Train Acc: 0.8344\n",
      "Epoch [85/100], Loss: 3.6512, Train Acc: 0.8313\n",
      "Epoch [86/100], Loss: 4.0403, Train Acc: 0.8094\n",
      "Epoch [87/100], Loss: 3.7276, Train Acc: 0.8250\n",
      "Epoch [88/100], Loss: 3.6775, Train Acc: 0.8281\n",
      "Epoch [89/100], Loss: 3.5889, Train Acc: 0.8250\n",
      "Epoch [90/100], Loss: 3.8524, Train Acc: 0.8125\n",
      "Epoch [91/100], Loss: 3.7928, Train Acc: 0.8219\n",
      "Epoch [92/100], Loss: 3.8725, Train Acc: 0.8094\n",
      "Epoch [93/100], Loss: 3.7205, Train Acc: 0.8250\n",
      "Epoch [94/100], Loss: 3.5999, Train Acc: 0.8313\n",
      "Epoch [95/100], Loss: 3.5863, Train Acc: 0.8375\n",
      "Epoch [96/100], Loss: 3.7173, Train Acc: 0.8219\n",
      "Epoch [97/100], Loss: 3.8192, Train Acc: 0.8219\n",
      "Epoch [98/100], Loss: 3.7306, Train Acc: 0.8344\n",
      "Epoch [99/100], Loss: 3.7481, Train Acc: 0.8250\n",
      "Epoch [100/100], Loss: 3.7187, Train Acc: 0.8250\n",
      "Training complete!\n",
      "=== Multilayer Perceptron (MLP) Accuracy: 0.74 ===\n"
     ]
    }
   ],
   "source": [
    "# Train SVM model using SSA data\n",
    "output_file = 'data/mRNA_trajectories_example.csv'\n",
    "X_train, X_test, y_train, y_test = load_and_split_data(output_file)\n",
    "mlp_accuracy = mlp_classifier(X_train, X_test, y_train, y_test, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
